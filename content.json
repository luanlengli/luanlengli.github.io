{"meta":{"title":"luanlengli's Blog","subtitle":"“不知道”的五大理由，不读，不查，不试，理解能力差，满脑子想着怎么利用他人","description":"just do it!","author":"乱愣黎","url":"https://luanlengli.github.io","root":"/"},"pages":[{"title":"about","date":"2018-12-11T01:57:28.000Z","updated":"2018-12-11T01:58:02.000Z","comments":true,"path":"about/index.html","permalink":"https://luanlengli.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2018-12-11T01:57:18.000Z","updated":"2018-12-11T01:58:16.000Z","comments":true,"path":"categories/index.html","permalink":"https://luanlengli.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-12-11T01:57:23.000Z","updated":"2018-12-11T01:58:25.000Z","comments":true,"path":"tags/index.html","permalink":"https://luanlengli.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"阿里云GPU计算型ECS实例安装NVIDIA驱动和CUDA","slug":"阿里云GPU计算型ECS实例安装NVIDIA驱动和CUDA","date":"2019-12-31T15:40:43.000Z","updated":"2020-01-03T12:45:47.000Z","comments":true,"path":"2019/12/31/阿里云GPU计算型ECS实例安装NVIDIA驱动和CUDA.html","link":"","permalink":"https://luanlengli.github.io/2019/12/31/阿里云GPU计算型ECS实例安装NVIDIA驱动和CUDA.html","excerpt":"","text":"说明理论上适用于其他云平台的GPU计算型实例或者自建GPU计算服务器阿里云实例类型GPU计算型 gn5GPU型号Nvidia Tesla P100操作系统CentOS-7.7-1908 64位Nvidia驱动版本390.116CUDA Driver版本9.1.85CUDA Runtime版本9.0.176cuDNN版本v7.6.0.64Nvidia GPU 兼容性的可以看这篇文章Docker版本19.03.5GPU驱动、CUDA、cuDNN版本的选择，请自行根据项目要求进行调整！手动安装环境准备禁用SELINUX12sed -i 's,^SELINUX=.*,SELINUX=disabled,g' /etc/selinux/configsetenforce 0禁用防火墙1systemctl disable --now firewalld.service添加sysctl参数fs参数123456789101112cat &gt; /etc/sysctl.d/99-fs.conf &lt;&lt;EOF# 最大文件句柄数fs.file-max=1048576# 最大文件打开数fs.nr_open=1048576# 同一时间异步IO请求数fs.aio-max-nr=1048576# 在CentOS7.4引入了一个新的参数来控制内核的行为。 # /proc/sys/fs/may_detach_mounts 默认设置为0# 当系统有容器运行的时候，需要将该值设置为1。fs.may_detach_mounts=1EOFvm参数12345678910cat &gt; /etc/sysctl.d/99-vm.conf &lt;&lt;EOF# 内存耗尽才使用swap分区vm.swappiness=10# 当内存耗尽时，内核会触发OOM killer根据oom_score杀掉最耗内存的进程vm.panic_on_oom=0# 允许overcommitvm.overcommit_memory=1# 定义了进程能拥有的最多内存区域，默认65536vm.max_map_count=262144EOFnet参数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465cat &gt; /etc/sysctl.d/99-net.conf &lt;&lt;EOF# 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤net.bridge.bridge-nf-call-arptables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# 关闭严格校验数据包的反向路径，默认值1net.ipv4.conf.default.rp_filter=0net.ipv4.conf.all.rp_filter=0# 进程间通信发送数据, 默认100net.unix.max_dgram_qlen=512# 设置 conntrack 的上限net.netfilter.nf_conntrack_max=1048576# 设置连接跟踪表中处于TIME_WAIT状态的超时时间net.netfilter.nf_conntrack_tcp_timeout_timewait=30# 设置连接跟踪表中TCP连接超时时间net.netfilter.nf_conntrack_tcp_timeout_established=1200# 端口最大的监听队列的长度net.core.somaxconn=21644# 接收自网卡、但未被内核协议栈处理的报文队列长度net.core.netdev_max_backlog=262144# 系统无内存压力、启动压力模式阈值、最大值，单位为页的数量#net.ipv4.tcp_mem=1541646 2055528 3083292# 内核socket接收缓存区字节数min/default/maxnet.core.rmem=4096 65536 8388608# 内核socket发送缓存区字节数min/default/maxnet.core.wmem=4096 65536 8388608# 开启自动调节缓存模式net.ipv4.tcp_moderate_rcvbuf=1# TCP阻塞控制算法BBR，Linux内核版本4.9开始内置BBR算法#net.ipv4.tcp_congestion_control=bbr#net.core.default_qdisc=fq# 用作本地随机TCP端口的范围net.ipv4.ip_local_port_range=10000 65000# 打开ipv4数据包转发net.ipv4.ip_forward=1# 允许应用程序能够绑定到不属于本地网卡的地址net.ipv4.ip_nonlocal_bind=1# 系统中处于 SYN_RECV 状态的 TCP 连接数量net.ipv4.tcp_max_syn_backlog=16384# 内核中管理 TIME_WAIT 状态的数量net.ipv4.tcp_max_tw_buckets=5000# 指定重发 SYN/ACK 的次数net.ipv4.tcp_synack_retries=2# TCP连接中TIME_WAIT sockets的快速回收net.ipv4.tcp_tw_recycle=0# 不属于任何进程的tcp socket最大数量. 超过这个数量的socket会被reset, 并告警net.ipv4.tcp_max_orphans=1024# TCP FIN报文重试次数net.ipv4.tcp_orphan_retries=8# 加快系统关闭处于 FIN_WAIT2 状态的 TCP 连接net.ipv4.tcp_fin_timeout=15# TCP连接keepalive的持续时间，默认7200net.ipv4.tcp_keepalive_time=600# TCP keepalive探测包发送间隔net.ipv4.tcp_keepalive_intvl=30# TCP keepalive探测包重试次数net.ipv4.tcp_keepalive_probes=10# TCP FastOpen# 0:关闭 ; 1:作为客户端时使用 ; 2:作为服务器端时使用 ; 3:无论作为客户端还是服务器端都使用net.ipv4.tcp_fastopen=3# 限制TCP重传次数net.ipv4.tcp_retries1=3# TCP重传次数到达上限时，关闭TCP连接net.ipv4.tcp_retries2=15EOF修改limits参数1234cat &gt; /etc/security/limits.d/99-centos.conf &lt;&lt;EOF* - nproc 1048576* - nofile 1048576EOF修改journal设置12345sed -e 's,^#Compress=yes,Compress=yes,' \\ -e 's,^#SystemMaxUse=,SystemMaxUse=2G,' \\ -e 's,^#Seal=yes,Seal=yes,' \\ -e 's,^#RateLimitBurst=1000,RateLimitBurst=5000,' \\ -i /etc/systemd/journald.conf修改history参数1234567cat &gt; /etc/profile.d/history.sh &lt;&lt;EOFexport HISTSIZE=10000export HISTFILESIZE=10000export HISTCONTROL=ignoredupsexport HISTTIMEFORMAT=\"`whoami` %F %T \"export HISTIGNORE=\"ls:pwd:ll:ls -l:ls -a:ll -a\"EOF更新软件1yum update -y安装编译环境1yum groups install base 'Development Tools'安装常用工具1234567891011121314151617181920yum install -y nc \\ git \\ vim \\ ipvsadm \\ tree \\ dstat \\ iotop \\ htop \\ socat \\ ipset\\ conntrack \\ bash-completion-extras \\ tcpdump \\ wireshark \\ bcc-tools \\ perf \\ trace-cmd \\ systemtap \\ nethogs \\ lshw检查GPU查看PCI设备1lspci | grep -i nvidia输出示例100:08.0 3D controller: NVIDIA Corporation GP100GL [Tesla P100 PCIe 16GB] (rev a1)查看硬件信息1lshw -numeric -C display输出示例12345678910111213141516171819202122232425*-display:0 description: VGA compatible controller product: GD 5446 [1013:B8] vendor: Cirrus Logic [1013] physical id: 2 bus info: pci@0000:00:02.0 version: 00 width: 32 bits clock: 33MHz capabilities: vga_controller rom configuration: driver=cirrus latency=0 resources: irq:0 memory:fa000000-fbffffff memory:fe850000-fe850fff memory:fe840000-fe84ffff*-display:1 description: 3D controller product: GP100GL [Tesla P100 PCIe 16GB] [10DE:15F8] vendor: NVIDIA Corporation [10DE] physical id: 8 bus info: pci@0000:00:08.0 logical name: /dev/fb0 version: a1 width: 64 bits clock: 33MHz capabilities: pm msi pciexpress bus_master cap_list fb configuration: depth=16 driver=nvidia latency=0 mode=1024x768 visual=truecolor xres=1024 yres=768 resources: iomemory:100-ff iomemory:140-13f irq:11 memory:fd000000-fdffffff memory:1000000000-13ffffffff memory:1400000000-1401ffffff禁用nouveau驱动1234echo \"blacklist nouveau\" &gt; /etc/modprobe.d/blacklist-nouveau.confecho \"options nouveau modeset=0\" &gt;&gt; /etc/modprobe.d/blacklist-nouveau.confrmmod nouveaudracut --force重启1reboot安装GPU驱动访问Nvidia官网Nvidia驱动程序下载下载驱动网页下载产品类型Tesla产品类型P-Series产品家族Tesla P100操作系统Linux 64-bit RHEL7CUDA Toolkit9.1点击搜索，跳转到驱动程序下载页面点击下载，跳转到驱动下载再次点击下载，保存驱动文件命令行下载1wget -t 10 --timeout=10 http://cn.download.nvidia.com/tesla/390.116/nvidia-diag-driver-local-repo-rhel7-390.116-1.0-1.x86_64.rpm安装驱动123yum install nvidia-diag-driver-local-repo-rhel7-390.116-1.0-1.x86_64.rpmyum clean allyum install cuda-drivers-390.116-1.x86_64重启1reboot检查驱动情况检查是否加载开源驱动nouveau这里不应该有结果输出！1lsmod | grep nouveau检查驱动版本1cat /proc/driver/nvidia/version输出示例12NVRM version: NVIDIA UNIX x86_64 Kernel Module 390.116 Sun Jan 27 07:21:36 PST 2019GCC version: gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC)查看GPU Summary1nvidia-smi输出示例12345678910111213141516+-----------------------------------------------------------------------------+| NVIDIA-SMI 390.116 Driver Version: 390.116 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P100-PCIE... Off | 00000000:00:08.0 Off | 0 || N/A 32C P0 27W / 250W | 0MiB / 16280MiB | 3% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+查看GPU详细信息1nvidia-smi -i 0 -q输出示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173==============NVSMI LOG==============Driver Version : 390.116CUDA Version : 9.1Attached GPUs : 1GPU 00000000:00:08.0 Product Name : Tesla P100-PCIE-16GB Product Brand : Tesla Display Mode : Enabled Display Active : Disabled Persistence Mode : Enabled Accounting Mode : Disabled Accounting Mode Buffer Size : 4000 Driver Model Current : N/A Pending : N/A Serial Number : 0322818005606 GPU UUID : GPU-81662b67-e58c-d9b6-763f-8adc65071cba Minor Number : 0 VBIOS Version : 86.00.4D.00.01 MultiGPU Board : No Board ID : 0x8 GPU Part Number : 900-2H400-0000-000 Inforom Version Image Version : H400.0201.00.08 OEM Object : 1.1 ECC Object : 4.1 Power Management Object : N/A GPU Operation Mode Current : N/A Pending : N/A GPU Virtualization Mode Virtualization mode : Pass-Through IBMNPU Relaxed Ordering Mode : N/A PCI Bus : 0x00 Device : 0x08 Domain : 0x0000 Device Id : 0x15F810DE Bus Id : 00000000:00:08.0 Sub System Id : 0x118F10DE GPU Link Info PCIe Generation Max : 3 Current : 3 Link Width Max : 16x Current : 16x Bridge Chip Type : N/A Firmware : N/A Replays since reset : 0 Tx Throughput : 0 KB/s Rx Throughput : 0 KB/s Fan Speed : N/A Performance State : P0 Clocks Throttle Reasons Idle : Not Active Applications Clocks Setting : Not Active SW Power Cap : Not Active HW Slowdown : Not Active HW Thermal Slowdown : Not Active HW Power Brake Slowdown : Not Active Sync Boost : Not Active SW Thermal Slowdown : Not Active Display Clock Setting : Not Active FB Memory Usage Total : 16280 MiB Used : 8133 MiB Free : 8147 MiB BAR1 Memory Usage Total : 16384 MiB Used : 2 MiB Free : 16382 MiB Compute Mode : Default Utilization Gpu : 0 % Memory : 0 % Encoder : 0 % Decoder : 0 % Encoder Stats Active Sessions : 0 Average FPS : 0 Average Latency : 0 FBC Stats Active Sessions : 0 Average FPS : 0 Average Latency : 0 Ecc Mode Current : Enabled Pending : Enabled ECC Errors Volatile Single Bit Device Memory : 0 Register File : 0 L1 Cache : N/A L2 Cache : 0 Texture Memory : 0 Texture Shared : 0 CBU : N/A Total : 0 Double Bit Device Memory : 0 Register File : 0 L1 Cache : N/A L2 Cache : 0 Texture Memory : 0 Texture Shared : 0 CBU : N/A Total : 0 Aggregate Single Bit Device Memory : 0 Register File : 0 L1 Cache : N/A L2 Cache : 0 Texture Memory : 0 Texture Shared : 0 CBU : N/A Total : 0 Double Bit Device Memory : 0 Register File : 0 L1 Cache : N/A L2 Cache : 0 Texture Memory : 0 Texture Shared : 0 CBU : N/A Total : 0 Retired Pages Single Bit ECC : 0 Double Bit ECC : 0 Pending Page Blacklist : No Temperature GPU Current Temp : 34 C GPU Shutdown Temp : 85 C GPU Slowdown Temp : 82 C GPU Max Operating Temp : N/A Memory Current Temp : N/A Memory Max Operating Temp : N/A Power Readings Power Management : Supported Power Draw : 31.01 W Power Limit : 250.00 W Default Power Limit : 250.00 W Enforced Power Limit : 250.00 W Min Power Limit : 125.00 W Max Power Limit : 250.00 W Clocks Graphics : 1189 MHz SM : 1189 MHz Memory : 715 MHz Video : 1063 MHz Applications Clocks Graphics : 1189 MHz Memory : 715 MHz Default Applications Clocks Graphics : 1189 MHz Memory : 715 MHz Max Clocks Graphics : 1328 MHz SM : 1328 MHz Memory : 715 MHz Video : 1328 MHz Max Customer Boost Clocks Graphics : 1328 MHz Clock Policy Auto Boost : N/A Auto Boost Default : N/A Processes禁用本地YUM源1yum-config-manager --disable nvidia-diag-driver-local-390.116安装CUDA环境添加CUDA源1yum-config-manager --add-repo http://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo修改CUDA源地址123sed -e 's,developer.download.nvidia.cn/compute/cuda/repos/,mirrors.aliyun.com/nvidia-cuda,g' \\ -e 's,developer.download.nvidia.com/compute/cuda/repos,mirrors.aliyun.com/nvidia-cuda,g' \\ -i /etc/yum.repos.d/cuda-rhel7.repo安装CUDA12yum makecacheyum install cuda-9-0设置环境变量123echo 'export PATH=/usr/local/cuda-9.0/bin:$PATH' &gt;&gt; /etc/profile.d/cuda-9.0.shecho 'export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64:$LD_LIBRARY_PATH' &gt;&gt; /etc/profile.d/cuda-9.0.shsource /etc/profile.d/cuda-9.0.sh测试CUDA查看安装路径1ls -ld /usr/local/cuda*输出示例12lrwxrwxrwx 1 root root 8 Jan 1 11:08 /usr/local/cuda -&gt; cuda-9.0drwxr-xr-x 15 root root 4096 Jan 1 11:08 /usr/local/cuda-9.0检查CUDA版本1nvcc --version输出示例1234nvcc: NVIDIA (R) Cuda compiler driverCopyright (c) 2005-2017 NVIDIA CorporationBuilt on Fri_Sep__1_21:08:03_CDT_2017Cuda compilation tools, release 9.0, V9.0.176运行Samples以下两个测试结果都为Result=PASS，则说明CUDA安装成功deviceQuery切换目录1cd /usr/local/cuda-9.0/samples/1_Utilities/deviceQuery编译运行1make &amp;&amp; ./deviceQuery输出示例12345678910111213141516171819202122232425262728293031323334353637383940414243./deviceQuery Starting... CUDA Device Query (Runtime API) version (CUDART static linking)Detected 1 CUDA Capable device(s)Device 0: \"Tesla P100-PCIE-16GB\" CUDA Driver Version / Runtime Version 9.1 / 9.0 CUDA Capability Major/Minor version number: 6.0 Total amount of global memory: 16281 MBytes (17071734784 bytes) (56) Multiprocessors, ( 64) CUDA Cores/MP: 3584 CUDA Cores GPU Max Clock rate: 1329 MHz (1.33 GHz) Memory Clock rate: 715 Mhz Memory Bus Width: 4096-bit L2 Cache Size: 4194304 bytes Maximum Texture Dimension Size (x,y,z) 1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) Maximum Layered 1D Texture Size, (num) layers 1D=(32768), 2048 layers Maximum Layered 2D Texture Size, (num) layers 2D=(32768, 32768), 2048 layers Total amount of constant memory: 65536 bytes Total amount of shared memory per block: 49152 bytes Total number of registers available per block: 65536 Warp size: 32 Maximum number of threads per multiprocessor: 2048 Maximum number of threads per block: 1024 Max dimension size of a thread block (x,y,z): (1024, 1024, 64) Max dimension size of a grid size (x,y,z): (2147483647, 65535, 65535) Maximum memory pitch: 2147483647 bytes Texture alignment: 512 bytes Concurrent copy and kernel execution: Yes with 2 copy engine(s) Run time limit on kernels: No Integrated GPU sharing Host Memory: No Support host page-locked memory mapping: Yes Alignment requirement for Surfaces: Yes Device has ECC support: Enabled Device supports Unified Addressing (UVA): Yes Supports Cooperative Kernel Launch: Yes Supports MultiDevice Co-op Kernel Launch: Yes Device PCI Domain ID / Bus ID / location ID: 0 / 0 / 8 Compute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 9.1, CUDA Runtime Version = 9.0, NumDevs = 1Result = PASSbandwidthTest切换目录1cd /usr/local/cuda-9.0/samples/1_Utilities/bandwidthTest编译运行1make &amp;&amp; ./bandwidthTest输出示例123456789101112131415161718192021222324[CUDA Bandwidth Test] - Starting...Running on... Device 0: Tesla P100-PCIE-16GB Quick Mode Host to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(MB/s) 33554432 10731.5 Device to Host Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(MB/s) 33554432 12833.8 Device to Device Bandwidth, 1 Device(s) PINNED Memory Transfers Transfer Size (Bytes) Bandwidth(MB/s) 33554432 499641.4Result = PASSNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.禁用CUDA源1yum-config-manager --disable cuda安装cuDNN确定cuDNN下载地址方法一访问cuDNN Archive，登录之后下载方法二访问Nvidia托管在Gitlab的容器项目地址，根据自己操作系统的版本，找到对应的Dockerfile，提取cuDNN的下载地址CentOS-6CentOS-7Ubuntu-16.04Ubuntu-18.04下载cuDNN这里直接用CUDA-9.0能用到的、最新的cuDNN 7.6.0.641wget -O - https://developer.download.nvidia.cn/compute/redist/cudnn/v7.6.0/cudnn-9.0-linux-x64-v7.6.0.64.tgz | tar xz安装cuDNN123456cp cuda/include/cudnn.h /usr/local/cuda-9.0/includecp cuda/lib64/libcudnn.so.7.6.0 cuda/lib64/libcudnn_static.a /usr/local/cuda-9.0/lib64cd /usr/local/cuda-9.0/lib64ln -sv libcudnn.so.7.6.0 libcudnn.soln -sv libcudnn.so.7.6.0 libcudnn.so.7ldconfig -v验证cuDNN123git clone --depth=1 https://github.com/wilhelmguo/cudnn_samples_v7.gitcd cudnn_samples_v7/mnistCUDNNmake &amp;&amp; ./mnistCUDNN输出示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657cudnnGetVersion() : 7600 , CUDNN_VERSION from cudnn.h : 7600 (7.6.0)Host compiler version : GCC 4.8.5There are 1 CUDA capable devices on your machine :device 0 : sms 56 Capabilities 6.0, SmClock 1328.5 Mhz, MemSize (Mb) 16280, MemClock 715.0 Mhz, Ecc=1, boardGroupID=0Using device 0Testing single precisionLoading image data/one_28x28.pgmPerforming forward propagation ...Testing cudnnGetConvolutionForwardAlgorithm ...Fastest algorithm is Algo 1Testing cudnnFindConvolutionForwardAlgorithm ...^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.040256 time requiring 0 memory^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.048864 time requiring 3464 memory^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.066016 time requiring 57600 memory^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.090944 time requiring 2057744 memory^^^^ CUDNN_STATUS_SUCCESS for Algo 5: 0.115776 time requiring 203008 memoryResulting weights from Softmax:0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 Loading image data/three_28x28.pgmPerforming forward propagation ...Resulting weights from Softmax:0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 Loading image data/five_28x28.pgmPerforming forward propagation ...Resulting weights from Softmax:0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 Result of classification: 1 3 5Test passed!Testing half precision (math in single precision)Loading image data/one_28x28.pgmPerforming forward propagation ...Testing cudnnGetConvolutionForwardAlgorithm ...Fastest algorithm is Algo 1Testing cudnnFindConvolutionForwardAlgorithm ...^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.045184 time requiring 0 memory^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.051328 time requiring 3464 memory^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.067872 time requiring 28800 memory^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.087776 time requiring 2057744 memory^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.112032 time requiring 207360 memoryResulting weights from Softmax:0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001 Loading image data/three_28x28.pgmPerforming forward propagation ...Resulting weights from Softmax:0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000 Loading image data/five_28x28.pgmPerforming forward propagation ...Resulting weights from Softmax:0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006 Result of classification: 1 3 5Test passed!设置GPU手动设置开启Persistence Mode在Linux平台上，可以设置PersistenceMode让Nvidia GPU驱动在空闲的时候依然保持加载状态。此功能可以加快频繁运行短任务，避免频繁加载驱动会提高待机功耗12nvidia-persistencednvidia-smi -pm 1禁用受限模式允许非管理员运行的CUDA程序调整频率1nvidia-smi -acp 0禁用GPU BoostGPU Boost功能可以让GPU根据负载、散热、供电动态超频运行GPU Boost功能需要管理员权限才能调用，这里把权限放开给非管理员程序使用12nvidia-smi --auto-boost-permission=0nvidia-smi --auto-boost-default=0设置GPU运行频率获取GPU支持的频率设置GPU运行频率-i参数指定显卡ID12CLOCK=$(nvidia-smi -i 0 --query-supported-clocks=mem,gr --format=csv,noheader| head -n1 | awk 'BEGIN &#123; FS=\" \" &#125; ; &#123; print $1 \",\" $3 &#125;')nvidia-smi -ac $CLOCK -i 0加载Nvidia模块按需调整这里可以独立于Linux发行版的方式创建Nvidia设备文件create-nvidia-device-file创建给定编号的Nvidia设备文件unified-memory加载Nvidia Unified Memory模块modeset加载Nvidia内核模块并创建设备文件1nvidia-modprobe --unified-memory --create-nvidia-device-file=0 --modeset开机自动设置12345678cat &gt;&gt; /etc/rc.local &lt;&lt;EOFnvidia-smi -pm 1 || truenvidia-smi -acp 0 || truenvidia-smi --auto-boost-default=0 || truenvidia-smi --auto-boost-permission=0 || true#nvidia-modprobe --unified-memory --create-nvidia-device-file=0 --modeset || true#nvidia-smi -ac $(nvidia-smi -i 0 --query-supported-clocks=mem,gr --format=csv,noheader| head -n1 | awk 'BEGIN &#123; FS=\" \" &#125; ; &#123; print $1 \",\" $3 &#125;') -i 0EOF使用阿里云自动安装脚本说明阿里云在创建GPU计算型实例时，可以通过metadata的方式传入初始化脚本，实现自动安装Nvidia GPU驱动和CUDA详细流程可以看阿里云的操作文档运行脚本驱动版本390.116CUDA版本9.0.176cuDNN版本7.5.0（阿里云没提供7.6.0.64的版本，根据文档选择了最新的7.5.0）12345678910111213DRIVER_VERSION=\"390.116\"CUDA_VERSION=\"9.0.176\"CUDNN_VERSION=\"7.5.0\"IS_INSTALL_RAPIDS=\"FALSE\"IS_INSTALL_PERSEUS=\"TRUE\"INSTALL_DIR=\"/root/auto_install\"log=$&#123;INSTALL_DIR&#125;/nvidia_install.logmkdir $INSTALL_DIR &amp;&amp; cd $INSTALL_DIRscript_download_url=$(curl http://100.100.100.200/latest/meta-data/source-address | head -1)\"/opsx/ecs/linux/binary/script/auto_install.sh\"wget -t 10 --timeout=10 $script_download_urlsh $&#123;INSTALL_DIR&#125;/$&#123;auto_install_script&#125; $DRIVER_VERSION $CUDA_VERSION $CUDNN_VERSION $IS_INSTALL_PERSEUS $IS_INSTALL_RAPIDSNvidia-Docker说明安装Docker-CE Stable安装Nvidia-Docker安装docker这里以docker-ce 最新版本为例清理旧Docker包1yum remove -y docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine安装Docker依赖包1yum install -y yum-utils device-mapper-persistent-data lvm2添加Docker-CE源1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo安装Docker-CE1yum install docker-ce -y配置Docker启动参数12345678910111213141516171819mkdir -p /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\"], \"insecure-registries\": [], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\": \"3\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"data-root\": \"/var/lib/docker\", \"max-concurrent-downloads\": 10&#125;EOF设置Docker命令补全1cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/禁用Docker-CE源1yum-config-manager --disable docker-ce-stable启动Docker1systemctl enable --now docker.service检查Docker信息1docker info安装Nvidia-Docker添加YUM源12distribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | tee /etc/yum.repos.d/nvidia-docker.repo安装NVIDIA Container Toolkit1yum install -y nvidia-container-toolkit重启Docker1systemctl restart docker.service测试容器调用GPU1docker run --gpus all nvidia/cuda:9.0-base nvidia-smi输出示例12345678910111213141516171819202122232425262728Unable to find image 'nvidia/cuda:9.0-base' locally9.0-base: Pulling from nvidia/cuda976a760c94fc: Pull complete c58992f3c37b: Pull complete 0ca0e5e7f12e: Pull complete f2a274cc00ca: Pull complete 708a53113e13: Pull complete 371ddc2ca87b: Pull complete f81888eb6932: Pull complete Digest: sha256:56bfa4e0b6d923bf47a71c91b4e00b62ea251a04425598d371a5807d6ac471cbStatus: Downloaded newer image for nvidia/cuda:9.0-base +-----------------------------------------------------------------------------+| NVIDIA-SMI 390.116 Driver Version: 390.116 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 Tesla P100-PCIE... On | 00000000:00:08.0 Off | 0 || N/A 33C P0 25W / 250W | 0MiB / 16280MiB | 0% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+","categories":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/categories/Linux/"},{"name":"CUDA","slug":"Linux/CUDA","permalink":"https://luanlengli.github.io/categories/Linux/CUDA/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/tags/Linux/"},{"name":"CUDA","slug":"CUDA","permalink":"https://luanlengli.github.io/tags/CUDA/"}]},{"title":"Python pip命令行设置国内镜像源","slug":"Python-pip命令行设置国内镜像源","date":"2019-12-19T01:52:26.000Z","updated":"2019-12-19T02:05:06.573Z","comments":true,"path":"2019/12/19/Python-pip命令行设置国内镜像源.html","link":"","permalink":"https://luanlengli.github.io/2019/12/19/Python-pip命令行设置国内镜像源.html","excerpt":"","text":"说明pip是Python自带的包管理工具。国内的网络环境影响，使用默认设置进行包依赖安装会非常慢，经常卡住导致安装失败。为此特地记录下配置过程配置pip国内的PYPI源清华大学豆瓣阿里云腾讯云华为云永久生效这里以阿里云的pip镜像作为演示Windows平台【打开命令行提示符或者PowerShell】Linux/Mac平台打开终端123pip config set global.index-url='https://mirrors.aliyun.com/pypi/simple'pip config set global.timeout='120'pip config set global.trusted-host='mirrors.aliyun.com'临时使用这里以阿里云的pip镜像作为演示1pip install -i https://mirrors.aliyun.com/pypi/simple &lt;PACKAGE_NAME&gt;","categories":[{"name":"Python","slug":"Python","permalink":"https://luanlengli.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://luanlengli.github.io/tags/Python/"}]},{"title":"Kubernetes挂载NFS添加挂载选项","slug":"Kubernetes挂载NFS添加挂载选项","date":"2019-12-18T14:54:55.000Z","updated":"2019-12-20T00:58:49.015Z","comments":true,"path":"2019/12/18/Kubernetes挂载NFS添加挂载选项.html","link":"","permalink":"https://luanlengli.github.io/2019/12/18/Kubernetes挂载NFS添加挂载选项.html","excerpt":"","text":"前情概要这里引用阿里云的说明文档为什么要使用noresvport参数挂载NAS？不重新挂载会有什么后果？如果发生网络切换或者后端服务的HA倒换，小概率会造成NFS文件系统阻塞，那就可能需要几分钟时间连接才会自动恢复，极端情况下甚至需要重启ECS才能恢复。使用noresvport参数后，这个恢复几秒就可以自动完成。什么情况会引发网络切换或者后端服务的HA倒换？NAS服务是稳定的，网络切换或者后端服务的HA倒换都是罕见情况。后端服务升级会触发上述切换，但导致客户端阻塞的概率很低，并且在升级之前我们会提前通知相关集群的用户，留出充足时间使用noresvport参数。其他可能引发切换的场景，还有负载均衡调整、服务端硬件故障等情况，有一定的不可预测性，所以即使服务端没有升级安排，也请尽快使用noresvport参数避免这样的风险。为什么需要重新挂载？还有没有其他的方案？需要重新挂载的原因是要把之前没有使用noresvport参数的TCP连接断掉，然后使用noresvport参数挂载时，会建立新的TCP连接。为了把老的TCP连接断掉，就必须把NAS相关的业务都停掉，然后执行umount卸载。如果不希望重新挂载，可以考虑新建NAS挂载点，使用noresvport参数挂载到新的本地路径，然后把业务进程逐步迁移过去，最后废弃老的挂载路径和挂载点。挂载NFS静态存储卷NFS v3版12345678910111213141516apiVersion: v1kind: PersistentVolumemetadata: name: pv-nfs-v3spec: accessModes: - ReadWriteOnce capacity: storage: 2Gi mountOptions: - vers=3 - nolock,tcp,noresvport nfs: path: /nfs-v3 server: nas_server persistentVolumeReclaimPolicy: RetainNFS v4.0版12345678910111213141516apiVersion: v1kind: PersistentVolumemetadata: name: pv-nfs-v4.0spec: accessModes: - ReadWriteOnce capacity: storage: 2Gi mountOptions: - vers=4.0 - noresvport nfs: path: /nfs-v4.0 server: nas_server persistentVolumeReclaimPolicy: Retain动态存储卷假设集群已经部署了nfs-client-provisioner用来实现在动态提供PersistentVolume创建StorageClassNFS v3版123456789apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfsv3-scmountOptions:- vers=3- nolock,tcp,noresvportprovisioner: fuseim.pri/ifsreclaimPolicy: RetainNFS v4.0版123456789apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfsv4-scmountOptions:- vers=4.0- nolock,tcp,noresvportprovisioner: fuseim.pri/ifsreclaimPolicy: Retain创建PVCNFS v3版12345678910111213apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfsv3-pvc namespace: defaultspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfsv3-sc volumeMode: FilesystemNFS v4.0版12345678910111213apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfsv4-pvc namespace: defaultspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfsv4-sc volumeMode: Filesystem测试挂载PV123456789101112131415161718192021222324252627kind: PodapiVersion: v1metadata: name: test-nfs-pod namespace: defaultspec: containers: - name: test-nfs-pod image: busybox:1.24 command: - \"/bin/sh\" args: - \"-c\" - \"while true; do sleep 99999;done\" volumeMounts: - name: nfsv3-pvc mountPath: \"/mnt/nfsv3\" - name: nfsv4-pvc mountPath: \"/mnt/nfsv4\" restartPolicy: \"Never\" volumes: - name: nfsv3-pvc persistentVolumeClaim: claimName: nfsv3-pvc - name: nfsv4-pvc persistentVolumeClaim: claimName: nfsv4-pvc验证挂载选项确认Pod所在宿主机1kubectl -n default get pod test-nfs-pod -o wide登录到宿主机查看mount，关注是否有noresvport1mount | grep nfs1nas_server:/default on /var/lib/kubelet/pods/a3d10191-04f2-11ea-b668-162fb9b39758/volumes/kubernetes.io~nfs/nfsv4-pvc type nfs4 (rw,relatime,vers=4.0,rsize=1048576,wsize=1048576,namlen=255,hard,noresvport,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=192.168.0.146,local_lock=none,addr=192.168.0.128,_netdev)存量NFS存储卷更新修改NFS存储卷对应的PV，添加mount参数重新调度使用NFS存储卷的Pod注意！如果服务器上还有其他挂载点使用了同一个NFS存储，有可能无法更新挂载选项","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"如何使用Azure中国提供的容器镜像代理服务","slug":"如何使用Azure中国提供的Docker镜像代理服务","date":"2019-12-16T06:12:59.000Z","updated":"2019-12-16T06:38:59.369Z","comments":true,"path":"2019/12/16/如何使用Azure中国提供的Docker镜像代理服务.html","link":"","permalink":"https://luanlengli.github.io/2019/12/16/如何使用Azure中国提供的Docker镜像代理服务.html","excerpt":"","text":"说明由于Dockerhub、k8s.gcr.io、Quay.io这些常用的容器镜像源都在国外，在国内访问经常不稳定，特别是托管在Google的k8s.gcr.io，对新手安装Kubernetes非常不方便Azure中国提供了容器镜像代理服务，速度也算比较文档，这里记录一下配置过程项目说明：container-service-for-azure-china配置镜像源DockerHub以前Docker公司在国内是有专门的镜像服务器的，后来不知道咋的就没了。这里使用Azure中国的镜像代理原镜像地址替换为Azure中国的地址alpine:3.10dockerhub.azk8s.cn/library/alpine:3.10jenkins/jenkins:2.190.1dockerhub.azk8s.cn/jenkins/jenkins:2.190.1Docker编辑/etec/docker/daemon.json在registry-mirrors配置中添加https://dockerhub.azk8s.cn。12345&#123; ... \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\"], ...&#125;Dockerfile在FROM镜像的时候，直接使用Azure中国的源。需要注意的是，Docker代码在split的时候，如果只有xxxx:tag会拉取/library/xxxx:tag。使用Azure中国的代理服务器，在处理这样的镜像时，需要自己手动添加一下/library，否则会找不到镜像。相关issue1FROM dockerhub.azk8s.cn/library/alpine:3.10gcr.io国内用gcr.io比较少，通常都是用来下载Kubernetes的容器镜像。可以通过替换镜像地址的方式来进行下载。k8s.gcr.io会被重定向到gcr.io/google_containers。原镜像地址替换为Azure中国的地址k8s.gcr.io/pause:3.1gcr.azk8s.cn/google_containers/pause:3.1gcr.io/abc/image:1111gcr.azk8s.cn/abc/image:1111Quay.io这里Quay也可以通过替换镜像的方式来下载原镜像地址替换为Azure中国的地址quay.io/coreos/flannel:v0.11.0-amd64quay.azk8s.cn/coreos/flannel:v0.11.0-amd64Azure中国原版项目说明Container Registry ProxySince some well known container registries like docker.io, gcr.io are not accessible or very slow in China, we have set up container registry proxies on Azure China for public anonymous access:The first docker pull of new image will be still slow, and then image would be cached, would be much faster in the next docker pull action.globalproxy in Chinaformatexampledockerhub (docker.io)dockerhub.azk8s.cndockerhub.azk8s.cn//:dockerhub.azk8s.cn/microsoft/azure-cli:2.0.61 dockerhub.azk8s.cn/library/nginx:1.15gcr.iogcr.azk8s.cngcr.azk8s.cn//:gcr.azk8s.cn/google_containers/hyperkube-amd64:v1.13.5quay.ioquay.azk8s.cnquay.azk8s.cn//:quay.azk8s.cn/deis/go-dev:v1.10.0Note: k8s.gcr.io would redirect to gcr.io/google-containers, following image urls are identical:12k8s.gcr.io/pause-amd64:3.1gcr.io/google_containers/pause-amd64:3.1Container Registry Proxy Examplespecify defaultBackend.image.repository as gcr.azk8s.cn/google_containers/defaultbackend in nginx-ingress chart since original k8s.gcr.io does not work in Azure China:1helm install stable/nginx-ingress --set defaultBackend.image.repository=gcr.azk8s.cn/google_containers/defaultbackend --set defaultBackend.image.tag=1.4","categories":[{"name":"Docker","slug":"Docker","permalink":"https://luanlengli.github.io/categories/Docker/"},{"name":"Kubernetes","slug":"Docker/Kubernetes","permalink":"https://luanlengli.github.io/categories/Docker/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://luanlengli.github.io/tags/Docker/"}]},{"title":"OpenVPN客户端(Windows/Linux/MacOS)连接OpenVPN服务器","slug":"OpenVPN客户端-Windows-Linux-MacOS-连接OpenVPN服务器","date":"2019-11-25T13:20:02.000Z","updated":"2019-11-25T14:02:51.000Z","comments":true,"path":"2019/11/25/OpenVPN客户端-Windows-Linux-MacOS-连接OpenVPN服务器.html","link":"","permalink":"https://luanlengli.github.io/2019/11/25/OpenVPN客户端-Windows-Linux-MacOS-连接OpenVPN服务器.html","excerpt":"","text":"说明使用社区版的OpenVPN客户端连接社区版OpenVPN服务器OpenVPN版本2.4.8Linux版本CentOS-7.7Windows版本Windows 10 1903MacOS版本本人没有MacOS平台，这里简单参考官网的指南Windows平台下载客户端官方下载地址页面客户端选择官网提供两种Windows的客户端，根据自己的Windows版本安装即可openvpn-install-2.4.8-I602-Win7.exeopenvpn-install-2.4.8-I602-Win10.exe安装OpenVPN双击下载好的exe文件一路下一步直至安装完成获取OpenVPN配置文件OpenVPN的客户端配置文件为*.ovpn在使用证书认证的情况下，在ovpn文件同一个目录下面会有*.crt*.keyca.crt再开启了tls-auth时还会有ta.key文件证书文件可以内嵌到ovpn文件中，因此有时候会只有一个ovpn文件配置文件使用方式直接双击ovpn文件Windows版OpenVPN客户端安装完成之后，会自动关联ovpn文件，双击即可打开OpenVPN复制到配置目录OpenVPN默认会从这两个目录找配置文件，可以存在多个不同的ovpn配置C:\\Program Files\\OpenVPN\\configC:\\User\\用户名\\OpenVPN\\config（通过双击ovpn的方式会把ovpn拷贝到这个目录）连接OpenVPN服务器双击桌面的OpenVPN GUI图标在任务栏右下角通知栏中找到OpenVPN的图标，右键点击连接，在有多个ovpn配置时，可以根据名字选择不同的ovpn配置，然后点击连接连接过程会出现很多日志，连接成功后，右下角会提示连接成功开机自启动在任务栏右下角通知栏中找到OpenVPN的图标，右键点击选项在常规标签中勾选在Windows开机时启动Linux平台这里以CentOS-7.7为例，YUM源自带了OpenVPN-2.4.8安装客户端1yum install -y openvpn获取配置文件CentOS-7.7安装OpenVPN之后系统服务会识别*.conf的文件配置文件跟Windows平台只有扩展名的区别，可以直接把Windows平台的ovpn文件改名为conf文件复制到配置目录CentOS-7.7安装OpenVPN之后会在/etc/openvpn下创建client和server目录启动OpenVPN客户端服务以配置文件abc.conf为例1systemctl start openvpn-client@abc.service开机自启动1systemctl enable openvpn-client@abc.serviceMacOS平台系统要求OS X 10.8 Mountain LionOS X 10.9 MavericksOS X 10.10 YosemiteOS X 10.11 El CapitanmacOS 10.12 SierramacOS 10.13 High SierramacOS 10.14 Mojave下载客户端官网下载页面里面有几个客户端可以选择OpenVPN Connect Client for macOS version 2.7.1.100OpenVPN Connect for macOS version 3.1.0 (885) betaTunnelblick安装客户端这里用Tunnelblick为例双击下载好的dmg文件一路同意、下一步、安装配置方式参考gitbook上面的教程","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/tags/Linux/"},{"name":"Windows","slug":"Windows","permalink":"https://luanlengli.github.io/tags/Windows/"},{"name":"OpenVPN","slug":"OpenVPN","permalink":"https://luanlengli.github.io/tags/OpenVPN/"},{"name":"MacOS","slug":"MacOS","permalink":"https://luanlengli.github.io/tags/MacOS/"}]},{"title":"CentOS7部署OpenVPN实现内网互通","slug":"CentOS7部署OpenVPN实现内网互通","date":"2019-11-25T01:17:46.000Z","updated":"2019-12-24T01:16:06.787Z","comments":true,"path":"2019/11/25/CentOS7部署OpenVPN实现内网互通.html","link":"","permalink":"https://luanlengli.github.io/2019/11/25/CentOS7部署OpenVPN实现内网互通.html","excerpt":"","text":"说明此法用于多个客户端通过OpenVPN服务器实现内网访问OpenVPN服务器操作系统CentOS-7.7OpenVPN版本2.4.8easy-rsa版本3.0.6使用tap模式客户端IP地址池10.8.0.0/24多个客户端直接可以通过OpenVPN实现内网通信准备操作添加EPEL源1yum install epel-release -y替换为阿里云的源12345sed -e 's,^#baseurl,baseurl,g' \\ -e 's,^metalink,#metalink,g' \\ -e 's,^mirrorlist=,#mirrorlist=,g' \\ -e 's,http://download.fedoraproject.org/pub,https://mirrors.aliyun.com,g' \\ -i /etc/yum.repos.d/epel.repo更新软件12yum makecacheyum update -y修改sysctl参数123456789101112131415161718192021cat &gt; /etc/sysctl.d/99-net.conf &lt;&lt;EOF# 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤net.bridge.bridge-nf-call-arptables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# 关闭严格校验数据包的反向路径，默认值1net.ipv4.conf.default.rp_filter=0net.ipv4.conf.all.rp_filter=0# 设置 conntrack 的上限net.netfilter.nf_conntrack_max=1048576# 端口最大的监听队列的长度net.core.somaxconn=21644# TCP阻塞控制算法BBR，Linux内核版本4.9开始内置BBR算法#net.ipv4.tcp_congestion_control=bbr#net.core.default_qdisc=fq# 打开ipv4数据包转发net.ipv4.ip_forward=1# TCP FastOpen# 0:关闭 ; 1:作为客户端时使用 ; 2:作为服务器端时使用 ; 3:无论作为客户端还是服务器端都使用net.ipv4.tcp_fastopen=3EOF修改limits参数1234cat &gt; /etc/security/limits.d/99-centos.conf &lt;&lt;EOF* - nproc 1048576* - nofile 1048576EOF安装OpenVPN软件1yum install -y openvpn easy-rsa lrzsz iptables-services配置服务器证书复制文件拷贝easy-rsa的文件到/etc/openvpn下12cp -r /usr/share/easy-rsa/3.0.6 /etc/openvpn/easy-rsacp /usr/share/doc/easy-rsa-3.0.6/vars.example /etc/openvpn/easy-rsa/vars修改vars编辑vars文件1vim /etc/openvpn/easy-rsa/vars修改以下几项123456789101112#set_var EASYRSA_REQ_COUNTRY \"US\"#set_var EASYRSA_REQ_PROVINCE \"California\"#set_var EASYRSA_REQ_CITY \"San Francisco\"#set_var EASYRSA_REQ_ORG \"Copyleft Certificate Co\"#set_var EASYRSA_REQ_EMAIL \"me@example.net\"#set_var EASYRSA_REQ_OU \"My Organizational Unit\"#set_var EASYRSA_KEY_SIZE 4096#set_var EASYRSA_ALGO rsa#set_var EASYRSA_CA_EXPIRE 3650#set_var EASYRSA_CERT_EXPIRE 365#set_var EASYRSA_CERT_RENEW 180#set_var EASYRSA_CRL_DAYS 60初始化PKI和CA切换目录1cd /etc/openvpn/easy-rsa创建PKI1./easyrsa init-pki创建CA1./easyrsa build-ca nopass创建服务器证书1./easyrsa build-server-full openvpn-server nopass创建DH证书1./easyrsa gen-dh拷贝证书12345678mkdir -p /etc/openvpn/pkicp /etc/openvpn/easy-rsa/pki/ca.crt \\ /etc/openvpn/easy-rsa/pki/dh.pem \\ /etc/openvpn/easy-rsa/pki/issued/openvpn-server.crt \\ /etc/openvpn/easy-rsa/pki/private/openvpn-server.key \\ /etc/openvpn/pki/ln -sv /etc/openvpn/easy-rsa/pki/crl.pem /etc/openvpn/pki/crl.pemchown -R root:openvpn /etc/openvpn/pki创建ta.key1openvpn --genkey --secret /etc/openvpn/pki/ta.key配置OpenVPN创建日志目录12mkdir -p /var/log/openvpnchown -R openvpn:openvpn /var/log/openvpn创建客户端配置目录12mkdir -p /etc/openvpn/client/&#123;config,user&#125;chown -R root:openvpn /etc/openvpn/client/&#123;config,user&#125;配置OpenVPN服务器端1vim /etc/openvpn/server/srv.conf下面内容按需更改123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# 监听地址#local 0.0.0.0# 监听端口port 51194# 通信协议proto tcp# TUN模式还是TAP模式dev tap# 证书ca /etc/openvpn/pki/ca.crtcert /etc/openvpn/pki/openvpn-server.crtkey /etc/openvpn/pki/openvpn-server.keydh /etc/openvpn/pki/dh.pemcrl-verify /etc/openvpn/pki/crl.pem# 禁用OpenVPN自定义缓冲区大小，由操作系统控制sndbuf 0rcvbuf 0# TLS rules “client” | “server”#remote-cert-tls \"client\"# TLS认证tls-auth /etc/openvpn/pki/ta.key# TLS最小版本#tls-version-min \"1.2\"# 重新协商数据交换的key，默认3600#reneg-sec 3600# 在此文件中维护客户端与虚拟IP地址之间的关联记录# 如果OpenVPN重启，重新连接的客户端可以被分配到先前分配的虚拟IP地址ifconfig-pool-persist /etc/openvpn/ipp.txt# 配置client配置文件client-config-dir /etc/openvpn/client/config# 该网段为 open VPN 虚拟网卡网段，不要和内网网段冲突即可。server 10.8.0.0 255.255.255.0# 配置网桥模式，需要在OpenVPN服务添加启动关闭脚本，将tap设备桥接到物理网口# 假定内网地址为192.168.0.0/24，内网网关是192.168.0.1# 分配192.168.0.200-250给VPN使用#server-bridge 192.168.0.1 255.255.255.0 192.168.0.200 192.168.0.250# 给客户端推送自定义路由#push \"route 192.168.0.0 255.255.255.0\"# 所有客户端的默认网关都将重定向到VPN#push \"redirect-gateway def1 bypass-dhcp\" # 向客户端推送DNS配置#push \"dhcp-option DNS 223.5.5.5\"#push \"dhcp-option DNS 223.6.6.6\"# 允许客户端之间互相访问client-to-client# 限制最大客户端数量#max-clients 10# 客户端连接时运行脚本#client-connect ovpns.script# 客户端断开连接时运行脚本#client-disconnect ovpns.script# 保持连接时间keepalive 20 120# 开启vpn压缩comp-lzo# 允许多人使用同一个证书连接VPN，不建议使用，注释状态#duplicate-cn# 运行用户user openvpn#运行组group openvpn# 持久化选项可以尽量避免访问那些在重启之后由于用户权限降低而无法访问的某些资源persist-keypersist-tun# 显示当前的连接状态status /var/log/openvpn/openvpn-status.log# 日志路径，不指定文件路径时输出到控制台# log代表每次启动时清空日志文件#log /var/log/openvpn/openvpn.log# log-append代表追加写入到日志文件#log-append /var/log/openvpn/openvpn.log# 日志级别verb 4# 忽略过多的重复信息，相同类别的信息只有前20条会输出到日志文件中mute 20启动OpenVPN-server123chown -R root:openvpn /etc/openvpnsystemctl enable openvpn-server@srv.servicesystemctl start openvpn-server@srv.service添加SNAT访问内网让10.8.0.0/24访问192.168.0.0/24的时候做SNAT，源地址改成OpenVPN服务器的内网地址1iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -d 192.168.0.0/24 -j SNAT --to-source OPENVPN_SERVER_IP客户端管理客户端配置模板1vim /etc/openvpn/templates/ovpn.template1234567891011121314151617181920212223242526272829303132# 配置为客户端模式client# 与服务器端保持一致proto tcp# 与服务器端保持一致dev tap# 配置服务器端的地址和端口remote vpn-server 51194resolv-retry infinitenobindpersist-keypersist-tuncomp-lzonice 0verb 3mute 10# 禁用OpenVPN自定义缓冲区大小，由操作系统控制sndbuf 0rcvbuf 0# 禁止在内存中缓存passwordauth-nocache# TLS rules “client” | “server”#remote-cert-tls server# 过滤从服务器端发过来的路由规则#pull-filter ignore redirect-gateway# 不从服务器端拉取路由规则#route nopull# 手动指定所有流量走VPN#redirect-gateway def1# 客户端自定义路由，例如192.168.0.0/24走VPN，192.168.1.0/24不走VPN#route 192.168.0.0 255.255.255.0 vpn_gateway#route 192.168.1.0 255.255.255.0 net_gateway指定客户端推送配置模板1vim /etc/openvpn/templates/ipconfig_push.template12345678# 给客户端推送固定的IP地址ifconfig-push #IP地址# 255.255.255.0# 给客户端推送路由iroute 192.168.0.0 255.255.0.0# 为客户端开启压缩comp-lzo yes# 推送客户端配置push \"comp-lzo yes\"客户端配置这里的客户端名字指的是证书的Common Name，这里以user1为例1vim /etc/openvpn/client/config/user112# 给客户端推送固定的IP地址10.8.0.100ifconfig-push 10.8.0.100 255.255.255.0用户管理脚本12vim /usr/local/bin/ovpn_mgmt.shchmod +x /usr/local/bin/ovpn_mgmt.sh123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140#!/bin/bashset -e# 定义环境变量export EASY_RSA_DIR=\"/etc/openvpn/easy-rsa\"export CLIENT_CONFIG_DIR=\"/etc/openvpn/client/config\"export PKI_DIR=\"$&#123;EASY_RSA_DIR&#125;/pki\"export TEMPLATES_DIR=\"/etc/openvpn/templates\"export CLIENT_USER_DIR=\"/etc/openvpn/client/user\"# 创建ovpn配置generate_ovpn() &#123; USER_OVPN=\"$&#123;CLIENT_USER_DIR&#125;/$&#123;EXPIRED&#125;-$&#123;USER&#125;.ovpn\" # 根据证书生成对应的ovpn文件 mkdir -p $&#123;CLIENT_USER_DIR&#125;/ cp -f $&#123;TEMPLATES_DIR&#125;/ovpn.template $&#123;USER_OVPN&#125; echo \"&lt;ca&gt;\" &gt;&gt; $&#123;USER_OVPN&#125; cat $&#123;PKI_DIR&#125;/ca.crt &gt;&gt; $&#123;USER_OVPN&#125; echo \"&lt;/ca&gt;\" &gt;&gt; $&#123;USER_OVPN&#125; echo \"&lt;cert&gt;\" &gt;&gt; $&#123;USER_OVPN&#125; cat $&#123;PKI_DIR&#125;/issued/$&#123;USER&#125;.crt &gt;&gt; $&#123;USER_OVPN&#125; echo \"&lt;/cert&gt;\" &gt;&gt; $&#123;USER_OVPN&#125; echo \"&lt;key&gt;\" &gt;&gt; $&#123;USER_OVPN&#125; cat $&#123;PKI_DIR&#125;/private/$&#123;USER&#125;.key &gt;&gt; $&#123;USER_OVPN&#125; echo \"&lt;/key&gt;\" &gt;&gt; $&#123;USER_OVPN&#125; echo \"&lt;tls-auth&gt;\" &gt;&gt; $&#123;USER_OVPN&#125; cat /etc/openvpn/pki/ta.key &gt;&gt; $&#123;USER_OVPN&#125; echo \"&lt;/tls-auth&gt;\" &gt;&gt; $&#123;USER_OVPN&#125; sz --binary $&#123;USER_OVPN&#125;&#125;# 添加客户端IPclient_ip_config() &#123; # 根据ip地址生成对应的client-config sed -e \"s,#ipaddress#,$&#123;IPADDRESS&#125;,g\" \\ $&#123;TEMPLATES_DIR&#125;/ipconfig_push.template \\ &gt; $&#123;CLIENT_CONFIG_DIR&#125;/$&#123;USER&#125;&#125;# 生成客户端证书add_cert() &#123; # 切换到easy-rsa目录 cd $&#123;EASY_RSA_DIR&#125; $&#123;EASY_RSA_DIR&#125;/easyrsa build-client-full $&#123;USER&#125; nopass&#125;# 注销客户端证书revoke_cert() &#123; # 切换到easy-rsa目录 cd $&#123;EASY_RSA_DIR&#125; echo \"yes\" | $&#123;EASY_RSA_DIR&#125;/easyrsa revoke $&#123;USER&#125; $&#123;EASY_RSA_DIR&#125;/easyrsa gen-crl&#125;# 更新客户端证书renew_cert() &#123; # 切换到easy-rsa目录 cd $&#123;EASY_RSA_DIR&#125; echo \"yes\" | $&#123;EASY_RSA_DIR&#125;/easyrsa renew $&#123;USER&#125; nopass&#125;# 检查证书过期时间check_cert_expired() &#123; export EXPIRED=$(date --date=\"$(openssl x509 -enddate -noout -in $&#123;PKI_DIR&#125;/issued/$&#123;USER&#125;.crt |cut -d= -f 2)\" --iso-8601)&#125;# 创建用户add_user() &#123; if [[ -e \"$&#123;EASY_RSA_DIR&#125;/pki/reqs/$&#123;USER&#125;.req\" ]];then read -p\"此用户已经申请证书，是否重新生成？[y/n]：\" ANSWER case $&#123;ANSWER&#125; in y|Y) revoke_cert check_cert_expired client_ip_config add_cert generate_ovpn exit 0 ;; n|N) check_cert_expired client_ip_config generate_ovpn ;; esac else add_cert check_cert_expired client_ip_config generate_ovpn fi&#125;# 删除用户del_user() &#123; revoke_cert rm -rf $&#123;CLIENT_USER_DIR&#125;/*$&#123;USER&#125;.ovpn rm -rf $&#123;CLIENT_CONFIG_DIR&#125;/$&#123;USER&#125;&#125;# 用户旧证过期重新生成证书renew_user() &#123; if [[ -e \"$&#123;EASY_RSA_DIR&#125;/pki/reqs/$&#123;USER&#125;.req\" ]];then renew_cert check_cert_expired generate_ovpn else echo \"用户证书不存在！\" exit 1 fi&#125;main() &#123; # 切换到easy-rsa目录 cd $&#123;EASY_RSA_DIR&#125; # 根据传入的method运行对应函数 $&#123;METHOD&#125;&#125;# 获取参数while getopts 'm:u:i:r' OPT;do case $OPT in u) USER=\"$OPTARG\" echo \"USER=$&#123;USER&#125;\" ;; i) IPADDRESS=\"$OPTARG\" echo \"IPADDRESS=$&#123;IPADDRESS&#125;\" ;; m) METHOD=\"$OPTARG\" echo \"METHOD=$&#123;METHOD&#125;\" ;; ?) echo \"Usage: $(base \\\"$0\\\") -m [add_user|del_user|renew_user] -u USER -i IPADDRESS\" exit 0 ;; esacdonemain添加用户1/usr/local/bin/ovpn_mgmt.sh -m add_user -u USERNAME -i 10.8.0.10删除用户1/usr/local/bin/ovpn_mgmt.sh -m del_user -u USERNAME","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/tags/Linux/"},{"name":"OpenVPN","slug":"OpenVPN","permalink":"https://luanlengli.github.io/tags/OpenVPN/"}]},{"title":"Kubernetes集群利用RBAC+ServiceAccount管理权限","slug":"Kubernetes集群利用RBAC+ServiceAccount管理权限","date":"2019-08-17T08:31:24.000Z","updated":"2019-11-28T03:17:14.000Z","comments":true,"path":"2019/08/17/Kubernetes集群利用RBAC+ServiceAccount管理权限.html","link":"","permalink":"https://luanlengli.github.io/2019/08/17/Kubernetes集群利用RBAC+ServiceAccount管理权限.html","excerpt":"","text":"说明这里简单记录一下如何通过k8s集群创建clusterrole配合serviceaccount实现权限控制，并且使用kubectl生成kubeconfig文件用于实现人员权限管理。操作步骤创建ClusterRole这里参考了kube-apiserver初始化时生成的clusterrole.views权限设定123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: cluster-readonlyaggregationRule: clusterRoleSelectors: - matchLabels: rbac.example.com/aggregate-to-monitoring: \"true\"rules:- apiGroups: - \"\" resources: - configmaps - endpoints - nodes - nodes/metrics - nodes/proxy - nodes/status - persistentvolumeclaims - pods - replicationcontrollers - replicationcontrollers/scale - serviceaccounts - services verbs: - get - list - watch- apiGroups: - \"\" resources: - bindings - events - limitranges - namespaces/status - pods/log - pods/status - replicationcontrollers/status - resourcequotas - resourcequotas/status verbs: - get - list - watch- apiGroups: - \"\" resources: - namespaces verbs: - get - list - watch- apiGroups: - apps resources: - controllerrevisions - daemonsets - deployments - deployments/scale - replicasets - replicasets/scale - statefulsets - statefulsets/scale verbs: - get - list - watch- apiGroups: - autoscaling resources: - horizontalpodautoscalers verbs: - get - list - watch- apiGroups: - batch resources: - cronjobs - jobs verbs: - get - list - watch- apiGroups: - extensions resources: - daemonsets - deployments - deployments/scale - ingresses - networkpolicies - replicasets - replicasets/scale - replicationcontrollers/scale verbs: - get - list - watch- apiGroups: - policy resources: - poddisruptionbudgets verbs: - get - list - watch- apiGroups: - networking.k8s.io resources: - networkpolicies verbs: - get - list - watch- apiGroups: - metrics.k8s.io resources: - nodes - pods verbs: - get - list - watch- apiGroups: - monitoring.coreos.com resources: - alertmanagers - podmonitors - prometheuses - prometheusrules - servicemonitors verbs: - get - list - watch- nonResourceURLs: - /metrics - /log - /logs - /healthz - /healthz/* verbs: - get如果需要有Pod的exec权限可以创建一个ClusterRole1234567891011apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: pod-execrules:- apiGroups: - \"\" resources: - pods/exec verbs: - create创建用户12345apiVersion: v1kind: ServiceAccountmetadata: name: cluster-readonly namespace: default创建ClusterRoleBinding这里创建的ClusterRoleBinding是整个集群级别的1234567891011121314apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: cluster-readonly annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\"roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-readonlysubjects:- kind: ServiceAccount name: cluster-readonly namespace: default创建RoleBinding这里创建的RoleBinding可以具体到某一个namespace这里创建一个default命名空间的RoleBinding12345678910111213apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: default-pod-exec namespace: defaultroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: pod-execsubjects:- kind: ServiceAccount name: cluster-readonly namespace: kube-system获取ServiceAccount对应的Secret1SECRET=$(kubectl -n default get serviceaccount cluster-readonly -o go-template='&#123;&#123;range .secrets&#125;&#125;&#123;&#123;.name&#125;&#125;&#123;&#123;end&#125;&#125;')定义Kube-APIServer1API_SERVER=\"https://192.168.1.100:6443\"获取集群CA证书1kubectl -n default get secret $&#123;SECRET&#125; -o yaml | awk '/ca.crt:/&#123;print $2&#125;' | base64 -d &gt; ca.crt获取ServiceAccount对应的Token1TOKEN=$(kubectl -n default get secret $&#123;SECRET&#125; -o go-template='&#123;&#123;.data.token&#125;&#125;')定义kubeconfig文件名1KUBECONFIG=\"cluster-readonly.kubeconfig\"创建kubeconfig设置kubeconfig集群信息12345kubectl config \\ set-cluster k8s-cluster \\ --server=$&#123;API_SERVER&#125; \\ --embed-certs=true \\ --certificate-authority=./ca.crt设置kubeconfig使用用户名+token认证1234kubectl config \\ set-credentials cluster-readonly \\ --token=`echo $&#123;TOKEN&#125; | base64 -d` \\ --kubeconfig=$&#123;KUBECONFIG&#125;设置kubeconfig的context12345kubectl config \\ set-context default \\ --cluster=k8s-cluster \\ --user=cluster-readonly \\ --kubeconfig=$&#123;KUBECONFIG&#125;将context设置为kubeconfig默认值123kubectl config \\ use-context default \\ --kubeconfig=$&#123;KUBECONFIG&#125;验证权限有权限的操作获取Pod1kubectl --kubeconfig=$&#123;KUBECONFIG&#125; auth can-i get pod --all-namespaces获取Pod日志1kubectl --kubeconfig=$&#123;KUBECONFIG&#125; auth can-i get pod--subresource=log --all-namespaces获取ingress1kubectl --kubeconfig=$&#123;KUBECONFIG&#125; auth can-i get ingress --all-namespaces返回结果1yes无权限的操作创建容器1kubectl --kubeconfig=$&#123;KUBECONFIG&#125; auth can-i create pods --all-namespaces删除容器1kubectl --kubeconfig=$&#123;KUBECONFIG&#125; auth can-i delete pods --all-namespaces获取Secret1kubectl --kubeconfig=$&#123;KUBECONFIG&#125; auth can-i get secrets -n kube-system返回结果1no - no RBAC policy matched","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[]},{"title":"Kubernetes集群使用ECK部署elasticsearch和kibana","slug":"Kubernetes集群使用ECK部署elasticsearch和kibana","date":"2019-08-03T01:33:50.000Z","updated":"2019-08-07T03:17:34.000Z","comments":true,"path":"2019/08/03/Kubernetes集群使用ECK部署elasticsearch和kibana.html","link":"","permalink":"https://luanlengli.github.io/2019/08/03/Kubernetes集群使用ECK部署elasticsearch和kibana.html","excerpt":"","text":"说明仅记录操作过程和部署过程请考虑再三之后再决定是否上生产环境！操作系统使用的CentOS-7.6.1810 x86_64Kubernetes集群版本v1.14.4elastic-operator版本为0.9.0elasticsearch和kibana版本为7.2.0elasticsearch和kibana默认提供HTTPS，其中elasticsearch无法关闭HTTPS参考文档Elastic Cloud on Kubernetes QuickstartElastic Cloud on k8s design项目地址cloud-on-k8s准备环境Kubernetes集群kubectl版本v1.11+kubectl当前content拥有cluster-admin权限准备存储出于测试目的，本文使用emptyDir另外还可以使用volumeClaimTemplates作为持久化数据存储部署ECK部署elastic-operator、CRD资源、RBAC1kubectl apply -f https://download.elastic.co/downloads/eck/0.9.0/all-in-one.yaml检查部署情况1kubectl -n elastic-system get pod输出示例12NAME READY STATUS RESTARTS AGEelastic-operator-0 1/1 Running 1 50m部署elasticsearch编辑YAML文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394apiVersion: elasticsearch.k8s.elastic.co/v1alpha1kind: Elasticsearchmetadata: name: elasticsearc-demo namespace: defaultspec: version: 7.2.0 image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 updateStrategy: changeBudget: maxSurge: 1 maxUnavailable: 1 # groups: # - selector: # matchLabels: # nodesGroup: group-a # - selector: # matchLabels: # nodesGroup: group-b http: service: spec: type: ClusterIP tls: # certificate: # secretName: my-cert selfSignedCertificate: # subjectAltNames: # - ip: 160.46.176.15 # - dns: hulk.example.com disabled: false nodes: # - nodeCount: 10 # config: # node.master: false # node.data: true # node.ingest: true # node.ml: true # cluster.remote.connect: false - nodeCount: 2 config: node.master: true node.data: true node.ingest: true setVmMaxMapCount: true# volumeClaimTemplates:# - metadata:# name: elasticsearch-data# spec:# accessModes:# - ReadWriteOnce# storageClassName: cephfs# resources:# requests:# storage: 10Gi podTemplate: spec: # affinity: # nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: failure-domain.beta.kubernetes.io/zone # operator: In # values: # - abc # initContainers: # 使用initContainers安装elasticsearch插件 # - name: install-plugins # command: # - \"sh\" # - \"-c\" # - | # bin/elasticsearch-plugin install --batch analysis-icu containers: - name: elasticsearch env: - name: ES_JAVA_OPTS value: -Xms2G -Xmx2G resources: requests: memory: 4Gi limits: memory: 4Gi volumeMounts: - name: timezone-volume mountPath: /etc/localtime readOnly: true volumes: - name: timezone-volume hostPath: path: /usr/share/zoneinfo/Asia/Shanghai - name: elasticsearch-data emptyDir: &#123;&#125;部署YAML1kubectl apply -f elasticsearch.yaml部署kibana编辑YAML文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455apiVersion: kibana.k8s.elastic.co/v1alpha1kind: Kibanametadata: name: kibana-demo namespace: defaultspec: version: 7.2.0 image: docker.elastic.co/kibana/kibana:7.2.0 updateStrategy: changeBudget: maxSurge: 1 maxUnavailable: 1 # groups: # - selector: # matchLabels: # nodesGroup: group-a # - selector: # matchLabels: # nodesGroup: group-b nodeCount: 1 http: service: spec: type: ClusterIP tls: # certificate: # secretName: my-cert selfSignedCertificate: # subjectAltNames: # - ip: 160.46.176.15 # - dns: hulk.example.com disabled: false elasticsearchRef: name: elasticsearc-demo namespace: default podTemplate: spec: containers: - name: kibana env: - name: I18N_LOCALE value: zh-CN resources: requests: memory: 1Gi limits: memory: 2Gi volumeMounts: - name: timezone-volume mountPath: /etc/localtime readOnly: true volumes: - name: timezone-volume hostPath: path: /usr/share/zoneinfo/Asia/Shanghai部署YAML1kubectl apply -f kibana.yaml验证和访问服务查看elasticsearch查看Pod1kubectl get pod -l common.k8s.elastic.co/type=elasticsearch输出示例123NAME READY STATUS RESTARTS AGEelasticsearc-demo-es-6vpj7k9qxk 1/1 Running 1 38melasticsearc-demo-es-x8nzjksw84 1/1 Running 1 38m查看集群状态1kubectl get elasticsearch输出示例12NAME HEALTH NODES VERSION PHASE AGEelasticsearc-demo green 2 7.2.0 Operational 34m查看secret1kubectl get secret -l common.k8s.elastic.co/type=elasticsearch输出示例12345678NAME TYPE DATA AGEelasticsearc-demo-es-elastic-user Opaque 1 38melasticsearc-demo-es-http-ca-internal Opaque 2 38melasticsearc-demo-es-http-certs-internal Opaque 2 38melasticsearc-demo-es-internal-users Opaque 3 38melasticsearc-demo-es-transport-ca-internal Opaque 2 38melasticsearc-demo-es-transport-certs-public Opaque 1 38melasticsearc-demo-es-xpack-file-realm Opaque 3 38m查看kibana查看Pod1kubectl get pod -l common.k8s.elastic.co/type=kibana输出示例12NAME READY STATUS RESTARTS AGEkibana-demo-kb-77cc75688f-lwqnk 1/1 Running 0 37m查看集群状态1kubectl get kibana输出示例12NAME HEALTH NODES VERSION AGEkibana-demo green 1 7.2.0 35m查看secret1kubectl get secret -l common.k8s.elastic.co/type=kibana输出示例12345NAME TYPE DATA AGEkibana-demo-kb-es-ca Opaque 1 40mkibana-demo-kb-http-ca-internal Opaque 2 37mkibana-demo-kb-http-certs-internal Opaque 2 37mkibana-demo-kibana-user Opaque 1 40m获取用户名密码默认用户名是elastic访问elasticsearch和kibana的密码存放在elasticsearc-demo-es-elastic-user可以通过以下方式获取12ELASTIC_PASSWORD=$(kubectl get secrets elasticsearc-demo-es-elastic-user -o=jsonpath='&#123;.data.elastic&#125;' | base64 --decode)echo $ELASTIC_PASSWORD访问服务获取svcelasticsearch1kubectl get svc -l common.k8s.elastic.co/type=elasticsearch输出示例12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEelasticsearc-demo-es-http ClusterIP 10.96.217.252 &lt;none&gt; 9200/TCP 43mkibana1kubectl get svc -l common.k8s.elastic.co/type=kibana输出示例12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkibana-demo-kb-http ClusterIP 10.96.102.185 &lt;none&gt; 5601/TCP 44m访问elasticsearch1curl -k -u \"elastic:$&#123;ELASTIC_PASSWORD&#125;\" \"http://10.96.217.252:9200\"输出示例1234567891011121314151617&#123; \"name\" : \"elasticsearc-demo-es-x8nzjksw84\", \"cluster_name\" : \"elasticsearc-demo\", \"cluster_uuid\" : \"U8xgFU1NSB6o8qmKYgYyxw\", \"version\" : &#123; \"number\" : \"7.2.0\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"508c38a\", \"build_date\" : \"2019-06-20T15:54:18.811730Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.0.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" &#125;, \"tagline\" : \"You Know, for Search\"&#125;访问kibana修改svc为NodePort这种做法比较low，建议走Ingress Controller1kubectl patch svc kibana-demo-kb-http -p '&#123;\"spec\":&#123;\"type\":\"NodePort\"&#125;&#125;'获取端口1kubectl get svc -l common.k8s.elastic.co/type=kibana示例输出12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkibana-demo-kb-http NodePort 10.96.102.185 &lt;none&gt; 5601:31364/TCP 47m浏览器访问https://k8s-master:31364登录用户名：elastic密码：上面访问elasticsearch时的密码创建ingress规则1234567891011121314151617181920212223242526272829303132333435apiVersion: extensions/v1beta1kind: Ingressmetadata: name: elasticsearch-ingress namespace: default annotations: nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"spec: rules: - host: elasticsearch.default.cluster.local http: paths: - path: / backend: serviceName: elasticsearch-demo-es-http servicePort: 9200---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: kibana-ingress namespace: default annotations: nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"spec: rules: - host: kibana.default.cluster.local http: paths: - path: / backend: serviceName: kibana-demo-kb-http servicePort: 5601","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://luanlengli.github.io/categories/Elasticsearch/"},{"name":"Kubernetes","slug":"Elasticsearch/Kubernetes","permalink":"https://luanlengli.github.io/categories/Elasticsearch/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"},{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://luanlengli.github.io/tags/Elasticsearch/"}]},{"title":"Kubernetes部署TiDB数据库集群","slug":"Kubernetes部署TiDB数据库集群","date":"2019-07-29T06:29:58.000Z","updated":"2019-08-04T03:47:24.000Z","comments":true,"path":"2019/07/29/Kubernetes部署TiDB数据库集群.html","link":"","permalink":"https://luanlengli.github.io/2019/07/29/Kubernetes部署TiDB数据库集群.html","excerpt":"","text":"说明仅记录操作过程和部署过程操作系统使用的CentOS-7.6.1810 x86_64虚拟机配置4CPU 8G内存 30G系统盘 20G数据盘A 5G数据盘BKubernetes集群版本v1.14.4使用本地PV作为数据存储TiDB-Operator版本v1.0.0TiDB组件版本v3.0.1服务器拓扑服务器IP部署实例172.16.80.201TiKv*1 TiDB*1 PD*1172.16.80.202TiKv*1 TiDB*1 PD*1172.16.80.203TiKv*1 TiDB*1 PD*1准备工作安装Helm二进制安装123wget -O - https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz | tar xz linux-amd64/helmmv linux-amd64/helm /usr/local/bin/helmrm -rf linux-amd64创建RBAC12345678910111213141516171819202122cat &lt;&lt; EOF | kubectl apply -f -# 创建名为tiller的ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---# 给tiller绑定cluster-admin权限apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tiller-cluster-ruleroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: tiller namespace: kube-systemEOF安装Helm服务端123helm init --tiller-image gcr.azk8s.cn/google_containers/tiller:v2.14.1 \\ --service-account tiller \\ --stable-repo-url http://mirror.azure.cn/kubernetes/charts/检查部署结果查看Pod状态1kubectl -n kube-system get pod -l app=helm,name=tiller输出示例12NAME READY STATUS RESTARTS AGEtiller-deploy-84fc6cd5f9-nz4m7 1/1 Running 0 1m查看Helm版本信息1helm version输出示例12Client: &amp;version.Version&#123;SemVer:\"v2.14.1\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.14.1\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;添加Helm Repo1helm repo add pingcap https://charts.pingcap.org/更新helm缓存1helm repo update查看TiDB-Operator版本1helm search pingcap -l输出示例12345678910111213NAME CHART VERSION APP VERSION DESCRIPTION pingcap/tidb-backup v1.0.0 A Helm chart for TiDB Backup or Restorepingcap/tidb-backup v1.0.0-rc.1 A Helm chart for TiDB Backup or Restorepingcap/tidb-backup v1.0.0-beta.3 A Helm chart for TiDB Backup or Restorepingcap/tidb-backup v1.0.0-beta.2 A Helm chart for TiDB Backup or Restorepingcap/tidb-cluster v1.0.0 A Helm chart for TiDB Cluster pingcap/tidb-cluster v1.0.0-rc.1 A Helm chart for TiDB Cluster pingcap/tidb-cluster v1.0.0-beta.3 A Helm chart for TiDB Cluster pingcap/tidb-cluster v1.0.0-beta.2 A Helm chart for TiDB Cluster pingcap/tidb-operator v1.0.0 tidb-operator Helm chart for Kubernetespingcap/tidb-operator v1.0.0-rc.1 tidb-operator Helm chart for Kubernetespingcap/tidb-operator v1.0.0-beta.3 tidb-operator Helm chart for Kubernetespingcap/tidb-operator v1.0.0-beta.2 tidb-operator Helm chart for Kubernetes配置本地PV参考【Kubernetes创建本地PV】修改ext4挂载选项为defaults,nodelalloc,noatime示例如下1UUID=f8727d20-3ef9-4f83-b865-25943bc342a6 /mnt/disks/f8727d20-3ef9-4f83-b865-25943bc342a6 ext4 defaults,nodelalloc,noatime 0 2创建CRD1kubectl apply -f https://raw.githubusercontent.com/pingcap/tidb-operator/master/manifests/crd.yaml部署TiDB-Operator创建工作目录1mkdir -p /home/TiDB下载TiDB-Operator Chart包12cd /home/TiDBhelm fetch pingcap/tidb-operator --version=v1.0.0解压Chart包1tar xzf tidb-operator-v1.0.0.tgz编辑values.yaml1vim tidb-operator/values.yaml修改如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# Default values for tidb-operator# clusterScoped is whether tidb-operator should manage kubernetes cluster wide tidb clusters# Also see rbac.create and controllerManager.serviceAccountclusterScoped: true# Also see clusterScoped and controllerManager.serviceAccountrbac: create: true# operatorImage is TiDB Operator imageoperatorImage: pingcap/tidb-operator:v1.0.0imagePullPolicy: IfNotPresentdefaultStorageClassName: local-storagecontrollerManager: # With rbac.create=false, the user is responsible for creating this account # With rbac.create=true, this service account will be created # Also see rbac.create and clusterScoped serviceAccount: tidb-controller-manager logLevel: 2 replicas: 1 resources: limits: cpu: 250m memory: 150Mi requests: cpu: 80m memory: 50Mi # autoFailover is whether tidb-operator should auto failover when failure occurs autoFailover: true # pd failover period default(5m) pdFailoverPeriod: 5m # tikv failover period default(5m) tikvFailoverPeriod: 5m # tidb failover period default(5m) tidbFailoverPeriod: 5mscheduler: # With rbac.create=false, the user is responsible for creating this account # With rbac.create=true, this service account will be created # Also see rbac.create and clusterScoped serviceAccount: tidb-scheduler logLevel: 2 replicas: 1 schedulerName: tidb-scheduler # features: # - StableScheduling=true resources: limits: cpu: 250m memory: 150Mi requests: cpu: 80m memory: 50Mi kubeSchedulerImageName: gcr.azk8s.cn/google_containers/kube-scheduler # This will default to matching your kubernetes version # kubeSchedulerImageTag:部署Operator12345helm install pingcap/tidb-operator \\ --name=tidb-operator \\ --namespace=tidb-admin \\ --version=v1.0.0 \\ -f /home/tidb/tidb-operator/values.yaml查看Operator部署情况1kubectl -n tidb-admin get pod -l app.kubernetes.io/name=tidb-operator部署TiDB-Cluster创建工作目录1mkdir -p /home/TiDB下载Chart包12cd /home/TiDBhelm fetch pingcap/tidb-cluster --version=v1.0.0解压Chart包1tar xzf tidb-cluster-v1.0.0.tgz编辑values.yaml配置含义请看这里【Kubernetes 上的 TiDB 集群配置】1vim tidb-cluster/values.yaml修改后如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524# Default values for tidb-cluster.# This is a YAML-formatted file.# Declare variables to be passed into your templates.# Also see monitor.serviceAccount# If you set rbac.create to false, you need to provide a value for monitor.serviceAccountrbac: create: true# clusterName is the TiDB cluster name, if not specified, the chart release name will be used# clusterName: demo# Add additional TidbCluster labels# ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/extraLabels: &#123;&#125;# schedulerName must be same with charts/tidb-operator/values#scheduler.schedulerNameschedulerName: tidb-scheduler# timezone is the default system timzone for TiDBtimezone: Asia/Shanghai# default reclaim policy of a PVpvReclaimPolicy: Retain# services is the service list to expose, default is ClusterIP# can be ClusterIP | NodePort | LoadBalancerservices: - name: pd type: ClusterIPdiscovery: image: pingcap/tidb-operator:v1.0.0 imagePullPolicy: IfNotPresent resources: limits: cpu: 250m memory: 150Mi requests: cpu: 80m memory: 50Mi# Whether enable ConfigMap Rollout management.# When enabling, change of ConfigMap will trigger a graceful rolling-update of the component.# This feature is only available in tidb-operator v1.0 or higher.# Note: Switch this variable against an existing cluster will cause an rolling-update of each component even# if the ConfigMap was not changed.enableConfigMapRollout: truepd: # Please refer to https://github.com/pingcap/pd/blob/master/conf/config.toml for the default # pd configurations (change to the tags of your pd version), # just follow the format in the file and configure in the 'config' section # as below if you want to customize any configuration. # Please refer to https://pingcap.com/docs-cn/v3.0/reference/configuration/pd-server/configuration-file/ # (choose the version matching your pd) for detailed explanation of each parameter. config: | [log] level = \"info\" [replication] location-labels = [\"region\", \"zone\", \"rack\", \"host\"] replicas: 3 image: pingcap/pd:v3.0.1 # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. # different classes might map to quality-of-service levels, or to backup policies, # or to arbitrary policies determined by the cluster administrators. # refer to https://kubernetes.io/docs/concepts/storage/storage-classes storageClassName: local-storage # Image pull policy. imagePullPolicy: IfNotPresent resources: limits: &#123;&#125; # cpu: 8000m # memory: 8Gi requests: # cpu: 4000m # memory: 4Gi storage: 1Gi ## affinity defines pd scheduling rules,it's default settings is empty. ## please read the affinity document before set your scheduling rule: ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: &#123;&#125; ## The following is typical example of affinity settings: ## The PodAntiAffinity setting of the example keeps PD pods does not co-locate on a topology node as far as possible to improve the disaster tolerance of PD on Kubernetes. ## The NodeAffinity setting of the example ensure that the PD pods can only be scheduled to nodes with label:[type=\"pd\"], # affinity: # podAntiAffinity: # preferredDuringSchedulingIgnoredDuringExecution: # # this term work when the nodes have the label named region # - weight: 10 # podAffinityTerm: # labelSelector: # matchLabels: # app.kubernetes.io/instance: &lt;release name&gt; # app.kubernetes.io/component: \"pd\" # topologyKey: \"region\" # namespaces: # - &lt;helm namespace&gt; # # this term work when the nodes have the label named zone # - weight: 20 # podAffinityTerm: # labelSelector: # matchLabels: # app.kubernetes.io/instance: &lt;release name&gt; # app.kubernetes.io/component: \"pd\" # topologyKey: \"zone\" # namespaces: # - &lt;helm namespace&gt; # # this term work when the nodes have the label named rack # - weight: 40 # podAffinityTerm: # labelSelector: # matchLabels: # app.kubernetes.io/instance: &lt;release name&gt; # app.kubernetes.io/component: \"pd\" # topologyKey: \"rack\" # namespaces: # - &lt;helm namespace&gt; # # this term work when the nodes have the label named kubernetes.io/hostname # - weight: 80 # podAffinityTerm: # labelSelector: # matchLabels: # app.kubernetes.io/instance: &lt;release name&gt; # app.kubernetes.io/component: \"pd\" # topologyKey: \"kubernetes.io/hostname\" # namespaces: # - &lt;helm namespace&gt; # nodeAffinity: # requiredDuringSchedulingIgnoredDuringExecution: # nodeSelectorTerms: # - matchExpressions: # - key: \"kind\" # operator: In # values: # - \"pd\" ## nodeSelector ensure pods only assigning to nodes which have each of the indicated key-value pairs as labels ## ref:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector nodeSelector: local-pv: present ## Tolerations are applied to pods, and allow pods to schedule onto nodes with matching taints. ## refer to https://kubernetes.io/docs/concepts/configuration/taint-and-toleration tolerations: [] # - key: node-role # operator: Equal # value: tidb # effect: \"NoSchedule\" annotations: &#123;&#125;tikv: # Please refer to https://github.com/tikv/tikv/blob/master/etc/config-template.toml for the default # tikv configurations (change to the tags of your tikv version), # just follow the format in the file and configure in the 'config' section # as below if you want to customize any configuration. # Please refer to https://pingcap.com/docs-cn/v3.0/reference/configuration/tikv-server/configuration-file/ # (choose the version matching your tikv) for detailed explanation of each parameter. config: | log-level = \"info\" [server] status-addr = \"0.0.0.0:20180\" # Here are some parameters you may want to customize (Please configure in the above 'config' section): # [readpool.storage] # ## Size of the thread pool for high-priority operations. # # high-concurrency = 4 # ## Size of the thread pool for normal-priority operations. # # normal-concurrency = 4 # ## Size of the thread pool for low-priority operations. # # low-concurrency = 4 # [readpool.coprocessor] # ## Most read requests from TiDB are sent to the coprocessor of TiKV. high/normal/low-concurrency is # ## used to set the number of threads of the coprocessor. # ## If there are many read requests, you can increase these config values (but keep it within the # ## number of system CPU cores). For example, for a 32-core machine deployed with TiKV, you can even # ## set these config to 30 in heavy read scenarios. # ## If CPU_NUM &gt; 8, the default thread pool size for coprocessors is set to CPU_NUM * 0.8. # # high-concurrency = 8 # # normal-concurrency = 8 # # low-concurrency = 8 # [server] # ## Size of the thread pool for the gRPC server. # # grpc-concurrency = 4 # [storage] # ## Scheduler's worker pool size, i.e. the number of write threads. # ## It should be less than total CPU cores. When there are frequent write operations, set it to a # ## higher value. More specifically, you can run `top -H -p tikv-pid` to check whether the threads # ## named `sched-worker-pool` are busy. # # scheduler-worker-pool-size = 4 #### Below parameters available in TiKV 2.x only # [rocksdb.defaultcf] # ## block-cache used to cache uncompressed blocks, big block-cache can speed up read. # ## in normal cases should tune to 30%-50% tikv.resources.limits.memory # # block-cache-size = \"1GB\" # [rocksdb.writecf] # ## in normal cases should tune to 10%-30% tikv.resources.limits.memory # # block-cache-size = \"256MB\" #### Below parameters available in TiKV 3.x and above only # [storage.block-cache] # ## Size of the shared block cache. Normally it should be tuned to 30%-50% of container's total memory. # # capacity = \"1GB\" # [raftstore] # ## true (default value) for high reliability, this can prevent data loss when power failure. # # sync-log = true # # apply-pool-size = 2 # # store-pool-size = 2 replicas: 3 image: pingcap/tikv:v3.0.1 # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. # different classes might map to quality-of-service levels, or to backup policies, # or to arbitrary policies determined by the cluster administrators. # refer to https://kubernetes.io/docs/concepts/storage/storage-classes storageClassName: local-storage # Image pull policy. imagePullPolicy: IfNotPresent resources: limits: &#123;&#125; # cpu: 16000m # memory: 32Gi # storage: 300Gi requests: # cpu: 12000m # memory: 24Gi storage: 10Gi ## affinity defines tikv scheduling rules,affinity default settings is empty. ## please read the affinity document before set your scheduling rule: ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: &#123;&#125; ## nodeSelector ensure pods only assigning to nodes which have each of the indicated key-value pairs as labels ## ref:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector nodeSelector: local-pv: present ## Tolerations are applied to pods, and allow pods to schedule onto nodes with matching taints. ## refer to https://kubernetes.io/docs/concepts/configuration/taint-and-toleration tolerations: [] # - key: node-role # operator: Equal # value: tidb # effect: \"NoSchedule\" annotations: &#123;&#125;tidb: # Please refer to https://github.com/pingcap/tidb/blob/master/config/config.toml.example for the default # tidb configurations(change to the tags of your tidb version), # just follow the format in the file and configure in the 'config' section # as below if you want to customize any configuration. # Please refer to https://pingcap.com/docs-cn/v3.0/reference/configuration/tidb-server/configuration-file/ # (choose the version matching your tidb) for detailed explanation of each parameter. config: | [log] level = \"info\" replicas: 3 # The secret name of root password, you can create secret with following command: # kubectl create secret generic tidb-secret --from-literal=root=&lt;root-password&gt; --namespace=&lt;namespace&gt; # If unset, the root password will be empty and you can set it after connecting # passwordSecretName: tidb-secret # initSql is the SQL statements executed after the TiDB cluster is bootstrapped. # initSql: |- # create database app; image: pingcap/tidb:v3.0.1 # Image pull policy. imagePullPolicy: IfNotPresent resources: limits: &#123;&#125; # cpu: 16000m # memory: 16Gi requests: &#123;&#125; # cpu: 12000m # memory: 12Gi ## affinity defines tikv scheduling rules,affinity default settings is empty. ## please read the affinity document before set your scheduling rule: ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity affinity: &#123;&#125; ## nodeSelector ensure pods only assigning to nodes which have each of the indicated key-value pairs as labels ## ref:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector nodeSelector: &#123;&#125; ## Tolerations are applied to pods, and allow pods to schedule onto nodes with matching taints. ## refer to https://kubernetes.io/docs/concepts/configuration/taint-and-toleration tolerations: [] # - key: node-role # operator: Equal # value: tidb # effect: \"NoSchedule\" annotations: &#123;&#125; maxFailoverCount: 3 service: type: NodePort exposeStatus: true # annotations: # cloud.google.com/load-balancer-type: Internal separateSlowLog: true slowLogTailer: image: busybox:1.26.2 resources: limits: cpu: 100m memory: 50Mi requests: cpu: 20m memory: 5Mi # tidb plugin configuration plugin: # enable plugin or not enable: false # the start argument to specify the folder containing directory: /plugins # the start argument to specify the plugin id (name \"-\" version) that needs to be loaded, e.g. 'conn_limit-1'. list: [\"whitelist-1\"]# mysqlClient is used to set password for TiDB# it must has Python MySQL client installedmysqlClient: image: tnir/mysqlclient imagePullPolicy: IfNotPresentmonitor: create: true # Also see rbac.create # If you set rbac.create to false, you need to provide a value here. # If you set rbac.create to true, you should leave this empty. # serviceAccount: persistent: false storageClassName: local-storage storage: 10Gi initializer: image: pingcap/tidb-monitor-initializer:v3.0.1 imagePullPolicy: IfNotPresent reloader: create: true image: pingcap/tidb-monitor-reloader:v1.0.0 imagePullPolicy: IfNotPresent service: type: NodePort grafana: create: true image: grafana/grafana:6.0.1 imagePullPolicy: IfNotPresent logLevel: info resources: limits: &#123;&#125; # cpu: 8000m # memory: 8Gi requests: &#123;&#125; # cpu: 4000m # memory: 4Gi username: admin password: admin config: # Configure Grafana using environment variables except GF_PATHS_DATA, GF_SECURITY_ADMIN_USER and GF_SECURITY_ADMIN_PASSWORD # Ref https://grafana.com/docs/installation/configuration/#using-environment-variables GF_AUTH_ANONYMOUS_ENABLED: \"true\" GF_AUTH_ANONYMOUS_ORG_NAME: \"Main Org.\" GF_AUTH_ANONYMOUS_ORG_ROLE: \"Viewer\" # if grafana is running behind a reverse proxy with subpath http://foo.bar/grafana # GF_SERVER_DOMAIN: foo.bar # GF_SERVER_ROOT_URL: \"%(protocol)s://%(domain)s/grafana/\" service: type: NodePort prometheus: image: prom/prometheus:v2.11.1 imagePullPolicy: IfNotPresent logLevel: info resources: limits: &#123;&#125; # cpu: 8000m # memory: 8Gi requests: &#123;&#125; # cpu: 4000m # memory: 4Gi service: type: NodePort reserveDays: 12 # alertmanagerURL: \"\" nodeSelector: &#123;&#125; # kind: monitor # zone: cn-bj1-01,cn-bj1-02 # region: cn-bj1 tolerations: [] # - key: node-role # operator: Equal # value: tidb # effect: \"NoSchedule\"binlog: pump: create: false replicas: 1 image: pingcap/tidb-binlog:v3.0.1 imagePullPolicy: IfNotPresent logLevel: info # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. # different classes might map to quality-of-service levels, or to backup policies, # or to arbitrary policies determined by the cluster administrators. # refer to https://kubernetes.io/docs/concepts/storage/storage-classes storageClassName: local-storage storage: 20Gi syncLog: true # a integer value to control expiry date of the binlog data, indicates for how long (in days) the binlog data would be stored. # must bigger than 0 gc: 7 # number of seconds between heartbeat ticks (in 2 seconds) heartbeatInterval: 2 drainer: create: false image: pingcap/tidb-binlog:v3.0.1 imagePullPolicy: IfNotPresent logLevel: info # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. # different classes might map to quality-of-service levels, or to backup policies, # or to arbitrary policies determined by the cluster administrators. # refer to https://kubernetes.io/docs/concepts/storage/storage-classes storageClassName: local-storage storage: 10Gi # the number of the concurrency of the downstream for synchronization. The bigger the value, # the better throughput performance of the concurrency (16 by default) workerCount: 16 # the interval time (in seconds) of detect pumps' status (default 10) detectInterval: 10 # disbale detect causality disableDetect: false # disable dispatching sqls that in one same binlog; if set true, work-count and txn-batch would be useless disableDispatch: false # # disable sync these schema ignoreSchemas: \"INFORMATION_SCHEMA,PERFORMANCE_SCHEMA,mysql,test\" # if drainer donesn't have checkpoint, use initial commitTS to initial checkpoint initialCommitTs: 0 # enable safe mode to make syncer reentrant safeMode: false # the number of SQL statements of a transaction that are output to the downstream database (20 by default) txnBatch: 20 # downstream storage, equal to --dest-db-type # valid values are \"mysql\", \"pb\", \"kafka\" destDBType: pb mysql: &#123;&#125; # host: \"127.0.0.1\" # user: \"root\" # password: \"\" # port: 3306 # # Time and size limits for flash batch write # timeLimit: \"30s\" # sizeLimit: \"100000\" kafka: &#123;&#125; # only need config one of zookeeper-addrs and kafka-addrs, will get kafka address if zookeeper-addrs is configed. # zookeeperAddrs: \"127.0.0.1:2181\" # kafkaAddrs: \"127.0.0.1:9092\" # kafkaVersion: \"0.8.2.0\"scheduledBackup: create: false # https://github.com/pingcap/tidb-cloud-backup mydumperImage: pingcap/tidb-cloud-backup:20190610 mydumperImagePullPolicy: IfNotPresent # storageClassName is a StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. # different classes might map to quality-of-service levels, or to backup policies, # or to arbitrary policies determined by the cluster administrators. # refer to https://kubernetes.io/docs/concepts/storage/storage-classes storageClassName: local-storage storage: 100Gi # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#schedule schedule: \"0 0 * * *\" # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#suspend suspend: false # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#jobs-history-limits successfulJobsHistoryLimit: 3 failedJobsHistoryLimit: 1 # https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#starting-deadline startingDeadlineSeconds: 3600 # https://github.com/maxbube/mydumper/blob/master/docs/mydumper_usage.rst#options options: \"--verbose=3\" # secretName is the name of the secret which stores user and password used for backup # Note: you must give the user enough privilege to do the backup # you can create the secret by: # kubectl create secret generic backup-secret --from-literal=user=root --from-literal=password=&lt;password&gt; secretName: backup-secret # backup to gcp gcp: &#123;&#125; # bucket: \"\" # secretName is the name of the secret which stores the gcp service account credentials json file # The service account must have read/write permission to the above bucket. # Read the following document to create the service account and download the credentials file as credentials.json: # https://cloud.google.com/docs/authentication/production#obtaining_and_providing_service_account_credentials_manually # And then create the secret by: kubectl create secret generic gcp-backup-secret --from-file=./credentials.json # secretName: gcp-backup-secret # backup to ceph object storage ceph: &#123;&#125; # endpoint: \"\" # bucket: \"\" # secretName is the name of the secret which stores ceph object store access key and secret key # You can create the secret by: # kubectl create secret generic ceph-backup-secret --from-literal=access_key=&lt;access-key&gt; --from-literal=secret_key=&lt;secret-key&gt; # secretName: ceph-backup-secret # backup to s3 s3: &#123;&#125; # region: \"\" # bucket: \"\" # secretName is the name of the secret which stores s3 object store access key and secret key # You can create the secret by: # kubectl create secret generic s3-backup-secret --from-literal=access_key=&lt;access-key&gt; --from-literal=secret_key=&lt;secret-key&gt; # secretName: s3-backup-secretmetaInstance: \"&#123;&#123; $labels.instance &#125;&#125;\"metaType: \"&#123;&#123; $labels.type &#125;&#125;\"metaValue: \"&#123;&#123; $value &#125;&#125;\"部署Cluster12345helm install pingcap/tidb-cluster \\ --name=tidb-cluster \\ --namespace=tidb \\ --version=v1.0.0 \\ -f /home/tidb/tidb-cluster/values.yaml查看Cluster部署情况1kubectl -n tidb get pods -l app.kubernetes.io/instance=tidb-cluster输出示例123456789101112NAME READY STATUS RESTARTS AGEtidb-cluster-discovery-84d6cf454c-6c2cl 1/1 Running 0 77mtidb-cluster-monitor-77cd9d7965-49v8t 3/3 Running 0 77mtidb-cluster-pd-0 1/1 Running 0 23mtidb-cluster-pd-1 1/1 Running 0 72mtidb-cluster-pd-2 1/1 Running 0 72mtidb-cluster-tidb-0 2/2 Running 0 2m11stidb-cluster-tidb-1 2/2 Running 0 2m2stidb-cluster-tidb-2 2/2 Running 0 100stidb-cluster-tikv-0 1/1 Running 0 8m56stidb-cluster-tikv-1 1/1 Running 0 8m54stidb-cluster-tikv-2 1/1 Running 0 8m52s访问TiDBTiDB兼容MySQL，因此能直接使用MySQL客户端连接TiDB集群这里使用mysql-community-client-5.7.27-1.el7作为客户端程序获取TiDB Service信息1kubectl -n tidb get svc -l app.kubernetes.io/component=tidb输出示例123NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtidb-cluster-tidb NodePort 10.96.129.51 &lt;none&gt; 4000:30944/TCP,10080:32052/TCP 85mtidb-cluster-tidb-peer ClusterIP None &lt;none&gt; 10080/TCP 19m从输出示例，可以看到tidb-cluster-tidb是NodePort类型，业务端口4000被映射为节点的30944端口，直接通过此端口即可访问TiDB。登录TiDB默认集群部署完成后，root用户是无密码的1mysql -u root -P 30944 -h k8s-master简单查询查看系统表mysql.tidb1mysql&gt; select VARIABLE_NAME,VARIABLE_VALUE from mysql.tidb;输出示例123456789101112131415161718+--------------------------+--------------------------------------------------------------------------------------------------+| VARIABLE_NAME | VARIABLE_VALUE |+--------------------------+--------------------------------------------------------------------------------------------------+| bootstrapped | True || tidb_server_version | 33 || system_tz | Asia/Shanghai || tikv_gc_leader_uuid | 5b16245c9840001 || tikv_gc_leader_desc | host:tidb-cluster-tidb-0, pid:1, start at 2019-08-04 01:40:22.757972817 +0800 CST m=+0.859764216 || tikv_gc_leader_lease | 20190804-01:58:22 +0800 || tikv_gc_enable | true || tikv_gc_run_interval | 10m0s || tikv_gc_life_time | 10m0s || tikv_gc_last_run_time | 20190804-01:48:22 +0800 || tikv_gc_safe_point | 20190804-01:38:22 +0800 || tikv_gc_auto_concurrency | true || tikv_gc_mode | distributed |+--------------------------+--------------------------------------------------------------------------------------------------+13 rows in set (0.00 sec)查看系统变量1mysql&gt; show global variables like '%tidb%';输出示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273+-------------------------------------+-----------------------+| Variable_name | Value |+-------------------------------------+-----------------------+| tidb_optimizer_selectivity_level | 0 || tidb_slow_log_threshold | 300 || tidb_distsql_scan_concurrency | 15 || tidb_check_mb4_value_in_utf8 | 1 || tidb_checksum_table_concurrency | 4 || tidb_query_log_max_len | 2048 || tidb_mem_quota_sort | 34359738368 || tidb_low_resolution_tso | 0 || tidb_skip_utf8_check | 0 || tidb_constraint_check_in_place | 0 || tidb_snapshot | || tidb_current_ts | 0 || tidb_opt_write_row_id | 0 || tidb_opt_join_reorder_threshold | 0 || tidb_build_stats_concurrency | 4 || tidb_mem_quota_topn | 34359738368 || tidb_batch_insert | 0 || tidb_config | || tidb_batch_delete | 0 || tidb_opt_correlation_exp_factor | 1 || tidb_auto_analyze_ratio | 0.5 || tidb_index_serial_scan_concurrency | 1 || tidb_ddl_error_count_limit | 512 || tidb_batch_commit | 0 || tidb_wait_split_region_timeout | 300 || tidb_mem_quota_query | 34359738368 || tidb_dml_batch_size | 20000 || tidb_mem_quota_mergejoin | 34359738368 || tidb_projection_concurrency | 4 || tidb_index_join_batch_size | 25000 || tidb_wait_split_region_finish | 1 || tidb_back_off_weight | 2 || tidb_enable_fast_analyze | 0 || tidb_skip_isolation_level_check | 0 || tidb_mem_quota_hashjoin | 34359738368 || tidb_hash_join_concurrency | 5 || tidb_scatter_region | 0 || tidb_enable_window_function | 1 || tidb_max_chunk_size | 1024 || tidb_enable_cascades_planner | 0 || tidb_ddl_reorg_batch_size | 1024 || tidb_txn_mode | || tidb_opt_correlation_threshold | 0.9 || tidb_hashagg_final_concurrency | 4 || tidb_opt_agg_push_down | 0 || tidb_index_lookup_concurrency | 4 || tidb_enable_table_partition | auto || tidb_auto_analyze_end_time | 23:59 +0000 || tidb_index_lookup_size | 20000 || tidb_hashagg_partial_concurrency | 4 || tidb_opt_insubq_to_join_and_agg | 1 || tidb_ddl_reorg_worker_cnt | 16 || tidb_mem_quota_indexlookupreader | 34359738368 || tidb_mem_quota_indexlookupjoin | 34359738368 || tidb_mem_quota_nestedloopapply | 34359738368 || tidb_general_log | 0 || tidb_force_priority | NO_PRIORITY || tidb_enable_streaming | 0 || tidb_retry_limit | 10 || tidb_enable_radix_join | 0 || tidb_ddl_reorg_priority | PRIORITY_LOW || tidb_backoff_lock_fast | 100 || tidb_auto_analyze_start_time | 00:00 +0000 || tidb_init_chunk_size | 32 || tidb_expensive_query_time_threshold | 60 || tidb_disable_txn_auto_retry | 1 || tidb_slow_query_file | /var/log/tidb/slowlog || tidb_index_lookup_join_concurrency | 4 |+-------------------------------------+-----------------------+68 rows in set (0.01 sec)查看监控信息TiDB 通过 Prometheus 和 Grafana 监控 TiDB 集群。在通过 TiDB Operator 创建新的 TiDB 集群时，对于每个 TiDB 集群，会同时创建、配置一套独立的监控系统，与 TiDB 集群运行在同一 Namespace，包括 Prometheus 和 Grafana 两个组件。监控数据默认没有持久化，如果由于某些原因监控容器重启，已有的监控数据会丢失。可以在 values.yaml 中设置 monitor.persistent 为 true 来持久化监控数据。获取监控端口1kubectl -n tidb get svc -l app.kubernetes.io/component=monitor输出示例1234NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEtidb-cluster-grafana NodePort 10.96.136.52 &lt;none&gt; 3000:30009/TCP 93mtidb-cluster-monitor-reloader NodePort 10.96.211.160 &lt;none&gt; 9089:31302/TCP 93mtidb-cluster-prometheus NodePort 10.96.75.4 &lt;none&gt; 9090:31666/TCP 93m访问监控可以看到监控服务都是NodePort类型，直接通过节点端口即可访问监控服务更新和升级TiDB-Operator当新版本 tidb-operator 发布，只要更新 values.yaml 中的 operatorImage 然后执行上述命令就可以。但是安全起见，最好从新版本 tidb-operator chart 中获取新版本 values.yaml 并和旧版本 values.yaml 合并生成新的 values.yaml，然后升级。TiDB Operator 是用来管理 TiDB 集群的，也就是说，如果 TiDB 集群已经启动并正常运行，你甚至可以停掉 TiDB Operator，而 TiDB 集群仍然能正常工作，直到你需要维护 TiDB 集群，比如伸缩、升级等等。升级TiDB-Operator1helm upgrade tidb-operator pingcap/tidb-operator --version=v1.0.0 -f /home/tidb/tidb-operator/values.yamlKubernetes集群版本升级当你的 Kubernetes 集群有版本升级，请确保 kubeSchedulerImageTag 与之匹配。默认情况下，这个值是由 Helm 在安装或者升级过程中生成的，要修改它你需要执行 helm upgrade。TiDB-Cluster滚动更新 TiDB 集群时，会按 PD、TiKV、TiDB 的顺序，串行删除 Pod，并创建新版本的 Pod，当新版本的 Pod 正常运行后，再处理下一个 Pod。滚动升级过程会自动处理 PD、TiKV 的 Leader 迁移与 TiDB 的 DDL Owner 迁移。因此，在多节点的部署拓扑下（最小环境：PD 3、TiKV 3、TiDB * 2），滚动更新 TiKV、PD 不会影响业务正常运行。对于有连接重试功能的客户端，滚动更新 TiDB 同样不会影响业务。对于无法进行重试的客户端，滚动更新 TiDB 则会导致连接到被关闭节点的数据库连接失效，造成部分业务请求失败。对于这类业务，推荐在客户端添加重试功能或在低峰期进行 TiDB 的滚动升级操作。滚动更新可以用于升级 TiDB 版本，也可以用于更新集群配置。更新TiDB-Cluster配置默认条件下，修改配置文件不会自动应用到 TiDB 集群中，只有在实例重启时，才会重新加载新的配置文件。操作步骤如下修改集群的 values.yaml 文件，将 enableConfigMapRollout 的值设为 true修改集群的 values.yaml 文件中需要调整的集群配置项，例如修改pd.replicas、tidb.replicas、tikv.replicas来进行水平扩容和缩容执行helm upgrade命令升级1helm upgrade tidb-cluster pingcap/tidb-cluster -f /home/tidb/tidb-cluster/values.yaml --version=v1.0.0查看升级进度1watch -n 1 'kubectl -n tidb get pod -o wide'升级 TiDB-Cluster 版本修改集群的 values.yaml 文件中的 tidb.image、tikv.image、pd.image 的值为新版本镜像；执行 helm upgrade 命令进行升级：1helm upgrade tidb-cluster pingcap/tidb-cluster -f /home/tidb/tidb-cluster/values.yaml --version=v1.0.0查看升级进度1watch -n 1 'kubectl -n tidb get pod -o wide'注意：将 enableConfigMapRollout 特性从关闭状态打开时，即使没有配置变更，也会触发一次 PD、TiKV、TiDB 的滚动更新。目前 PD 的 scheduler 和 replication 配置（values.yaml 中的 maxStoreDownTime 和 maxReplicas 字段）在集群安装完成后无法自动更新，需要通过 pd-ctl 手动更新。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"},{"name":"TiDB","slug":"Kubernetes/TiDB","permalink":"https://luanlengli.github.io/categories/Kubernetes/TiDB/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"},{"name":"TiDB","slug":"TiDB","permalink":"https://luanlengli.github.io/tags/TiDB/"}]},{"title":"Kubernetes创建本地PV","slug":"Kubernetes创建本地PV","date":"2019-07-23T06:21:37.000Z","updated":"2019-08-07T05:34:20.000Z","comments":true,"path":"2019/07/23/Kubernetes创建本地PV.html","link":"","permalink":"https://luanlengli.github.io/2019/07/23/Kubernetes创建本地PV.html","excerpt":"","text":"前情提要Kubernetes的持久化存储体系里面，提到最多的是对接外部ceph服务或者NFS服务。有时候，Kubernetes集群所在的环境不一定提供对应的外部存储服务，而且为了性能的需求（低延迟、高IOPS等），外部基于网络的存储无法很好的满足需求。除此之外想把数据存放到宿主机本地硬盘的做法就只有hostPath或者emptyDir，这种做法还是不够科学。在Kubernetes社区里面，对本地持久化存储的呼声非常高，因此在v1.10版本引入了Local Persistent Volume即本地PV，然后v1.14版本正式GA。本地PV并不适用于所有的应用，因为本地PV是直接跟节点绑定在一起的，如果Pod需要使用本地PV，在Pod调度过程中，会考虑本地PV的分布情况，然后选中有本地PV的节点作为调度目标节点。因此如果本地PV所在节点宕机，则可能会出现数据丢失的情况，需要应用自己能处理节点掉线数据无法访问的情况。说明仅记录操作过程和部署过程操作系统使用的CentOS-7.6.1810 x86_64虚拟机配置4CPU 8G内存 30G系统盘 20G数据盘A 5G数据盘BKubernetes集群版本v1.14.4这里演示两种方式管理本地PV手动管理本地PV使用社区提供的local-volume-provisioner来简化本地PV的管理准备步骤参考社区文档查看硬盘20G本地硬盘/dev/sdb5G本地硬盘/dev/sdc123456NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 30G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 29G 0 part /sdb 8:16 0 20G 0 disk sdc 8:32 0 5G 0 disk创建目录作为provisioner发现本地PV的目录1mkdir -p /mnt/disks格式化硬盘注意，这里是一个本地PV对应一个硬盘12mkfs.ext4 /dev/sdbmkfs.ext4 /dev/sdc获取硬盘UUID12SDB_UUID=$(blkid -s UUID -o value /dev/sdb)SDC_UUID=$(blkid -s UUID -o value /dev/sdc)创建挂载目录这里使用硬盘UUID作为挂载目录12mkdir -p /mnt/disks/$SDB_UUIDmkdir -p /mnt/disks/$SDC_UUID挂载硬盘12mount -t ext4 /dev/sdb /mnt/disks/$SDB_UUIDmount -t ext4 /dev/sdc /mnt/disks/$SDC_UUID输出示例123456NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 30G 0 disk ├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 29G 0 part /sdb 8:16 0 20G 0 disk /mnt/disks/f8727d20-3ef9-4f83-b865-25943bc342a6sdc 8:32 0 5G 0 disk /mnt/disks/9e0ead5f-ca8e-4018-98ad-960979d9cb26写入fstab12echo \"UUID=$&#123;SDB_UUID&#125; /mnt/disks/$&#123;SDB_UUID&#125; ext4 defaults 0 2\" | tee -a /etc/fstabecho \"UUID=$&#123;SDC_UUID&#125; /mnt/disks/$&#123;SDC_UUID&#125; ext4 defaults 0 2\" | tee -a /etc/fstab输出示例1234567891011## /etc/fstab# Created by anaconda on Tue Jun 18 05:24:00 2019## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#UUID=f3ca2ebf-a9fe-4cae-a20f-02ff93f2ba0c / xfs defaults 0 0UUID=3253298b-04d2-47ca-a383-e30b0b1e2267 /boot xfs defaults 0 0UUID=f8727d20-3ef9-4f83-b865-25943bc342a6 /mnt/disks/f8727d20-3ef9-4f83-b865-25943bc342a6 ext4 defaults 0 2UUID=9e0ead5f-ca8e-4018-98ad-960979d9cb26 /mnt/disks/9e0ead5f-ca8e-4018-98ad-960979d9cb26 ext4 defaults 0 2给节点打标签1kubectl label node k8s-node1 local-pv=present检查一下节点标签1kubectl get node -l local-pv=present输出示例12NAME STATUS ROLES AGE VERSIONk8s-node1 Ready &lt;none&gt; 102m v1.14.4手动管理本地PV创建PV123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: v1kind: PersistentVolumemetadata: name: k8s-node1-localpv-20G-1spec: capacity: storage: 20Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: /mnt/disks/f8727d20-3ef9-4f83-b865-25943bc342a6 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-node1---apiVersion: v1kind: PersistentVolumemetadata: name: k8s-node1-localpv-5G-1spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: /mnt/disks/9e0ead5f-ca8e-4018-98ad-960979d9cb26 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-node1创建StorageClass123456789apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: local-storage annotations: storageclass.kubernetes.io/is-default-class: trueprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumerreclaimPolicy: Retain使用Provisioner管理本地PV安装Helm二进制安装123wget -O - https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz | tar xz linux-amd64/helmmv linux-amd64/helm /usr/local/bin/helmrm -rf linux-amd64创建RBAC12345678910111213141516171819202122cat &lt;&lt; EOF | kubectl apply -f -# 创建名为tiller的ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---# 给tiller绑定cluster-admin权限apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tiller-cluster-ruleroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: tiller namespace: kube-systemEOF安装Helm服务端123helm init --tiller-image gcr.azk8s.cn/google_containers/tiller:v2.14.1 \\ --service-account tiller \\ --stable-repo-url http://mirror.azure.cn/kubernetes/charts/检查部署结果查看Pod状态1kubectl -n kube-system get pod -l app=helm,name=tiller输出示例12NAME READY STATUS RESTARTS AGEtiller-deploy-84fc6cd5f9-nz4m7 1/1 Running 0 1m查看Helm版本信息1helm version输出示例12Client: &amp;version.Version&#123;SemVer:\"v2.14.1\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.14.1\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;部署local-volume-provisioner下载项目代码1git clone --depth=1 https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner.git修改配置1vim sig-storage-local-static-provisioner/helm/provisioner/value.yaml修改如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748## Common options.#common: rbac: true namespace: kube-system createNamespace: true useAlphaAPI: false setPVOwnerRef: true useJobForCleaning: false useNodeNameOnly: false minResyncPeriod: 5m0s configMapName: \"local-provisioner-config\" podSecurityPolicy: false## Configure storage classes.#classes:- name: local-storage hostDir: /mnt/disks mountDir: /mnt/disks volumeMode: Filesystem fsType: ext4 blockCleanerCommand: - \"/scripts/fsclean.sh\" storageClass: reclaimPolicy: Retain isDefaultClass: true## Configure DaemonSet for provisioner.#daemonset: name: \"local-volume-provisioner\" image: quay.io/external_storage/local-volume-provisioner:v2.3.2 imagePullPolicy: IfNotPresent priorityClassName: system-node-critical serviceAccount: local-storage-admin nodeSelector: local-pv: present tolerations: [] resources: &#123;&#125;## Configure Prometheus monitoring#prometheus: operator: enabled: false生成YAML文件123helm template sig-storage-local-static-provisioner/helm/provisioner \\ -f sig-storage-local-static-provisioner/helm/provisioner/value.yaml \\ &gt; local-volume-provisioner.generated.yaml部署YAML文件1kubectl apply -f local-volume-provisioner.generated.yaml查看provisioner1kubectl -n kube-system get pod -l app=local-volume-provisioner输出示例12NAME READY STATUS RESTARTS AGElocal-volume-provisioner-cj242 1/1 Running 0 40s验证本地PV查看PersistentVolume1kubectl get pv输出示例123NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGElocal-pv-40a9a025 19Gi RWO Retain Available local-storage 32slocal-pv-6e9321fd 4911Mi RWO Retain Available local-storage 32s查看StorageClass1kubectl get sc输出示例12NAME PROVISIONER AGElocal-storage (default) kubernetes.io/no-provisioner 8m44s部署StatefulSet1vim localpv-sts.yaml内容如下123456789101112131415161718192021222324252627282930313233343536373839404142apiVersion: apps/v1kind: StatefulSetmetadata: name: local-testspec: serviceName: \"local-service\" replicas: 1 selector: matchLabels: app: local-test template: metadata: labels: app: local-test spec: containers: - name: test-container image: k8s.gcr.io/busybox command: - \"/bin/sh\" args: - \"-c\" - \"count=0; count_file=\\\"/usr/test-pod/count\\\"; test_file=\\\"/usr/test-pod/test_file\\\"; if [ -e $count_file ]; then count=$(cat $count_file); fi; echo $((count+1)) &gt; $count_file; while [ 1 ]; do date &gt;&gt; $test_file; echo \\\"This is $MY_POD_NAME, count=$(cat $count_file)\\\" &gt;&gt; $test_file; sleep 10; done\" volumeMounts: - name: local-vol mountPath: /usr/test-pod env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name securityContext: fsGroup: 1234 volumeClaimTemplates: - metadata: name: local-vol spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"local-storage\" resources: requests: storage: 1Gi查看PV1kubectl get pv输出示例12NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGElocal-pv-6e9321fd 4911Mi RWO Retain Bound default/local-vol-local-test-0 local-storage 14m查看PVC1kubectl get pvc输出示例1local-vol-local-test-0 Bound local-pv-6e9321fd 4911Mi RWO local-storage 2m27s清理现场删除StatefulSet1kubectl delete -f localpv-sts.yaml删除pvc1kubectl delete pvc local-vol-local-test-0删除pv1kubectl delete pv local-pv-6e9321fd清理本地目录1ssh root@k8s-node1 \"rm -rf /mnt/disks/9e0ead5f-ca8e-4018-98ad-960979d9cb26/*\"","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"Windows Server 2019安装Docker","slug":"Windows-Server-2019安装Docker","date":"2019-07-10T03:09:36.000Z","updated":"2019-07-10T13:54:56.000Z","comments":true,"path":"2019/07/10/Windows-Server-2019安装Docker.html","link":"","permalink":"https://luanlengli.github.io/2019/07/10/Windows-Server-2019安装Docker.html","excerpt":"","text":"说明Windows Server 从2016开始就已经支持Docker容器，这里以最新的2019作为演示，仅供参考系统镜像cn_windows_server_2019_x64_dvd_4de40f33.isoDocker版本18.09.7参考文档部署容器主机安装操作系统这里为了节省资源，没有选择桌面体验可以自己根据需求选择对应的操作系统版本安装Docker在powershell或者cmd里面运行命令，打开服务器配置界面1sconfig.cmd界面示意图服务器配置可以根据需要修改对应的配置，这里只做以下几项网络设置，配置网络，安装docker需要访问公网配置IP地址配置网关配置DNSWindows 更新设置，安装docker前需要更新Windows修改为手动更新选择下载并安装更新在弹出命令窗口里面选择搜索所有的更新选择安装所有更新安装完成之后重启系统启用Hyper-V和Containers功能运行管理员权限powershell1Install-WindowsFeature -Name Hyper-V,Containers -IncludeAllSubFeature -IncludeManagementTools -Verbose安装完成之后重启操作系统配置安装源运行管理员权限powershell1Install-Module -Name DockerMsftProvider -Repository PSGallery -Verbose安装Docker运行管理员权限powershell，国内安装可能会因为网络原因失败，可以尝试手动安装1Install-Package -Name docker -ProviderName DockerMsftProvider -Verbose手动安装Docker运行管理员权限powershell下载docker压缩包1Invoke-WebRequest -UseBasicParsing -OutFile C:\\docker-18.09.7.zip https://download.docker.com/components/engine/windows-server/18.09/docker-18.09.7.zip解压1Expand-Archive C:\\docker-18.09.7.zip -DestinationPath $Env:ProgramFiles -Force删除压缩包1Remove-Item -Force C:\\docker-18.09.7.zip为当前会话添加PATH变量1$env:path += \";$env:ProgramFiles\\docker\"配置新的PATH变量1$newPath = \"$env:ProgramFiles\\docker;\" + [System.Environment]::GetEnvironmentVariable(\"PATH\", [System.EnvironmentVariableTarget]::Machine)将系统PATH变量替换为新的PATH变量1[System.Environment]::SetEnvironmentVariable(\"PATH\", $newPath, [System.EnvironmentVariableTarget]::Machine)注册为系统服务1dockerd --register-service设置Docker开机启动1Set-Service -Name docker -StartupType Automatic启动Docker1Start-Service docker验证Docker1docker container run hello-world:nanoserver配置Docker在微软的官方网站上是有相关文档的配置文件路径C:\\ProgramData\\Docker\\config\\daemon.json配置文件模板12345678910111213141516&#123; \"dns\": [ \"114.114.114.114\", \"8.8.8.8\" ], \"storage-opts\": [ \"size=50GB\" ], \"data-root\": \"c:\\programdata\\docker\", \"bridge\": \"NAT\", \"registry-mirrors\": [ \"https://registry.docker-cn.com\" ], \"insecure-registries\": [], \"experimental\": true&#125;启用Linux容器支持目前Windows的container只支持Windows程序，运行Linux容器需要使用Hyper-V运行容器参考文献Getting started with Linux Containers on Windows Server 2019","categories":[{"name":"Docker","slug":"Docker","permalink":"https://luanlengli.github.io/categories/Docker/"},{"name":"Windows","slug":"Docker/Windows","permalink":"https://luanlengli.github.io/categories/Docker/Windows/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://luanlengli.github.io/tags/Docker/"},{"name":"Windows","slug":"Windows","permalink":"https://luanlengli.github.io/tags/Windows/"}]},{"title":"Kubernetes安装Helm和使用","slug":"Kubernetes安装Helm和使用","date":"2019-07-05T12:31:33.000Z","updated":"2019-08-05T13:18:44.000Z","comments":true,"path":"2019/07/05/Kubernetes安装Helm和使用.html","link":"","permalink":"https://luanlengli.github.io/2019/07/05/Kubernetes安装Helm和使用.html","excerpt":"","text":"部署Helm获取HelmGithub项目地址二进制安装123wget -O - https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz | tar xz linux-amd64/helmmv linux-amd64/helm /usr/local/bin/helmrm -rf linux-amd64通过官方脚本安装1curl -L https://git.io/get_helm.sh | bash创建RBAC12345678910111213141516171819202122cat &lt;&lt; EOF | kubectl apply -f -# 创建名为tiller的ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---# 给tiller绑定cluster-admin权限apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tiller-cluster-ruleroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: tiller namespace: kube-systemEOF安装Helm服务端123helm init --tiller-image dockerhub.azk8s.cn/gcrxio/tiller:v2.14.1 \\ --service-account tiller \\ --stable-repo-url http://mirror.azure.cn/kubernetes/charts/检查部署结果查看Pod状态1kubectl -n kube-system get pod -l app=helm,name=tiller输出示例12NAME READY STATUS RESTARTS AGEtiller-deploy-84fc6cd5f9-nz4m7 1/1 Running 0 1m查看Helm版本信息1helm version输出示例12Client: &amp;version.Version&#123;SemVer:\"v2.14.1\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.14.1\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;使用Helm命令自动补齐临时生效1source &lt;$(helm compeltion bash)永久生效12helm compeltion bash &gt; /etc/bash_completions./helmsource /etc/profile.d/bash_completion.sh查看Charthttps://hub.helm.sh/charts查看Repo1helm repo list输出示例123NAME URL local http://127.0.0.1:8879/charts stable https://kubernetes-charts.storage.googleapis.com/添加Repo1helm repo add pingcap &lt;Chart-URL&gt;更新Repo缓存1helm repo update查看Repo里的Chart1helm search &lt;Chart-Name&gt; -l输出示例1234helm search stable -lNAME CHART VERSION APP VERSION DESCRIPTION stable/acs-engine-autoscaler 2.2.2 2.1.1 DEPRECATED Scales worker nodes within agent pools stable/acs-engine-autoscaler 2.2.1 2.1.1 Scales worker nodes within agent pools查找Chart这里以redis作为关键字1helm search redis输出示例12345NAME CHART VERSION APP VERSION DESCRIPTION stable/prometheus-redis-exporter 3.0.0 1.0.3 Prometheus exporter for Redis metrics stable/redis 9.0.1 5.0.5 Open source, advanced key-value store. It is often referr...stable/redis-ha 3.6.2 5.0.5 Highly available Kubernetes implementation of Redis stable/sensu 0.2.3 0.28 Sensu monitoring framework backed by the Redis transpor下载Chart1helm fetch stable/redis --version=9.0.1查看Chart文件结构解压Chart1tar xzf redis-9.0.1.tgz查看目录结构1tree redis输出示例12345678910111213141516171819202122232425262728293031redis├── Chart.yaml├── ci│ ├── default-values.yaml│ ├── dev-values.yaml│ ├── extra-flags-values.yaml│ ├── production-sentinel-values.yaml│ ├── production-values.yaml│ ├── redisgraph-module-values.yaml│ └── redis-lib-values.yaml├── README.md├── templates│ ├── configmap.yaml│ ├── headless-svc.yaml│ ├── health-configmap.yaml│ ├── _helpers.tpl│ ├── metrics-prometheus.yaml│ ├── metrics-svc.yaml│ ├── networkpolicy.yaml│ ├── NOTES.txt│ ├── redis-master-statefulset.yaml│ ├── redis-master-svc.yaml│ ├── redis-rolebinding.yaml│ ├── redis-role.yaml│ ├── redis-serviceaccount.yaml│ ├── redis-slave-statefulset.yaml│ ├── redis-slave-svc.yaml│ ├── redis-with-sentinel-svc.yaml│ └── secret.yaml├── values-production.yaml└── values.yaml使用Helm安装Chart帮助文档里面是这么描述的123451. By chart reference: helm install stable/mariadb2. By path to a packaged chart: helm install ./nginx-1.2.3.tgz3. By path to an unpacked chart directory: helm install ./nginx4. By absolute URL: helm install https://example.com/charts/nginx-1.2.3.tgz5. By chart reference and repo url: helm install --repo https://example.com/charts/ nginx直接安装1helm install stable/redis --name=helm-redis --namespace=default --version=9.0.1安装时指定变量具体的变量定义在Chart目录下的values.yaml文件里面有1234567helm install stable/redis \\ --name=helm-redis \\ --namesapce=default \\ --version=9.0.1 \\ --set image.registry=docker.io \\ --set image.repository=bitnami/redis \\ --set image.tag=5.0.5-debian-9-r36安装时指定变量文件按需修改Chart目录下values.yaml文件12345helm install stable/redis \\ --name=helm-redis \\ --namesapce=default \\ --version=9.0.1 \\ -f redis/values.yaml列出Release1helm list --all输出示例12NAME UPDATED CHARThelm-redis Mon May 9 16:07:08 2019 redis-9.0.1更新Release1helm upgrade helm-redis stable/redis -f redis/values.yaml查看Release历史1234helm history helm-redisREVISION UPDATED STATUS CHART DESCRIPTION1 Mon Apr 16 10:21:44 2019 SUPERSEDED stable/redis-9.0.1 Install complete2 Mon Apr 16 10:43:10 2019 DEPLOYED stable/redis-9.0.1 Upgrade complete回滚Release1helm rollback helm-redis 1删除Release1helm delete helm-redis彻底删除1helm delete --purge helm-redis通过模板功能生成YAML文件12345helm template stable/redis \\ --name=helm-redis \\ --namesapce=default \\ --version=9.0.1 \\ -f redis/values.yaml &gt; helm_redis_generated.yaml此方法生成的YAML文件可以通过kubectl命令进行部署","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"通过Linux capabilities机制让kong监听80和443端口","slug":"通过Linux capabilities机制让kong监听80和443端口","date":"2019-07-05T02:09:51.000Z","updated":"2019-07-05T04:27:29.000Z","comments":true,"path":"2019/07/05/通过Linux capabilities机制让kong监听80和443端口.html","link":"","permalink":"https://luanlengli.github.io/2019/07/05/通过Linux capabilities机制让kong监听80和443端口.html","excerpt":"","text":"前情提要在做kong容器实验的时候，发现kong数据平面默认监听的端口是8000和8443，非常难受。通过容器启动时声明环境变量的方式，将数据平面监听端口改成了80和443，但是启动时提示[nginx: [emerg] bind() to 0.0.0.0:80 failed (13: permission denied)]。在Kong API网关搭建部署记录通过更改配置的方式，可以实现让数据平面监听在80和443，这就有点奇怪了。于是乎去看了下kong社区是怎么生成容器镜像的，项目地址在这里。Dockerfile以alpine为例，找到项目里面的alpine/Dockerfile文件，内容如下12345678910111213141516171819202122232425262728293031FROM alpine:3.10LABEL maintainer=\"Kong Core Team &lt;team-core@konghq.com&gt;\"ENV KONG_VERSION 1.2.1ENV KONG_SHA256 067bed966de064f15e548b1afbf859e724a3a5689865edc501db40cf61a7044cRUN adduser -Su 1337 kong \\ &amp;&amp; mkdir -p \"/usr/local/kong\" \\ &amp;&amp; apk add --no-cache --virtual .build-deps wget tar ca-certificates \\ &amp;&amp; apk add --no-cache libgcc openssl pcre perl tzdata curl libcap su-exec zip \\ &amp;&amp; wget -O kong.tar.gz \"https://bintray.com/kong/kong-alpine-tar/download_file?file_path=kong-$KONG_VERSION.apk.tar.gz\" \\ &amp;&amp; echo \"$KONG_SHA256 *kong.tar.gz\" | sha256sum -c - \\ &amp;&amp; tar -xzf kong.tar.gz -C /tmp \\ &amp;&amp; rm -f kong.tar.gz \\ &amp;&amp; cp -R /tmp/usr / \\ &amp;&amp; rm -rf /tmp/usr \\ &amp;&amp; cp -R /tmp/etc / \\ &amp;&amp; rm -rf /tmp/etc \\ &amp;&amp; apk del .build-deps \\ &amp;&amp; chown -R kong:0 /usr/local/kong \\ &amp;&amp; chmod -R g=u /usr/local/kongCOPY docker-entrypoint.sh /docker-entrypoint.shENTRYPOINT [\"/docker-entrypoint.sh\"]EXPOSE 8000 8443 8001 8444STOPSIGNAL SIGQUITCMD [\"kong\", \"docker-start\"]可以看到，Kong容器默认运行指令为/docker-entrypoint.sh kong docker-startEntrypoint那么来看看docker-entrypoint.sh里面做了什么事情吧。1234567891011121314151617181920212223242526272829303132333435363738394041#!/bin/shset -eexport KONG_NGINX_DAEMON=offhas_transparent() &#123; echo \"$1\" | grep -E \"[^\\s,]+\\s+transparent\\b\" &gt;/dev/null&#125;if [[ \"$1\" == \"kong\" ]]; then PREFIX=$&#123;KONG_PREFIX:=/usr/local/kong&#125; if [[ \"$2\" == \"docker-start\" ]]; then shift 2 kong prepare -p \"$PREFIX\" \"$@\" # workaround for https://github.com/moby/moby/issues/31243 chmod o+w /proc/self/fd/1 || true chmod o+w /proc/self/fd/2 || true if [ \"$(id -u)\" != \"0\" ]; then exec /usr/local/openresty/nginx/sbin/nginx \\ -p \"$PREFIX\" \\ -c nginx.conf else if [ ! -z $&#123;SET_CAP_NET_RAW&#125; ] \\ || has_transparent \"$KONG_STREAM_LISTEN\" \\ || has_transparent \"$KONG_PROXY_LISTEN\" \\ || has_transparent \"$KONG_ADMIN_LISTEN\"; then setcap cap_net_raw=+ep /usr/local/openresty/nginx/sbin/nginx fi chown -R kong:0 /usr/local/kong exec su-exec kong /usr/local/openresty/nginx/sbin/nginx \\ -p \"$PREFIX\" \\ -c nginx.conf fi fifiexec \"$@\"从脚本里面可以看到做了以下几个事情判断参数为docker-start时，执行kong prepare判断环境变量是否带有transparent，有的话就执行setcap cap_net_raw=+ep，没有就直接结束判断修改/usr/local/kong的owner和group最终是使用kong用户去启动Nginx进程。123456789/ # ps -efPID USER TIME COMMAND 1 kong 0:00 nginx: master process /usr/local/openresty/nginx/sbin/nginx -p /usr/local/kong -c nginx.conf 32 kong 0:00 nginx: worker process 33 kong 0:00 nginx: worker process 34 kong 0:00 nginx: worker process 35 kong 0:00 nginx: worker process 36 root 0:00 /bin/sh 45 root 0:00 ps -ef从容器里面可以看到，Nginx进程是以kong用户运行的，自然就没有权限监听小于1024的端口，那么transparent怎么就可以了呢，回到if判断里面执行了setcap cap_net_raw=+ep这样的命令。这个cap_net_raw是干什么用的，查了下文档，针对此项的说明如下12345CAP_NET_RAW * use RAW and PACKET sockets; * bind to any address for transparent proxying.也就是，根据脚本判断到kong需要实现transparent功能时，会给nginx添加CAP_NET_RAW，以实现普通用户也能运行需要trasparent proxying权限的程序。思考那么问题来了，在Linux capabilities里面，是否有相关的权限可以让程序绑定小于1024的端口呢。恩，很好，一下就找到了CAP_NET_BIND_SERVICE，说明如下12CAP_NET_BIND_SERVICE Bind a socket to Internet domain privileged ports (port numbers less than 1024).魔改接下来就是魔改entrypoint的节奏了，修改如下在34行给nginx进程添加了CAP_NET_BIND_SERVICE权限123456789101112131415161718192021222324252627282930313233343536373839404142#!/bin/shset -eexport KONG_NGINX_DAEMON=offhas_transparent() &#123; echo \"$1\" | grep -E \"[^\\s,]+\\s+transparent\\b\" &gt;/dev/null&#125;if [[ \"$1\" == \"kong\" ]]; then PREFIX=$&#123;KONG_PREFIX:=/usr/local/kong&#125; if [[ \"$2\" == \"docker-start\" ]]; then shift 2 kong prepare -p \"$PREFIX\" \"$@\" # workaround for https://github.com/moby/moby/issues/31243 chmod o+w /proc/self/fd/1 || true chmod o+w /proc/self/fd/2 || true if [ \"$(id -u)\" != \"0\" ]; then exec /usr/local/openresty/nginx/sbin/nginx \\ -p \"$PREFIX\" \\ -c nginx.conf else if [ ! -z $&#123;SET_CAP_NET_RAW&#125; ] \\ || has_transparent \"$KONG_STREAM_LISTEN\" \\ || has_transparent \"$KONG_PROXY_LISTEN\" \\ || has_transparent \"$KONG_ADMIN_LISTEN\"; then setcap cap_net_raw=+ep /usr/local/openresty/nginx/sbin/nginx fi chown -R kong:0 /usr/local/kong setcap cap_net_bind_service=+ep /usr/local/openresty/nginx/sbin/nginx exec su-exec kong /usr/local/openresty/nginx/sbin/nginx \\ -p \"$PREFIX\" \\ -c nginx.conf fi fifiexec \"$@\"重新构建容器镜像了1docker build -t newkong:v1 .成果检验重新运行容器12345678910docker run -d \\ -e KONG_DATABASE=off \\ -e KONG_PROXY_LISTEN='0.0.0.0:80, 0.0.0.0:443 ssl' \\ -e KONG_LOG_LEVEL=notice \\ -e \"KONG_PROXY_ACCESS_LOG=/dev/stdout\" \\ -e \"KONG_ADMIN_ACCESS_LOG=/dev/stdout\" \\ -e \"KONG_PROXY_ERROR_LOG=/dev/stderr\" \\ -e \"KONG_ADMIN_ERROR_LOG=/dev/stderr\" \\ --name kong \\ newkong:v1查看日志12345678docker logs kong2019/07/05 03:35:41 [notice] 1#0: using the \"epoll\" event method2019/07/05 03:35:41 [notice] 1#0: openresty/1.13.6.22019/07/05 03:35:41 [notice] 1#0: built by gcc 8.3.0 (Alpine 8.3.0) 2019/07/05 03:35:41 [notice] 1#0: OS: Linux 5.1.15-300.fc30.x86_642019/07/05 03:35:41 [notice] 1#0: getrlimit(RLIMIT_NOFILE): 1048576:10485762019/07/05 03:35:41 [notice] 1#0: start worker processes2019/07/05 03:35:41 [notice] 1#0: start worker process 33查看进程12345docker exec -it kong /bin/sh -c \"ps -ef\"PID USER TIME COMMAND 1 kong 0:00 nginx: master process /usr/local/openresty/nginx/sbin/ngin 33 kong 0:00 nginx: worker process 39 root 0:00 ps -ef查看监听123456789docker exec -it kong /bin/sh -c \"netstat -lp\"Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN -tcp 0 0 127.0.0.1:8444 0.0.0.0:* LISTEN -tcp 0 0 127.0.0.1:8001 0.0.0.0:* LISTEN -tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN -Active UNIX domain sockets (only servers)Proto RefCnt Flags Type State I-Node PID/Program name Path可以看到，Nginx已经可以成功监听80和443端口了。欧耶~！后记最后琢磨了一下，通过传入启动命令的方式也可以实现kong监听80和443端口。感觉绕了弯子……1234567891011docker run -d \\ -e KONG_DATABASE=off \\ -e KONG_PROXY_LISTEN='0.0.0.0:80, 0.0.0.0:443 ssl' \\ -e KONG_LOG_LEVEL=notice \\ -e \"KONG_PROXY_ACCESS_LOG=/dev/stdout\" \\ -e \"KONG_ADMIN_ACCESS_LOG=/dev/stdout\" \\ -e \"KONG_PROXY_ERROR_LOG=/dev/stderr\" \\ -e \"KONG_ADMIN_ERROR_LOG=/dev/stderr\" \\ --name kong \\ kong:1.2 \\ kong start","categories":[{"name":"Kong","slug":"Kong","permalink":"https://luanlengli.github.io/categories/Kong/"}],"tags":[{"name":"Kong","slug":"Kong","permalink":"https://luanlengli.github.io/tags/Kong/"}]},{"title":"Kong Ingress Controller部署","slug":"Kong-Ingress-Controller部署","date":"2019-07-02T03:33:33.000Z","updated":"2019-07-07T02:18:47.000Z","comments":true,"path":"2019/07/02/Kong-Ingress-Controller部署.html","link":"","permalink":"https://luanlengli.github.io/2019/07/02/Kong-Ingress-Controller部署.html","excerpt":"","text":"说明如果不知道kong怎么部署，参考Kong API网关搭建部署记录这里简单描述一下怎么修改官方的YAML文件，不保证ctrl+c和ctrl+v可以直接跑起来!Github项目地址kong-ingress-controller的版本号是0.5.0kong的版本号是1.2Kong-Ingress-Controller介绍这里是参考项目文档，简单的翻译一下。简介Kong Ingress Controller是一个动态且高度可用的Ingress Controller。它使用在Kubernetes集群中创建的Ingress资源来配置Kong。此外，它还可以为Kubernetes中运行的服务配置插件，负载平衡和运行状况检查。部署方式kong的ingress-controller是Go编写的程序，作用机制类似于Adapter，将Kubernetes里面的资源对象转换成Kong的配置规则。支持backed with a database模式和backed without a database模式。DBless模式运行在DBless模式下，Kong ingress controller会作为sidecar容器与Kong容器一起运行在同一个Pod里面，并且会根据从Kubernetes API Server接收到的信息动态配置Kong。支持增加kong容器副本数量来实现高可用和负载均衡。后端数据库模式运行在基于后端数据库的模式时，ingress controller与kong的控制平面部署在一起，kong的数据平面分开部署。在下图可以看明显看到kong的控制平面和数据平面是分开的。高可用当部署多个kong ingress controller时，多个kong ingress controller之间会选举出leader。同一时间只有leader能配置kong的规则，并且leader失效后会自动选主。横向扩展如果是基于后端数据库模式，kong的控制平面和数据平面分开部署，可以单独增加kong的数据平面来实现横向扩展。规则转换下图是Kubernetes资源对象怎么转换成Kong配置规则的过程PS：这个图感觉怪怪的，这哪里有转换过程，不是Kubernetes的Ingress对象模型吗CRD资源Kong ingress controller可以配置以下CRD资源对象，实现对kong更加精细的配置管理，官方文档在这里KongIngressKongPluginKongConsumerKongCredential官方YAML模板说明链接在这里YAML模板，下面几个资源对象可以直接套官方NamespaceCustomResourceDefinitionServiceAccountClusterRoleClusterRoleBinding其他的资源对象的定义还是有点不适合生产环境，需要改。基于后端数据库模式部署说明Kubernetes集群环境不一定有ELB或者SLB，可以在Ingress节点上部署keepalived+haproxy实现高可用和负载均衡使用外部独立的PostgreSQL，部署方式见PostgreSQL10安装部署和初始化，数据库用户kong，密码kongkong数据平面通过nodeSelector加tolerations将Pod调度到Kubernetes指定的Ingress节点修改kong数据平面运行参数共享宿主机网络栈PostgreSQL数据库这里使用Service配合Endpoints，将外部PostgreSQL声明为Kubernetes内部的服务。这样Kubernetes内部的Pod可以通过SVC访问外部PostgreSQL。12345678910111213141516171819202122---apiVersion: v1kind: Servicemetadata: name: postgres namespace: kongspec: ports: - name: pgsql port: 5432 targetPort: 5432 protocol: TCP---apiVersion: v1kind: Endpointsmetadata: name: postgressubsets: - addresses: - ip: 192.168.1.1 ports: - port: 5432Kong Bootstrap Jobkong-ingress-migrations12345678910111213141516171819202122232425262728293031323334353637---apiVersion: batch/v1kind: Jobmetadata: name: kong-migrations namespace: kongspec: template: metadata: name: kong-migrations spec: initContainers: - name: wait-for-postgres image: busybox env: - name: KONG_PG_HOST value: \"postgres.kong.svc.cluster.local\" - name: KONG_PG_PORT value: \"5432\" command: [ \"/bin/sh\", \"-c\", \"until nc -zv $KONG_PG_HOST $KONG_PG_PORT -w1; do echo 'waiting for db'; sleep 1; done\" ] containers: - name: kong-migrations image: kong:1.2 env: - name: KONG_PG_HOST value: \"postgres.kong.svc.cluster.local\" - name: KONG_PG_PORT value: \"5432\" - name: KONG_PG_DATABASE value: \"kong\" - name: KONG_PG_USER value: \"kong\" - name: KONG_PG_PASSWORD value: \"kong\" command: [ \"/bin/sh\", \"-c\", \"kong migrations bootstrap\" ] restartPolicy: OnFailure---Kong数据平面kong-proxy TLS证书可以通过kubectl命令生成Secret1kubectl -n kong create secret tls kong-tls-secret --key ./tls.key --cert ./tls.crtkong-ingress-proxy.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148---apiVersion: v1kind: Servicemetadata: name: kong-ingress-proxy namespace: kongspec: type: ClusterIP ports: - name: kong-proxy port: 80 targetPort: 80 protocol: TCP - name: kong-proxy-ssl port: 443 targetPort: 443 protocol: TCP selector: app: kong-ingres-proxy---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kong-ingress-proxy namespace: kongspec: template: metadata: labels: name: kong-ingress-proxy app: kong-ingress-proxy spec: # 共享宿主机网络栈 hostNetwork: true # 选择节点标签为node-role=kong的节点 nodeSelector: node-role: kong # 容忍node-role=kong的污点 tolerations: - effect: NoSchedule key: \"node-role\" operator: \"Equal\" value: \"kong\" dnsPolicy: ClusterFirstWithHostNet initContainers: # hack to verify that the DB is up to date or not # TODO remove this for Kong &gt;= 0.15.0 - name: wait-for-migrations image: kong:1.2 command: [ \"/bin/sh\", \"-c\", \"kong migrations list\" ] env: - name: KONG_ADMIN_LISTEN value: 'off' - name: KONG_PROXY_LISTEN value: 'off' - name: KONG_PROXY_ACCESS_LOG value: \"/dev/stdout\" - name: KONG_ADMIN_ACCESS_LOG value: \"/dev/stdout\" - name: KONG_PROXY_ERROR_LOG value: \"/dev/stderr\" - name: KONG_ADMIN_ERROR_LOG value: \"/dev/stderr\" - name: KONG_PG_HOST value: \"postgres.kong.svc.cluster.local\" - name: KONG_PG_PORT value: \"5432\" - name: KONG_PG_DATABASE value: \"kong\" - name: KONG_PG_USER value: \"kong\" - name: KONG_PG_PASSWORD value: \"kong\" containers: - name: kong-proxy image: kong:1.2 env: - name: KONG_NGINX_DAEMON value: \"off\" - name: KONG_PG_HOST value: \"postgres.kong.svc.cluster.local\" - name: KONG_PG_PORT value: \"5432\" - name: KONG_PG_DATABASE value: \"kong\" - name: KONG_PG_USER value: \"kong\" - name: KONG_PG_PASSWORD value: \"kong\" - name: KONG_PROXY_ACCESS_LOG value: \"/dev/stdout\" - name: KONG_PROXY_ERROR_LOG value: \"/dev/stderr\" - name: KONG_ADMIN_LISTEN value: 'off' - name: KONG_PROXY_LISTEN value: '0.0.0.0:80, 0.0.0.0:443 ssl' - name: KONG_SSL_CIPHER_SUITE value: \"modern\" - name: KONG_SSL_CERT value: \"/opt/tls/tls.crt\" - name: KONG_SSL_CERT_KEY value: \"/opt/tls/tls.key\" - name: KONG_CLIENT_MAX_BODY_SIZE value: \"0\" - name: KONG_CLIENT_BODY_BUFFER_SIZE value: \"16k\" - name: KONG_UPSTREAM_KEEPALIVE value: \"60\" - name: KONG_REAL_IP_HEADER value: \"X-Real-IP\" - name: KONG_DB_UPDATE_FREQUENCY value: \"5\" #- name: KONG_MEM_CACHE_SIZE # value: \"128m\" command: - /usr/local/bin/kong - start #securityContext: # capabilities: # add: # - NET_BIND_SERVICE ports: - name: proxy containerPort: 8000 protocol: TCP - name: proxy-ssl containerPort: 8443 protocol: TCP lifecycle: preStop: exec: command: [ \"/bin/sh\", \"-c\", \"kong quit\" ] volumeMounts: - name: timezone-volume mountPath: /etc/localtime readOnly: true - name: tls-volume mountPath: /opt/tls/ readOnly: true volumes: - name: timezone-volume hostPath: path: /usr/share/zoneinfo/Asia/Shanghai - name: tls-volume secret: secretName: kong-tls-secret---Kong控制平面kong-ingress-controller.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169---apiVersion: v1kind: Servicemetadata: name: kong-ingress-controller namespace: kongspec: type: ClusterIP ports: - name: kong-admin port: 8001 targetPort: 8001 protocol: TCP selector: app: ingress-kong---apiVersion: extensions/v1beta1kind: Deploymentmetadata: labels: app: ingress-kong name: kong-ingress-controller namespace: kongspec: selector: matchLabels: app: ingress-kong strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 0 type: RollingUpdate template: metadata: annotations: # the returned metrics are related to the kong ingress controller not kong itself prometheus.io/port: \"10254\" prometheus.io/scrape: \"true\" labels: app: ingress-kong spec: serviceAccountName: kong-serviceaccount initContainers: - name: wait-for-migrations image: kong:1.2 command: [ \"/bin/sh\", \"-c\", \"kong migrations list\" ] env: - name: KONG_ADMIN_LISTEN value: 'off' - name: KONG_PROXY_LISTEN value: 'off' - name: KONG_PROXY_ACCESS_LOG value: \"/dev/stdout\" - name: KONG_ADMIN_ACCESS_LOG value: \"/dev/stdout\" - name: KONG_PROXY_ERROR_LOG value: \"/dev/stderr\" - name: KONG_ADMIN_ERROR_LOG value: \"/dev/stderr\" - name: KONG_PG_HOST value: \"postgres.kong.svc.cluster.local\" - name: KONG_PG_PORT value: \"5432\" - name: KONG_PG_DATABASE value: \"kong\" - name: KONG_PG_USER value: \"kong\" - name: KONG_PG_PASSWORD value: \"kong\" containers: - name: admin-api image: kong:1.2 env: - name: KONG_NGINX_DAEMON value: \"off\" - name: KONG_PG_HOST value: \"postgres.kong.svc.cluster.local\" - name: KONG_PG_PORT value: \"5432\" - name: KONG_PG_DATABASE value: \"kong\" - name: KONG_PG_USER value: \"kong\" - name: KONG_PG_PASSWORD value: \"kong\" - name: KONG_ADMIN_ACCESS_LOG value: \"/dev/stdout\" - name: KONG_ADMIN_ERROR_LOG value: \"/dev/stderr\" - name: KONG_ADMIN_LISTEN value: \"0.0.0.0:8001, 0.0.0.0:8444 ssl\" - name: KONG_PROXY_LISTEN value: 'off' ports: - name: kong-admin containerPort: 8001 livenessProbe: failureThreshold: 3 httpGet: path: /status port: 8001 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /status port: 8001 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 volumeMounts: - name: timezone-volume mountPath: /etc/localtime readOnly: true - name: ingress-controller args: - /kong-ingress-controller # the kong URL points to the kong admin api server - --kong-url=https://localhost:8444 - --admin-tls-skip-verify # Service from were we extract the IP address/es to use in Ingress status - --publish-service=kong/kong-proxy - --sync-period=10m0s - --v=2 env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: kong-docker-kubernetes-ingress-controller.bintray.io/kong-ingress-controller:0.5.0 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 volumeMounts: - name: timezone-volume mountPath: /etc/localtime readOnly: true volumes: - name: timezone-volume hostPath: path: /usr/share/zoneinfo/Asia/Shanghai基于无数据库模式部署说明在DBless模式中，每个kong都是独立工作的，由Kong-Ingress-Controller作为sidecar调用kong的config接口来实现配置更新。这样就有点像把kong的配置保存在Kubernetes里面，每个kong都是独立配置，不需要依赖外部数据库，避免因为数据库故障影响所有的kong。DBless模式有些限制，可以看这里KongIngressDBlesskong-proxy TLS证书可以通过kubectl命令生成Secret1kubectl -n kong create secret tls kong-tls-secret --key ./tls.key --cert ./tls.crtkong-ingress-controller-dbless-cm.yaml123456789101112131415161718192021222324252627282930313233343536373839---apiVersion: v1kind: ConfigMapmetadata: name: kong-server-blocks namespace: kongdata: servers.conf: | # Prometheus metrics server server &#123; server_name kong_prometheus_exporter; listen 0.0.0.0:9542; # can be any other port as well access_log off; location /metrics &#123; default_type text/plain; content_by_lua_block &#123; local prometheus = require \"kong.plugins.prometheus.exporter\" prometheus:collect() &#125; &#125; location /nginx_status &#123; internal; access_log off; stub_status; &#125; &#125; # Health check server # TODO how to health check kong in dbless? server &#123; server_name kong_health_check; listen 0.0.0.0:9001; # can be any other port as well access_log off; location /health &#123; return 200; &#125; &#125;kong-ingress-controller-dbless-ds.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186---apiVersion: v1kind: Servicemetadata: name: kong-ingress-proxy namespace: kongspec: type: ClusterIP ports: - name: kong-proxy port: 80 targetPort: 80 protocol: TCP - name: kong-proxy-ssl port: 443 targetPort: 443 protocol: TCP selector: app: kong-ingres-proxy---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: kong-ingress-proxy namespace: kongspec: template: metadata: labels: name: kong-ingress-proxy app: kong-ingress-proxy annotations: prometheus.io/port: \"9542\" prometheus.io/scrape: \"true\" spec: # 共享宿主机网络栈 hostNetwork: true # 选择节点标签为node-role=kong的节点 nodeSelector: node-role: kong # 容忍node-role=kong的污点 tolerations: - effect: NoSchedule key: \"node-role\" operator: \"Equal\" value: \"kong\" dnsPolicy: ClusterFirstWithHostNet serviceAccountName: kong-serviceaccount containers: - name: proxy image: kong:1.2 env: - name: KONG_NGINX_DAEMON value: \"off\" - name: KONG_DATABASE value: \"off\" - name: KONG_NGINX_HTTP_INCLUDE value: \"/kong/servers.conf\" - name: KONG_ADMIN_ACCESS_LOG value: \"/dev/stdout\" - name: KONG_ADMIN_ERROR_LOG value: \"/dev/stderr\" - name: KONG_ADMIN_LISTEN value: \"127.0.0.1:8444 ssl\" - name: KONG_PROXY_LISTEN value: '0.0.0.0:80, 0.0.0.0:443 ssl' - name: KONG_SSL_CIPHER_SUITE value: \"modern\" - name: KONG_SSL_CERT value: \"/opt/tls/tls.crt\" - name: KONG_SSL_CERT_KEY value: \"/opt/tls/tls.key\" - name: KONG_CLIENT_MAX_BODY_SIZE value: \"0\" - name: KONG_CLIENT_BODY_BUFFER_SIZE value: \"16k\" - name: KONG_UPSTREAM_KEEPALIVE value: \"60\" - name: KONG_REAL_IP_HEADER value: \"X-Real-IP\" - name: KONG_MEM_CACHE_SIZE value: \"128m\" command: - /usr/local/bin/kong - start #securityContext: # capabilities: # add: # - NET_BIND_SERVICE lifecycle: preStop: exec: command: [ \"/bin/sh\", \"-c\", \"kong quit\" ] ports: - name: proxy containerPort: 80 protocol: TCP - name: proxy-ssl containerPort: 443 protocol: TCP - name: metrics containerPort: 9542 protocol: TCP livenessProbe: failureThreshold: 3 httpGet: path: /health port: 9001 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /health port: 9001 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 volumeMounts: - name: kong-server-blocks mountPath: /kong - name: timezone-volume mountPath: /etc/localtime readOnly: true - name: tls-volume mountPath: /opt/tls/ readOnly: true - name: ingress-controller args: - /kong-ingress-controller # the kong URL points to the kong admin api server - --kong-url=https://localhost:8444 - --admin-tls-skip-verify # Service from were we extract the IP address/es to use in Ingress status - --publish-service=kong/kong-proxy - --sync-period=10m0s - --v=2 env: - name: POD_NAME valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace image: kong-docker-kubernetes-ingress-controller.bintray.io/kong-ingress-controller:0.5.0 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 volumeMounts: - name: timezone-volume mountPath: /etc/localtime readOnly: true volumes: - name: kong-server-blocks configMap: name: kong-server-blocks - name: timezone-volume hostPath: path: /usr/share/zoneinfo/Asia/Shanghai - name: tls-volume secret: secretName: kong-tls-secretCRD资源关于CRD资源怎么用，在Github上面是有相关的说明文档KongPlugin在kong官方网站是有关于Plugin的介绍开启IP黑白名单1234567891011121314apiVersion: configuration.konghq.com/v1kind: KongPluginmetadata: name: ip-restriction namespace: defaultdisabled: false # optionalplugin: ip-restrictionconfig:# whitelist: # 黑名单和白名单只能选一# - 8.8.8.8# - 8.8.4.4 blacklist: - 8.8.8.8 - 8.8.4.4开启prometheus1234567891011121314apiVersion: configuration.konghq.com/v1kind: KongPluginmetadata: name: kong-prometheus namespace: kong label: global: trueplugin: prometheusdisabled: falseconfig: status_code: 503 content_type: null body: null message: null开启rate-limiting1234567891011121314apiVersion: configuration.konghq.com/v1kind: KongPluginmetadata: name: rate-limit namespace: default labels: global: \"false\"plugin: rate-limitingdisabled: falseconfig: second: 5 hour: 10000 limit_by: ip policy: local #local,cluster,redis开启zipkin链路追踪123456789101112apiVersion: configuration.konghq.com/v1kind: KongPluginmetadata: name: echo-http-zipkin-trace namespace: default labels: global: \"false\"disabled: falseplugin: zipkinconfig: http_endpoint: \"http://your.zipkin.collector:9411/api/v2/spans\" sample_ratio: 1 # 不带tracid的请求的采样比率，1是100%，全部采集开启Basic Authentication这里认证的时候会查找KongConsumer和KongCredential1234567apiVersion: configuration.konghq.com/v1kind: KongPluginmetadata: name: demo-basic-auth namespace: defaultdisabled: falseplugin: basic-auth配置Response Header123456789101112131415161718192021222324apiVersion: configuration.konghq.com/v1kind: KongPluginmetadata: name: response-transformer namespace: default label: global: falsedisable: falseplugin: response-transformerconfig: replace: json: [] headers: [] append: json: [] headers: [] add: json: [] headers: - \"abc: def\" remove: json: [] headers: - Last-ModifiedKongIngress1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162apiVersion: configuration.konghq.com/v1kind: KongIngressmetadata: name: configuration-demoupstream: hash_on: none hash_fallback: none healthchecks: active: concurrency: 10 healthy: http_statuses: - 200 - 302 interval: 0 successes: 0 http_path: \"/\" timeout: 1 unhealthy: http_failures: 0 http_statuses: - 429 interval: 0 tcp_failures: 0 timeouts: 0 passive: healthy: http_statuses: - 200 successes: 0 unhealthy: http_failures: 0 http_statuses: - 429 - 503 tcp_failures: 0 timeouts: 0 slots: 10proxy: protocol: http path: / connect_timeout: 10000 retries: 10 read_timeout: 10000 write_timeout: 10000route: methods: - GET - HEAD - POST - PUT - DELETE #- CONNECT #- OPTIONS #- TRACE #- PATCH regex_priority: 0 strip_path: false preserve_host: true protocols: - http - httpsKongConsumer示例123456apiVersion: configuration.konghq.com/v1kind: KongConsumermetadata: name: consumer-team-xusername: team-Xcustom_id: my_team_xKongCredential示例12345678apiVersion: configuration.konghq.com/v1kind: KongCredentialmetadata: name: credential-team-xconsumerRef: consumer-team-xtype: key-authconfig: key: 62eb165c070a41d5c1b58d9d3d725ca1Kubernetes Ingress配置在Ingress里面声明annotations属性，可以调用Kong Ingress Controller定义的CRD资源Github文档说明在此12345678910111213141516171819202122232425262728apiVersion: extensions/v1beta1kind: Ingressmetadata: name: tomcat-ingress namespace: default annotations: # 这里声明使用kong ingress controller kubernetes.io/ingress.class: \"kong\" # 声明使用kongIngress配置 configuration.konghq.com: \"configuration-demo\" # 声明使用KongPlugin plugins.konghq.com: \"ip-restriction,kong-prometheus,response-transformer\"spec: tls: - secretName: example-com-secret hosts: - \"*.example.com\" rules: # host定义域名 - host: tomcat.example.com # 定义HTTP服务 http: paths: - path: / backend: serviceName: tomcat-service # 这里可以写端口号或者端口别名http servicePort: 8080","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"},{"name":"Kong","slug":"Kubernetes/Kong","permalink":"https://luanlengli.github.io/categories/Kubernetes/Kong/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"},{"name":"Kong","slug":"Kong","permalink":"https://luanlengli.github.io/tags/Kong/"}]},{"title":"Kubernetes deployment模板","slug":"Kubernetes-deployment模板","date":"2019-07-02T02:58:36.000Z","updated":"2019-07-02T03:31:12.000Z","comments":true,"path":"2019/07/02/Kubernetes-deployment模板.html","link":"","permalink":"https://luanlengli.github.io/2019/07/02/Kubernetes-deployment模板.html","excerpt":"","text":"说明官方文档里面的deployment样例比较简单实际上deployment的配置选项非常的多这里做一下记录Dockerfile123FROM openjdk:8-jre-slimADD ./demo.jar /home/demo.jar模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197# 定义APP环境变量apiVersion: v1kind: ConfigMapmetadata: # 定义配置文件的名称 name: java-app-configmap # 定义运行在哪个命名空间 namespace: default labels: # 定义标签，后面可以通过标签选择器筛选 app: java-appdata: # 定义数据库连接地址和端口 DB_HOST: \"mysql-svc.default.svc.cluster.local\" DB_PORT: \"3306\"---apiVersion: apps/v1beta2kind: Deploymentmetadata: name: java-app # 定义运行在哪个命名空间 namespace: default labels: app: java-appspec: selector: matchLabels: app: java-app # 设置副本数量 replicas: 1 template: metadata: labels: app: java-app spec: # 禁止自动挂载kubernetes的ServiceAccount # 理解为禁止app通过kubernetes内建的ServiceAccount访问kubernetes集群 automountServiceAccountToken: false # 从镜像库拉镜像时使用用户名密码认证 imagePullSecrets: [] # 定义亲和性，调整调度策略 affinity: # 容器反亲和性 podAntiAffinity: # preferredDuringSchedulingIgnoredDuringExecution # 优先部署在满足条件的节点，如不满足则忽略条件继续调度 preferredDuringSchedulingIgnoredDuringExecution: # 这里定义亲和规则 # 调度时会检查候选节点上是否有带有app=java-app的app，权重为100 # 这里定义的是AntiAffinity，效果为同一个app在同一个节点上是互斥的 # 实现同一个app不会集中在同一个节点 # 极端情况下，节点数量不够，无法满足条件时，会忽略prefer规则，按照正常流程调度 - podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - java-app # 这里是作用域，没啥事不用调 topologyKey: \"kubernetes.io/hostname\" weight: 100 # 配置Pod挂载的卷和名字 volumes: # 这里定义Pod可以挂载哪些volume，容器挂载的volume要在这里定义好才可以使用 - name: timezone-volume # 定义宿主机文件路径 hostPath: path: /usr/share/zoneinfo/Asia/Shanghai # 容器初始化操作 initContainers: # 用于检测数据库就绪状态 # 检查条件为数据库服务器端口是否可达 - name: wait-for-db image: busybox:1.28 imagePullPolicy: IfNotPresent envFrom: # 这里从configmap里面获取环境变量 - configMapRef: name: java-app-configmap # 这里是容器初始化时，通过循环检查数据库是否就绪 command: - /bin/sh - -c - \"until nc -zv $(DB_HOST) $(DB_PORT) -w1; do echo 'waiting for db'; sleep 1; done\" # 启动容器 containers: # 业务容器 - name: java-app image: java-app:v1 # 镜像拉取策略 # IfNotPresent 如果没有就拉镜像 # Always 总是拉镜像，即使节点本地有镜像 # Nerver 不拉镜像 imagePullPolicy: IfNotPresent # 定义容器资源需求 resources: # 资源限制，超过限值会被杀掉 limits: # 限制CPU用量 # 1个核心的CPU等于1000m，这样多个容器是通过CPUShare共享CPU计算资源 # 如果直接写数字（1、2、3）这样的话，会被认为是将容器独占CPU核心 cpu: \"1500m\" # 限制容器最大内存用量 # 内存为不可压缩资源，超过限制会因为Out-Of-Memory被杀掉 # jdk-8u131+以上的版本可以通过强制检查Linux cgroup配置 memory: \"2Gi\" # Pod调度时会参考这里的资源需求调度 requests: # 限制CPU用量 # 1个核心的CPU等于1000m，这样多个容器是通过CPUShare共享CPU计算资源 # 如果直接写数字（1、2、3）这样的话，会被认为是将容器独占CPU核心 cpu: \"100m\" memory: \"2Gi\" env: # 这里定义JAVA_OPTS来让jvm启动时自动识别容器资源限值，然后计算出匹配的运行参数 # -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap开启openjdk8的实验特性以正确识别docker资源限值 # -XX:MaxRAMFraction默认值为4，即Heapsize为内存的25%，这里设置为2，即Heapsize为内存的50% # 可以通过-Xmx来指定jvm可使用的最大堆 - name: JAVA_OPTS value: \"-Xmx1800m\" # value: \"-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=2\" # 这里使用kubernetes的downwardAPI获取Pod的信息 # 对于某些APP需要获取Pod自身信息时可以直接调用环境变量 # Pod所在宿主机名字 - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName # Pod所在宿主机IP地址 - name: NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP # Pod名字 - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name # PodIP地址 - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP envFrom: # 这里直接从configmap里面获取环境变量 # 可以在command字段里面使用 # 具体定义了哪些环境变量，可以在configmap那里看到 - configMapRef: name: java-app-configmap # 设置容器工作目录 workingDir: /home # 设置容器启动命令和参数 command: - /usr/bin/java - -Djava.security.egd=file:/dev/./urandom - -Duser.timezone=Asia/Shanghai - -Xloggc:logs/gc.log - -jar - /home/demo.jar # 定义容器端口 ports: # 这里定义的端口别名可以在其他地方直接调用 - name: http # 定义容器运行时监听的端口，需要与Service的targetPort对应 containerPort: 8080 protocol: TCP # 定义容器存活探针 livenessProbe: # 使用TCP端口探测 tcpSocket: # 定义检查端口，可以写端口别名，也可以写端口号 port: 8080 # 容器初始化完成后延迟检测，单位秒 # 防止app启动时间过久导致被k8s判断为fail直接干掉 initialDelaySeconds: 60 # 检测频率 periodSeconds: 10 # 定义容器就绪探针 readinessProbe: # 使用HTTP GET请求 httpGet: path: /healthz port: 8080 httpHeaders: - name: readinessProbe value: k8s # 容器初始化完成后延迟检测，单位秒 # 防止app启动时间过久导致被k8s判断为fail直接干掉 initialDelaySeconds: 60 # 检测频率 periodSeconds: 10 # 把定义好的数据卷挂载到容器里面 volumeMounts: - name: timezone-volume # 这里用于配置容器的时区为Asia/Shanghai mountPath: /etc/localtime","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"Ubuntu-18.04-Server虚拟机模板制作","slug":"Ubuntu-18.04-Server虚拟机模板制作","date":"2019-06-29T04:20:34.000Z","updated":"2020-01-16T06:12:03.957Z","comments":true,"path":"2019/06/29/Ubuntu-18.04-Server虚拟机模板制作.html","link":"","permalink":"https://luanlengli.github.io/2019/06/29/Ubuntu-18.04-Server虚拟机模板制作.html","excerpt":"","text":"说明Ubuntu 18.04 LTS已经出来一年多了，小版本号已经更新到了18.04.2。这里记录一下Ubuntu 18.04虚拟机模板的制作过程下载镜像这里使用清华大学的镜像源↓↓↓↓↓↓↓↓下载地址↓↓↓↓↓↓↓↓ubuntu-18.04.2-live-server-amd64.iso创建虚拟机这里使用VMware Workstation 15 Pro 版本号15.0.2 build-10952284虚拟机规格客户机操作系统版本Ubuntu 64位处理器数量2内存2GB硬盘30GB网络适配器NAT模式安装操作系统启动菜单语言选择English选择Install Ubuntu Server或者Install Ubuntu Server with the HWE kernel都可以，关于HWE kernel的说明，看这里安装流程语言选择English键盘Layout和Variant都选择English (US)安装的类型选择Install Ubuntu即可，另外两个选择Install MAAS bare-metal cloud (region)和Install MAAS bare-metal cloud (region)可以看一下官方说明网络连接，这里根据需要配置即可代理地址设置，没有就直接跳过设置镜像地址，默认是http://archive.ubuntu.com/ubuntu，安装完系统之后可以修改，这里看需要配置即可配置分区，这里设置/boot分区1GB，/分区剩余空间，无swap分区，由于Ubuntu会自动创建一个1M大小的分区用于BIOS_GRUB，因此，实际上，硬盘分区是有三个的设置用户名密码和主机名，这里统一用Ubuntu了，密码这里尽量不要设置的太简单了Your name: ubuntuYour server’s name: ubuntuPick a username: ubuntuChoose a password:***********Confirm your password:***********设置SSH，选择Install Openssh Server，Import SSH identity选择NoFeatured Server Snaps这里可以在安装系统的时候顺带选择额外安装的软件包这个后续安装完系统可以自己操作，所以直接跳过，避免安装时间过长！接下来就等待系统安装完成，安装完成后选择Reboot Now重启操作系统启动后初始化设置配置ssh证书登录通过ssh命令生成密钥对将~/.ssh/id_rsa.pub提取出来1ssh-keygen -t rsa -b 4096 -N \"\" -f ~/.ssh/id_rsa添加sysctl参数fs参数12345678cat &gt; /etc/sysctl.d/99-fs.conf &lt;&lt;EOF# 最大文件句柄数fs.file-max=1048576# 最大文件打开数fs.nr_open=1048576# 同一时间异步IO请求数fs.aio-max-nr=1048576EOFvm参数12345678910cat &gt; /etc/sysctl.d/99-vm.conf &lt;&lt;EOF# 内存耗尽才使用swap分区vm.swappiness=10# 当内存耗尽时，内核会触发OOM killer根据oom_score杀掉最耗内存的进程vm.panic_on_oom=0# 允许overcommitvm.overcommit_memory=1# 定义了进程能拥有的最多内存区域，默认65536vm.max_map_count=262144EOFnet参数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465cat &gt; /etc/sysctl.d/99-net.conf &lt;&lt;EOF# 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤net.bridge.bridge-nf-call-arptables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# 关闭严格校验数据包的反向路径，默认值1net.ipv4.conf.default.rp_filter=0net.ipv4.conf.all.rp_filter=0# 进程间通信发送数据, 默认100net.unix.max_dgram_qlen=512# 设置 conntrack 的上限net.netfilter.nf_conntrack_max=1048576# 设置连接跟踪表中处于TIME_WAIT状态的超时时间net.netfilter.nf_conntrack_tcp_timeout_timewait=30# 设置连接跟踪表中TCP连接超时时间net.netfilter.nf_conntrack_tcp_timeout_established=1200# 端口最大的监听队列的长度net.core.somaxconn=21644# 接收自网卡、但未被内核协议栈处理的报文队列长度net.core.netdev_max_backlog=262144# 系统无内存压力、启动压力模式阈值、最大值，单位为页的数量#net.ipv4.tcp_mem=1541646 2055528 3083292# 内核socket接收缓存区字节数min/default/maxnet.core.rmem=4096 65536 8388608# 内核socket发送缓存区字节数min/default/maxnet.core.wmem=4096 65536 8388608# 开启自动调节缓存模式net.ipv4.tcp_moderate_rcvbuf=1# TCP阻塞控制算法BBR，Linux内核版本4.9开始内置BBR算法#net.ipv4.tcp_congestion_control=bbr#net.core.default_qdisc=fq# 用作本地随机TCP端口的范围net.ipv4.ip_local_port_range=10000 65000# 打开ipv4数据包转发net.ipv4.ip_forward=1# 允许应用程序能够绑定到不属于本地网卡的地址net.ipv4.ip_nonlocal_bind=1# 系统中处于 SYN_RECV 状态的 TCP 连接数量net.ipv4.tcp_max_syn_backlog=16384# 内核中管理 TIME_WAIT 状态的数量net.ipv4.tcp_max_tw_buckets=5000# 指定重发 SYN/ACK 的次数net.ipv4.tcp_synack_retries=2# TCP连接中TIME_WAIT sockets的快速回收net.ipv4.tcp_tw_recycle=0# 不属于任何进程的tcp socket最大数量. 超过这个数量的socket会被reset, 并告警net.ipv4.tcp_max_orphans=1024# TCP FIN报文重试次数net.ipv4.tcp_orphan_retries=8# 加快系统关闭处于 FIN_WAIT2 状态的 TCP 连接net.ipv4.tcp_fin_timeout=15# TCP连接keepalive的持续时间，默认7200net.ipv4.tcp_keepalive_time=600# TCP keepalive探测包发送间隔net.ipv4.tcp_keepalive_intvl=30# TCP keepalive探测包重试次数net.ipv4.tcp_keepalive_probes=10# TCP FastOpen# 0:关闭 ; 1:作为客户端时使用 ; 2:作为服务器端时使用 ; 3:无论作为客户端还是服务器端都使用net.ipv4.tcp_fastopen=3# 限制TCP重传次数net.ipv4.tcp_retries1=3# TCP重传次数到达上限时，关闭TCP连接net.ipv4.tcp_retries2=15EOF修改limits参数1234sudo cat &gt; /etc/security/limits.d/99-ubuntu.conf &lt;&lt;EOF* - nproc 1048576* - nofile 1048576EOF修改journal设置12345sudo sed -e 's,^#Compress=yes,Compress=yes,' \\ -e 's,^#SystemMaxUse=,SystemMaxUse=5G,' \\ -e 's,^#Seal=yes,Seal=yes,' \\ -e 's,^#RateLimitBurst=1000,RateLimitBurst=5000,' \\ -i /etc/systemd/journald.conf修改终端提示符1234export PS1='[\\t]\\[\\033[1;31m\\]&lt;\\u@\\h:\\w&gt;\\[\\033[0m\\]\\$ 'cat &gt;&gt; ~/.bashrc &lt;&lt; EOFexport PS1='[\\t]\\[\\033[1;31m\\]&lt;\\u@\\h:\\w&gt;\\[\\033[0m\\]\\\\$ 'EOF修改APT源/etc/apt/sources.list12345678910111213# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse# deb-src https://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse# deb-src https://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse# deb-src https://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse# deb-src https://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse# deb-src https://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse刷新APT缓存1sudo apt update更新系统软件1sudo apt upgrade安装常用软件12345678910111213141516171819202122232425262728293031sudo apt install git \\ tree \\ software-properties-common \\ dirmngr \\ apt-transport-https \\ jq \\ socat \\ tcpdump \\ vim \\ ipvsadm \\ tree \\ dstat \\ iotop \\ htop \\ atop \\ socat \\ ipset \\ conntrack \\ netcat \\ bash-completion \\ ufw \\ ca-certificates \\ curl \\ gnupg2 \\ sudo \\ unzip \\ uuid \\ gnupg-agent \\ sysstat \\ nethogs \\ linux-tools-common禁用系统服务123456systemctl disable iscsid.socket \\ iscsi.service \\ open-iscsi.service \\ rsync.service \\ ufw.service \\ uuidd.socket配置网络Ubuntu 18.04 LTS 使用netplan来管理网络配置，可以使用NetworkManager或者Systemd-networkd的网络守护程序来做为内核的接口。如果再通过原来的 ifupdown 工具包继续在 /etc/network/interfaces 文件里配置管理网络接口是无效的。默认的systemd-resolve会接管/etc/resolv.conf，无法直接修改，并且会监听localhost:53端口，看着非常不爽。修改过程如下网卡配置文件路径/etc/netplan/50-cloud-init.yaml，配置文件的样例在这里12345678910111213141516171819# This file is generated from information provided by# the datasource. Changes to it will not persist across an instance.# To disable cloud-init's network configuration capabilities, write a file# /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:# network: &#123;config: disabled&#125;network: ethernets: ens33: addresses: - 172.16.80.100/24 dhcp4: no gateway4: 172.16.80.2 nameservers: addresses: - 114.114.114.114 - 8.8.8.8 search: [] renderer: networkd version: 2使用netplan命令让配置生效1sudo netplan apply这时候会发现，/etc/resolv.conf里面的nameserver指向127.0.0.53并且是软链接到/run/systemd/resolve/stub-resolv.conf内容如下123456789101112131415161718# This file is managed by man:systemd-resolved(8). Do not edit.## This is a dynamic resolv.conf file for connecting local clients to the# internal DNS stub resolver of systemd-resolved. This file lists all# configured search domains.## Run \"systemd-resolve --status\" to see details about the uplink DNS servers# currently in use.## Third party programs must not access this file directly, but only through the# symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a different way,# replace this symlink by a static file or a different symlink.## See man:systemd-resolved.service(8) for details about the supported modes of# operation for /etc/resolv.conf.nameserver 127.0.0.53options edns0修改systemd-resolv的配置文件/etc/systemd/resolved.conf12345678910111213141516171819202122# This file is part of systemd.## systemd is free software; you can redistribute it and/or modify it# under the terms of the GNU Lesser General Public License as published by# the Free Software Foundation; either version 2.1 of the License, or# (at your option) any later version.## Entries in this file show the compile time defaults.# You can change settings by editing this file.# Defaults can be restored by simply deleting this file.## See resolved.conf(5) for details[Resolve]#DNS=#FallbackDNS=#Domains=LLMNR=noMulticastDNS=noDNSSEC=noCache=yesDNSStubListener=no重启systemd-resolv服务1systemctl restart systemd-resolved.service修改/etc/resolv.conf软链接指向1ln -svf /run/systemd/resolve/resolv.conf /etc/resolv.conf现在再看/etc/resolv.conf的内容就舒服了1234567891011121314# This file is managed by man:systemd-resolved(8). Do not edit.## This is a dynamic resolv.conf file for connecting local clients directly to# all known uplink DNS servers. This file lists all configured search domains.## Third party programs must not access this file directly, but only through the# symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a different way,# replace this symlink by a static file or a different symlink.## See man:systemd-resolved.service(8) for details about the supported modes of# operation for /etc/resolv.conf.nameserver 114.114.114.114nameserver 8.8.8.8修改HISTORY参数1234567cat &gt; /etc/profile.d/history.sh &lt;&lt;EOFexport HISTSIZE=10000export HISTFILESIZE=10000export HISTCONTROL=ignoredupsexport HISTTIMEFORMAT=\"`whoami` %F %T \"export HISTIGNORE=\"ls:pwd:ll:ls -l:ls -a:ll -a\"EOF修改时区1timedatectl set-timezone Asia/Shanghai配置时间同步Ubuntu 18.04 LTS 使用systemd-timesyncd实现跨网络同步系统时钟的守护服务，与NTP的复杂实现相比，这个服务简单的多，它只专注于从远程服务器查询然后同步到本地时钟。守护进程运行只需要尽可能小特权，并且会跟网络服务 networkd 挂钩，仅在网络连接可用时才工作。配置文件路径/etc/systemd/timesyncd.conf1sed -e 's,^#NTP=.*,NTP=cn.pool.ntp.org,' -i /etc/systemd/timesyncd.conf重启systemd-timesyncd服务1systemctl restart systemd-timesyncd.service修改LANG默认值123localectl set-locale LANG=en_US.UTF-8localectl set-keymap uslocalectl set-x11-keymap us修改SSH配置这里禁用root通过密码方式登录，修改默认端口22→223312345sed -e 's,^#PermitRootLogin prohibit-password,PermitRootLogin prohibit-password,' \\ -e 's,^#PubkeyAuthentication yes,PubkeyAuthentication yes,' \\ -e 's,^#UseDNS no,UseDNS no,' \\ -e 's,^#Port 22,Port 2233,' \\ -i /etc/ssh/sshd_config可选操作禁用终端欢迎消息广告关闭获取Ubuntu新闻1sed -e 's,^ENABLED=1,ENABLED=0,g' -i /etc/default/motd-news关闭动态motd不需要的内容12chmod -x /etc/update-motd.d/80-livepatchchmod -x /etc/update-motd.d/10-help-text禁用IPV6设置123456cat &gt; /etc/sysctl.d/99-disable-ipv6.conf &lt;&lt;EOF# 禁用ipv6net.ipv6.conf.all.disable_ipv6=1net.ipv6.conf.default.disable_ipv6=1net.ipv6.conf.lo.disable_ipv6=1EOF禁用ICMP1234cat &gt; /etc/sysctl.d/99-disable-icmp.conf &lt;&lt;EOFnet.ipv4.icmp_echo_ignore_all=1net.ipv4.icmp_echo_ignore_broadcasts=1EOF添加vim设置将vim设置写入~/.vimrc文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263cat &gt; ~/.vimrc &lt;&lt;EOF\" 显示行号set number\" 高亮光标所在行set cursorline\" 打开语法显示syntax on\" 关闭备份set nobackup\" 没有保存或文件只读时弹出确认set confirm\" 禁用modeline功能set nomodeline\" tab缩进set tabstop=4set shiftwidth=4set expandtabset smarttab\" 默认缩进4个空格大小 set shiftwidth=4 \" 文件自动检测外部更改set autoread\" 高亮查找匹配set hlsearch\" 显示匹配set showmatch\" 背景色设置为黑色set background=dark\" 浅色高亮显示当前行autocmd InsertLeave * se nocul\" 显示输入的命令set showcmd\" 字符编码set encoding=utf-8\" 开启终端256色显示set t_Co=256\" 增量式搜索 set incsearch\" 设置默认进行大小写不敏感查找set ignorecase\" 如果有一个大写字母，则切换到大小写敏感查找set smartcase\" 不产生swap文件set noswapfile\" 设置备份时的行为为覆盖set backupcopy=yes\" 关闭提示音set noerrorbells\" 历史记录set history=10000\" 显示行尾空格set listchars=tab:»■,trail:■\" 显示非可见字符set list\" c文件自动缩进set cindent\" 文件自动缩进set autoindent\" 检测文件类型filetype on\" 智能缩进set smartindentEOF配置内核模块配置lvs模块LVS的调度算法简介12345678910111213cat &gt; /etc/modules-load.d/ipvs.conf &lt;&lt;EOFip_vs# 负载均衡调度算法-最少连接ip_vs_lc# 负载均衡调度算法-加权最少连接ip_vs_wlc# 负载均衡调度算法-轮询ip_vs_rr# 负载均衡调度算法-加权轮询ip_vs_wrr# 源地址散列调度算法ip_vs_shEOF配置连接状态跟踪模块12345cat &gt; /etc/modules-load.d/nf_conntrack.conf &lt;&lt;EOFnf_conntracknf_conntrack_ipv4#nf_conntrack_ipv6EOF配置kvm模块12345678910cat &gt; /etc/modules-load.d/kvm.conf &lt;&lt;EOF# Intel CPU开启嵌套虚拟化options kvm-intel nested=1options kvm-intel enable_shadow_vmcs=1options kvm-intel enable_apicv=1options kvm-intel ept=1# AMD CPU开启嵌套虚拟化#options kvm-amd nested=1EOF安装Docker-CE添加GPG KEY1curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | apt-key add -添加APT源1234add-apt-repository \\ \"deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"刷新APT缓存1apt update查找Docker-CE版本1apt-cache madison docker-ce输出示例12345678910111213docker-ce | 5:18.09.7~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:18.09.6~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:18.09.5~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:18.09.4~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:18.09.3~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:18.09.2~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:18.09.1~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 5:18.09.0~3-0~ubuntu-bionic | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 18.06.3~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 18.06.2~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 18.06.1~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 18.06.0~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packagesdocker-ce | 18.03.1~ce~3-0~ubuntu | https://mirrors.aliyun.com/docker-ce/linux/ubuntu bionic/stable amd64 Packages安装指定版本的Docker-CE123apt install docker-ce=5:18.09.7~3-0~ubuntu-bionic \\ docker-ce-cli=5:18.09.7~3-0~ubuntu-bionic \\ containerd.io查看docker信息1docker info修改docker配置配置文件路径/etc/docker/daemon.json12345678910111213&#123; \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\"], \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"insecure-registries\": [], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\": \"3\" &#125;, \"data-root\": \"/var/lib/docker\", \"storage-driver\": \"overlay2\", \"max-concurrent-downloads\": 10&#125;启动docker1systemctl restart docker.service","categories":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/tags/Linux/"}]},{"title":"Kubernetes部署metrics-server监控集群性能","slug":"Kubernetes监控组件metrics-server部署","date":"2019-06-25T12:32:18.000Z","updated":"2019-08-05T13:33:16.000Z","comments":true,"path":"2019/06/25/Kubernetes监控组件metrics-server部署.html","link":"","permalink":"https://luanlengli.github.io/2019/06/25/Kubernetes监控组件metrics-server部署.html","excerpt":"","text":"说明Metrics Server是实现了 Metrics API 的元件,其目标是取代 Heapster 作位 Pod 与 Node 提供资源的 Usage metrics,该元件会从每个 Kubernetes 节点上的 Kubelet 所公开的 Summary API 中收集 MetricsHorizontal Pod Autoscaler（HPA）控制器用于实现基于CPU使用率进行自动Pod伸缩的功能。HPA控制器基于Master的kube-controller-manager服务启动参数–horizontal-pod-autoscaler-sync-period定义是时长（默认30秒）,周期性监控目标Pod的CPU使用率,并在满足条件时对ReplicationController或deployment中的Pod副本数进行调整,以符合用户定义的平均Pod CPU使用率。在新版本的kubernetes中 Pod CPU使用率不在来源于heapster,而是来自于metrics-server官网原话是 The –horizontal-pod-autoscaler-use-rest-clients is true or unset. Setting this to false switches to Heapster-based autoscaling, which is deprecated.准备工作kube-apiserver运行参数metric-server是扩展的apiserver，依赖于kube-aggregator，因此需要在apiserver中开启相关参数1234567--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem--requestheader-allowed-names=aggregator--requestheader-group-headers=X-Remote-Group--requestheader-extra-headers-prefix=X-Remote-Extra---requestheader-username-headers=X-Remote-Usermetrics-server版本兼容Metrics ServerMetrics API group/versionSupported Kubernetes version0.3.xmetrics.k8s.io/v1beta11.8+0.2.xmetrics.k8s.io/v1beta11.8+0.1.xmetrics/v1alpha11.7metrics-server部署创建工作目录1mkdir -p /home/k8s/下载项目代码1wget -O - https://github.com/kubernetes-incubator/metrics-server/archive/v0.3.1.tar.gz | tar xz --directory=/home/k8s/修改deployment文件路径/home/k8s/metrics-server-v0.3.1/deploy/1.8+/metrics-server-deployment.yaml1234567891011containers:- name: metrics-server image: gcrxio/metrics-server-amd64:v0.3.1 command: - /metrics-server - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS - --kubelet-insecure-tls - --requestheader-extra-headers-prefix=x-remote-extra- - --requestheader-group-headers=x-remote-group - --requestheader-username-headers=x-remote-user - -v=2部署metrics-server1kubectl apply -f /home/k8s/metrics-server-v0.3.1/deploy/1.8+/查看pod状态123kubectl -n kube-system get pod -l k8s-app=metrics-serverNAME READY STATUS RESTARTS AGEpod/metrics-server-86bd9d7667-5hbn6 1/1 Running 0 1m验证metrics完成后,等待一段时间(约 30s - 1m)收集 Metrics请求metrics api的结果1kubectl get --raw /apis/metrics.k8s.io/v1beta1示例输出123456789101112131415161718192021222324252627&#123; \"kind\": \"APIResourceList\", \"apiVersion\": \"v1\", \"groupVersion\": \"metrics.k8s.io/v1beta1\", \"resources\": [ &#123; \"name\": \"nodes\", \"singularName\": \"\", \"namespaced\": false, \"kind\": \"NodeMetrics\", \"verbs\": [ \"get\", \"list\" ] &#125;, &#123; \"name\": \"pods\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"PodMetrics\", \"verbs\": [ \"get\", \"list\" ] &#125; ]&#125;获取节点性能数据1kubectl top node输出示例123456NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-m1 695m 17% 914Mi 11% k8s-m2 360m 9% 553Mi 7% k8s-m3 492m 12% 533Mi 6% k8s-n1 144m 3% 311Mi 3% k8s-n2 149m 3% 321Mi 4%获取Pod性能数据1kubectl -n kube-system top pod输出示例1234567891011121314NAME CPU(cores) MEMORY(bytes) coredns-56c964478c-g7q8n 14m 14Mi coredns-56c964478c-zmt2j 13m 14Mi kube-flannel-ds-amd64-8g6f9 6m 15Mi kube-flannel-ds-amd64-hslkh 6m 21Mi kube-flannel-ds-amd64-ppqkb 8m 14Mi kube-flannel-ds-amd64-swj8m 7m 14Mi kube-flannel-ds-amd64-zs2sd 8m 17Mi kube-proxy-2dq5x 18m 23Mi kube-proxy-9lzmj 28m 23Mi kube-proxy-9xtjc 18m 23Mi kube-proxy-w7mtg 2m 23Mi kube-proxy-zvp8d 2m 21Mi metrics-server-v0.3.1-98bdfb766-s6jf6 8m 22Mi","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"Prometheus使用RemoteStorage将监控数据写入TimescaleDB","slug":"Prometheus使用RemoteStorage将监控数据写入TimescaleDB","date":"2019-06-25T07:28:52.000Z","updated":"2019-07-16T14:34:19.000Z","comments":true,"path":"2019/06/25/Prometheus使用RemoteStorage将监控数据写入TimescaleDB.html","link":"","permalink":"https://luanlengli.github.io/2019/06/25/Prometheus使用RemoteStorage将监控数据写入TimescaleDB.html","excerpt":"","text":"说明prometheus-postgresql-adapter最新版本是2018-10-17发布的v0.4.1不保证ctrl+c和ctrl+v能直接跑起来仅用于验证功能记录实验操作过程实验环境主机名IP地址说明timescaledb172.16.80.201部署TimescaleDB和prometheus-postgresql-adapterprometheus172.16.80.202部署prometheus和node_exporter部署TimescaleDB参考文档【PostgreSQL10安装部署和初始化】【PostgreSQL10搭建时间序列数据库TimescaleDB】添加YUM源使用清华大学的RPM文件1yum install -y https://mirrors.tuna.tsinghua.edu.cn/postgresql/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm替换YUM源地址1sed -e 's,download.postgresql.org/pub,mirrors4.tuna.tsinghua.edu.cn/postgresql,g' -i /etc/yum.repos.d/pgdg-redhat-all.repo安装PostgreSQL101234yum install -y postgresql10 \\ postgresql10-server \\ postgresql10-contrib \\ postgresql10-test安装TimescaleDB1yum install -y timescaledb_10安装pg_prometheus安装编译环境1yum install -y postgresql10-devel make gcc make下载源代码1wget -O - https://github.com/timescale/pg_prometheus/archive/0.2.1.tar.gz | tar xz切换目录1cd pg_prometheus-0.2.1编译安装12export PATH=/usr/pgsql-10/bin:$PATHmake &amp;&amp; make install初始化数据库切换到postgres用户1su - postgres初始化数据库123456/usr/pgsql-10/bin/initdb --encoding=UTF-8 \\ --local=en_US.UTF8 \\ --username=postgres \\ --pwprompt \\ --pgdata=$PGDATA \\ --data-checksums配置数据库postgresql.conf最重要的是添加shared_preload_libraries = &#39;timescaledb, pg_prometheus&#39;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406# -----------------------------# PostgreSQL configuration file# -----------------------------## This file consists of lines of the form:## name = value## (The \"=\" is optional.) Whitespace may be used. Comments are introduced with# \"#\" anywhere on a line. The complete list of parameter names and allowed# values can be found in the PostgreSQL documentation.## The commented-out settings shown in this file represent the default values.# Re-commenting a setting is NOT sufficient to revert it to the default value;# you need to reload the server.## This file is read on server startup and when the server receives a SIGHUP# signal. If you edit the file on a running system, you have to SIGHUP the# server for the changes to take effect, run \"pg_ctl reload\", or execute# \"SELECT pg_reload_conf()\". Some parameters, which are marked below,# require a server shutdown and restart to take effect.## Any parameter can also be given as a command-line option to the server, e.g.,# \"postgres -c log_connections=on\". Some parameters can be changed at run time# with the \"SET\" SQL command.## Memory units: kB = kilobytes Time units: ms = milliseconds# MB = megabytes s = seconds# GB = gigabytes min = minutes# TB = terabytes h = hours# d = days#------------------------------------------------------------------------------# FILE LOCATIONS#------------------------------------------------------------------------------#data_directory = 'ConfigDir'#hba_file = 'ConfigDir/pg_hba.conf'#ident_file = 'ConfigDir/pg_ident.conf'#external_pid_file = ''#------------------------------------------------------------------------------# CONNECTIONS AND AUTHENTICATION#------------------------------------------------------------------------------# - Connection Settings -listen_addresses = '*'port = 5432# max_connections默认是100max_connections = 500superuser_reserved_connections = 3# - TCP settings -# 默认值为0，即使用系统默认值#tcp_keepalives_idle = 0#tcp_keepalives_interval = 0#tcp_keepalives_count = 0#tcp_user_timeout = 0# - Authentication -# 认证超时时间，默认1minauthentication_timeout = 30s# 密码加密算法，默认md5password_encryption = md5# GSSAPI using Kerberos#krb_server_keyfile = ''#krb_caseins_users = off# - SSL -ssl = off#ssl_ca_file = ''#ssl_cert_file = 'server.crt'#ssl_crl_file = ''#ssl_key_file = 'server.key'#ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphers#ssl_prefer_server_ciphers = on#ssl_ecdh_curve = 'prime256v1'#ssl_min_protocol_version = 'TLSv1'#ssl_max_protocol_version = ''#ssl_dh_params_file = ''#ssl_passphrase_command = ''#ssl_passphrase_command_supports_reload = off#------------------------------------------------------------------------------# RESOURCE USAGE (except WAL)#------------------------------------------------------------------------------# - Memory -shared_buffers = 128MB#huge_pages = try#temp_buffers = 8MB#max_prepared_transactions = 0#work_mem = 4MB#maintenance_work_mem = 64MB#autovacuum_work_mem = -1#max_stack_depth = 2MB#shared_memory_type = mmapdynamic_shared_memory_type = posix# - Disk -#temp_file_limit = -1# - Kernel Resources -#max_files_per_process = 1000shared_preload_libraries = 'timescaledb, pg_prometheus'# - Cost-Based Vacuum Delay -#vacuum_cost_delay = 0#vacuum_cost_page_hit = 1#vacuum_cost_page_miss = 10#vacuum_cost_page_dirty = 20#vacuum_cost_limit = 200# - Background Writer -#bgwriter_delay = 200ms#bgwriter_lru_maxpages = 100#bgwriter_lru_multiplier = 2.0#bgwriter_flush_after = 0# - Asynchronous Behavior -#effective_io_concurrency = 1#max_worker_processes = 8#max_parallel_maintenance_workers = 2#max_parallel_workers_per_gather = 2#parallel_leader_participation = on#max_parallel_workers = 8#old_snapshot_threshold = -1#backend_flush_after = 0#------------------------------------------------------------------------------# WRITE AHEAD LOG#------------------------------------------------------------------------------# - Settings -wal_level = replicafsync = onsynchronous_commit = onwal_sync_method = fdatasync#full_page_writes = on#wal_compression = off# 如需要用pg_rewind修复WAL的timeline, 需要打开wal_log_hints# 但是开启它会导致写wal变多, 请斟酌wal_log_hints = off#wal_init_zero = on#wal_recycle = on#wal_buffers = -1#wal_writer_delay = 200ms#wal_writer_flush_after = 1MB#commit_delay = 0#commit_siblings = 5# - Checkpoints -#checkpoint_timeout = 5minmax_wal_size = 1GBmin_wal_size = 80MB#checkpoint_completion_target = 0.5#checkpoint_flush_after = 0#checkpoint_warning = 30s# - Archiving -#archive_mode = off#archive_command = ''#archive_timeout = 0#------------------------------------------------------------------------------# REPLICATION#------------------------------------------------------------------------------# - Sending Server(s) -max_wal_senders = 10# wal_keep_segments可以设置大一点，每个segments大小16MB# 这样在备库不会因为wal receiver启动太慢导致所需wal在主库被删除wal_sender_timeout = 60s#max_replication_slots = 10#track_commit_timestamp = off# - Master Server -#synchronous_standby_names = ''#vacuum_defer_cleanup_age = 0# - Standby Servers -#hot_standby = on#max_standby_archive_delay = 30s#max_standby_streaming_delay = 30s#wal_receiver_status_interval = 10s#hot_standby_feedback = off#wal_receiver_timeout = 60ss#wal_retrieve_retry_interval = 5s# - Subscribers -#max_logical_replication_workers = 4#max_sync_workers_per_subscription = 2#------------------------------------------------------------------------------# QUERY TUNING#------------------------------------------------------------------------------# - Planner Method Configuration -#enable_bitmapscan = on#enable_hashagg = on#enable_hashjoin = on#enable_indexscan = on#enable_indexonlyscan = on#enable_material = on#enable_mergejoin = on#enable_nestloop = on#enable_seqscan = on#enable_sort = on#enable_tidscan = on# - Planner Cost Constants -#seq_page_cost = 1.0#random_page_cost = 4.0#cpu_tuple_cost = 0.01#cpu_index_tuple_cost = 0.005#cpu_operator_cost = 0.0025#parallel_tuple_cost = 0.1#parallel_setup_cost = 1000.0#min_parallel_table_scan_size = 8MB#min_parallel_index_scan_size = 512kB#effective_cache_size = 4GB# - Genetic Query Optimizer -#geqo = on#geqo_threshold = 12#geqo_effort = 5#geqo_pool_size = 0#geqo_generations = 0#geqo_selection_bias = 2.0#geqo_seed = 0.0# - Other Planner Options -#default_statistics_target = 100#constraint_exclusion = partition#cursor_tuple_fraction = 0.1#from_collapse_limit = 8#join_collapse_limit = 8#force_parallel_mode = off#------------------------------------------------------------------------------# ERROR REPORTING AND LOGGING#------------------------------------------------------------------------------# - Where to Log -log_destination = 'stderr'logging_collector = onlog_directory = 'log'log_filename = 'postgresql-%Y%m%d.log'#log_file_mode = 0600log_truncate_on_rotation = offlog_rotation_age = 1dlog_rotation_size = 0# These are relevant when logging to syslog:#syslog_facility = 'LOCAL0'#syslog_ident = 'postgres'#syslog_sequence_numbers = on#syslog_split_messages = on# - When to Log -#log_min_messages = warning #log_min_error_statement = errorlog_min_duration_statement = 0 # - What to Log -#debug_print_parse = off#debug_print_rewritten = off#debug_print_plan = off#debug_pretty_print = onlog_checkpoints = on log_connections = on log_disconnections = on log_duration = on log_error_verbosity = verbose#log_hostname = offlog_line_prefix = '%m [%p]: user=%u,db=%d 'log_lock_waits = on#log_statement = 'none'#log_replication_commands = off#log_temp_files = -1log_timezone = 'PRC'# - Process Title -#cluster_name = ''#update_process_title = on#------------------------------------------------------------------------------# RUNTIME STATISTICS#------------------------------------------------------------------------------# - Query/Index Statistics Collector -track_activities = ontrack_counts = ontrack_io_timing = ontrack_functions = nonetrack_activity_query_size = 1024stats_temp_directory = 'pg_stat_tmp'# - Statistics Monitoring -log_parser_stats = onlog_planner_stats = onlog_executor_stats = onlog_statement_stats = off#------------------------------------------------------------------------------# AUTOVACUUM PARAMETERS#------------------------------------------------------------------------------#autovacuum = on#log_autovacuum_min_duration = -1#autovacuum_max_workers = 3#autovacuum_naptime = 1min#autovacuum_vacuum_threshold = 50#autovacuum_analyze_threshold = 50#autovacuum_vacuum_scale_factor = 0.2#autovacuum_analyze_scale_factor = 0.1#autovacuum_freeze_max_age = 200000000#autovacuum_multixact_freeze_max_age = 400000000#autovacuum_vacuum_cost_delay = 20ms#autovacuum_vacuum_cost_limit = -1#------------------------------------------------------------------------------# CLIENT CONNECTION DEFAULTS#------------------------------------------------------------------------------# - Statement Behavior -#client_min_messages = notice#search_path = '\"$user\", public'#default_tablespace = ''#temp_tablespaces = ''#check_function_bodies = on#default_transaction_isolation = 'read committed'#default_transaction_read_only = off#default_transaction_deferrable = off#session_replication_role = 'origin'#statement_timeout = 0#lock_timeout = 0#idle_in_transaction_session_timeout = 0#vacuum_freeze_min_age = 50000000#vacuum_freeze_table_age = 150000000#vacuum_multixact_freeze_min_age = 5000000#vacuum_multixact_freeze_table_age = 150000000#bytea_output = 'hex'#xmlbinary = 'base64'#xmloption = 'content'#gin_fuzzy_search_limit = 0#gin_pending_list_limit = 4MB# - Locale and Formatting -datestyle = 'iso, ymd'#intervalstyle = 'postgres'timezone = 'PRC'#timezone_abbreviations = 'Default'#extra_float_digits = 0#client_encoding = sql_ascii# These settings are initialized by initdb, but they can be changed.lc_messages = 'en_US.UTF-8' # locale for system error message stringslc_monetary = 'en_US.UTF-8' # locale for monetary formattinglc_numeric = 'en_US.UTF-8' # locale for number formattinglc_time = 'en_US.UTF-8' # locale for time formatting# default configuration for text searchdefault_text_search_config = 'pg_catalog.english'# - Other Defaults -#dynamic_library_path = '$libdir'#local_preload_libraries = ''#session_preload_libraries = ''#------------------------------------------------------------------------------# LOCK MANAGEMENT#------------------------------------------------------------------------------#deadlock_timeout = 1s#max_locks_per_transaction = 64#max_pred_locks_per_transaction = 64#max_pred_locks_per_relation = -2#max_pred_locks_per_page = 2#------------------------------------------------------------------------------# VERSION/PLATFORM COMPATIBILITY#------------------------------------------------------------------------------# - Previous PostgreSQL Versions -#array_nulls = on#backslash_quote = safe_encoding#default_with_oids = off#escape_string_warning = on#lo_compat_privileges = off#operator_precedence_warning = off#quote_all_identifiers = off#standard_conforming_strings = on#synchronize_seqscans = on# - Other Platforms and Clients -#transform_null_equals = off#------------------------------------------------------------------------------# ERROR HANDLING#------------------------------------------------------------------------------#exit_on_error = off#restart_after_crash = on#data_sync_retry = off#------------------------------------------------------------------------------# CONFIG FILE INCLUDES#------------------------------------------------------------------------------#include_dir = ''#include_if_exists = ''#include = ''#------------------------------------------------------------------------------# CUSTOMIZED OPTIONS#------------------------------------------------------------------------------# Add settings for extensions herepg_hba.conf1234567# TYPE DATABASE USER ADDRESS METHOD# 默认配置数据库主机以socket、127.0.0.1、::1的方式连接数据库可以跳过认证阶段直接登录数据库local all all trusthost all all 127.0.0.1/32 trusthost all all ::1/128 trust# 配置app_user可以基于用户名密码+MD5加密算法从0.0.0.0/0访问数据库服务，并且是能访问所有databaseshost all all 0.0.0.0/0 md5启动数据库12systemctl enable postgresql-10.servicesystemctl start postgresql-10.service创建Prometheus数据库登录数据库1psql -U postgres创建用户1CREATE ROLE prometheus WITH LOGIN PASSWORD 'prometheus_password';创建数据库1create database prometheus owner prometheus;切换数据库1\\connect prometheus ;添加Extension12CREATE EXTENSION pg_prometheus;CREATE EXTENSION timescaledb;授权1grant all on SCHEMA prometheus TO prometheus;创建表只使用pg_prometheus1SELECT create_prometheus_table('metrics');TimescaleDB可以实现时序数据库的功能，带来更强的性能和可扩展性。如果postgresql已经安装了TimescaleDB的Extension，则会自动启用TimescaleDB。可以显式的让pg_prometheus使用TimescaleDB扩展。1SELECT create_prometheus_table('metrics',use_timescaledb=&gt;true);部署Adapter下载二进制文件1wget -O - https://github.com/timescale/prometheus-postgresql-adapter/releases/download/0.4.1/prometheus-postgresql-adapter-0.4.1-linux-amd64.tar.gz | tar xz拷贝到PATH目录1mv prometheus-postgresql-adapter /usr/local/bin/创建systemd服务/usr/lib/systemd/system/prometheus-postgresql-adapter.service123456789101112131415161718192021[Unit]Description=prometheus-postgresql-adapterAfter=network.target[Service]Type=simpleUser=nobodyExecStart=/usr/local/bin/prometheus-postgresql-adapter \\ -adapter.send-timeout 30s \\ -log.level info \\ -pg.database prometheus \\ -pg.host timescaledb \\ -pg.port 5432 \\ -pg.user prometheus \\ -pg.password prometheus_password \\ -web.listen-address :9201 \\ -web.telemetry-path /metricsRestart=on-failureRestartSec=60s[Install]WantedBy=multi-user.target启动Adapter123systemctl daemon-reloadsystemctl enable prometheus-postgresql-adapter.servicesystemctl start prometheus-postgresql-adapter.service部署Prometheus参考文档【Prometheus搭建简单的监控告警系统】环境准备12345678# 创建prometheus配置目录mkdir -p /etc/prometheus /etc/prometheus/rules.d# 创建数据目录mkdir -p /var/lib/prometheus# 创建prometheus用户useradd prometheus# 修改属主属组chown -R prometheus:prometheus /etc/prometheus /var/lib/prometheus下载软件包1wget -O - https://github.com/prometheus/prometheus/releases/download/v2.9.2/prometheus-2.9.2.linux-amd64.tar.gz | tar xz拷贝到PATH目录12345678cd prometheus-2.9.2.linux-amd64chown -R prometheus:prometheus prometheus \\ promtool \\ console_libraries \\ consoles \\ prometheus.ymlmv prometheus promtool /usr/local/bin/mv console_libraries consoles prometheus.yml /etc/prometheus/配置prometheus/etc/prometheus/prometheus.yml1234567891011121314151617181920212223242526272829global: scrape_interval: 15s scrape_timeout: 10s evaluation_interval: 15salerting: &#123;&#125;rule_files: &#123;&#125;remote_write: - url: \"http://timescaledb:9201/write\"remote_read: - url: \"http://timescaledb:9201/read\"scrape_configs:- job_name: prometheus honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9090- job_name: node-exporter honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9100创建systemd服务/usr/lib/systemd/system/prometheus.service12345678910111213141516171819[Unit]Description=prometheusAfter=network.target[Service]Type=simpleUser=prometheusExecStart=/usr/local/bin/prometheus \\ --config.file=/etc/prometheus/prometheus.yml \\ --storage.tsdb.path=/var/lib/prometheus \\ --storage.tsdb.retention.time=15d \\ --storage.tsdb.retention.size=40GB \\ --web.console.templates=/etc/prometheus/consoles \\ --web.console.libraries=/etc/prometheus/console_librariesExecReload=/bin/kill -HUP $MAINPIDRestart=on-failureRestartSec=60s[Install]WantedBy=multi-user.target启动prometheus123systemctl daemon-reloadsystemctl enable prometheus.servicesystemctl start prometheus.service部署node_exporter下载软件包1wget -O - https://github.com/prometheus/node_exporter/releases/download/v0.18.0/node_exporter-0.18.0.linux-amd64.tar.gz | tar xz拷贝到PATH目录1234cd node_exporter-0.18.0.linux-amd64chown root:root node_exporterchmod 755 node_exportermv node_exporter /usr/local/bin/创建systemd服务/usr/lib/systemd/system/node_exporter.service1234567891011[Unit]Description=node_exporterAfter=network.target[Service]Type=simpleUser=nobodyExecStart=/usr/local/bin/node_exporterRestart=on-failureRestartSec=60s[Install]WantedBy=multi-user.target启动node_exporter123systemctl daemon-reloadsystemctl enable node_exporter.servicesystemctl start node_exporter.service测试验证查看Adapter日志1journalctl -u prometheus-postgresql-adapter.service -f输出示例1234Month Day HH:mm:ss timescaledb prometheus-postgresql-adapter[6953]: node_xfs_block_map_btree_records_inserted_total&#123;device=\"sda1\",instance=\"localhost:9100\",job=\"node-exporter\"&#125; 0 1563284554124Month Day HH:mm:ss timescaledb prometheus-postgresql-adapter[6953]: node_xfs_block_map_btree_records_inserted_total&#123;device=\"sda2\",instance=\"localhost:9100\",job=\"node-exporter\"&#125; 0 1563284554124Month Day HH:mm:ss timescaledb prometheus-postgresql-adapter[6953]: node_xfs_block_mapping_extent_list_compares_total&#123;device=\"sda1\",instance=\"localhost:9100\",job=\"node-exporter\"&#125; 0 1563284554124Month Day HH:mm:ss timescaledb prometheus-postgresql-adapter[6953]: node_xfs_block_mapping_extent_list_compares_total&#123;device=\"sda2\",instance=\"localhost:9100\",job=\"node-exporter\"&#125; 0 1563284554124查看网卡ens33总流量PromQL1node_network_transmit_bytes_total&#123;device=&quot;ens33&quot;&#125;SQL12345prometheus=&gt; SELECT time, value AS \"total transmitted bytes\" FROM metrics WHERE labels-&gt;&gt;'device' = 'ens33' AND name='node_network_transmit_bytes_total' ORDER BY time;输出示例12345678 time | total transmitted bytes ----------------------------+------------------------- YYYY-MM-DD 21:42:34.124+08 | 9058222 YYYY-MM-DD 21:42:39.123+08 | 9095088 YYYY-MM-DD 21:42:44.124+08 | 9128038 YYYY-MM-DD 21:42:49.124+08 | 9161183 YYYY-MM-DD 21:42:54.123+08 | 9198099 YYYY-MM-DD 21:42:59.124+08 | 9231065查系统最近24小时的5min平均负载PromQL只能返回一个值1node_load5SQL123456prometheus=&gt; SELECT time_bucket('5 minutes', time) AS five_min_bucket, name, avg(value) FROM metrics WHERE (name='node_load5' OR name='node_memory_Active_bytes') AND time &gt; NOW() - interval '1 day' GROUP BY five_min_bucket,name ORDER BY five_min_bucket;输出示例1234567891011 five_min_bucket | name | avg ------------------------+--------------------------+-------------------- YYYY-MM-DD 21:40:00+08 | node_load5 | 0.056 YYYY-MM-DD 21:40:00+08 | node_memory_Active_bytes | 177269828.266667 YYYY-MM-DD 21:45:00+08 | node_load5 | 0.0435593220338983 YYYY-MM-DD 21:45:00+08 | node_memory_Active_bytes | 177018845.288136 YYYY-MM-DD 21:50:00+08 | node_load5 | 0.0326666666666667 YYYY-MM-DD 21:50:00+08 | node_memory_Active_bytes | 182458094.933333 YYYY-MM-DD 21:55:00+08 | node_load5 | 0.024 YYYY-MM-DD 21:55:00+08 | node_memory_Active_bytes | 185453363.2(8 rows)","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://luanlengli.github.io/categories/Prometheus/"},{"name":"PostgreSQL","slug":"Prometheus/PostgreSQL","permalink":"https://luanlengli.github.io/categories/Prometheus/PostgreSQL/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://luanlengli.github.io/tags/Prometheus/"},{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://luanlengli.github.io/tags/PostgreSQL/"}]},{"title":"二进制部署Kubernetes-v1.14.x高可用集群","slug":"二进制部署Kubernetes-v1-14-x高可用集群","date":"2019-06-23T03:55:12.000Z","updated":"2019-12-17T02:25:37.830Z","comments":true,"path":"2019/06/23/二进制部署Kubernetes-v1-14-x高可用集群.html","link":"","permalink":"https://luanlengli.github.io/2019/06/23/二进制部署Kubernetes-v1-14-x高可用集群.html","excerpt":"","text":"介绍只记录部署过程，不保证ctrl+c和ctrl+v能直接跑！请自行判断参数是否能直接套用！Kubernetes-v1.15已经在2019-06-19发布，而Kubernetes-v1.14作为上一个release版本已经更新到了1.14.3。之前写的Kubernetes-v1.11的高可用集群部署文档由于版本变迁部分参数需要改动，部署过程有些地方欠缺考虑，这里以Kubernetes-v1.14.3版本重新做一次文档整理因此文章整体架构基本与【二进制部署 kubernetes v1.11.x 高可用集群】相同配置部分参考kubeadm的参数部署说明本次部署方式为二进制可执行文件的方式部署注意请根据自己的实际情况调整对于生产环境部署，请注意某些参数的选择如无特殊说明，均在k8s-m1节点上执行参考博文感谢两位大佬的文章，这里整合一下两位大佬的内容，结合自己的理解整理本文漠然大佬的【Kubernetes 1.13.4 搭建】张馆长的【二进制部署Kubernetes v1.13.5 HA可选】软件版本这里参考kubeadm-1.14的镜像版本和yum源依赖Kubernetes版本v1.14.3Docker-CE版本18.09.06CNI-Plugins版本v0.75etcd版本v3.2.24网络信息基于CNI的模式实现容器网络Cluster IP CIDR: 10.244.0.0/16Service Cluster IP CIDR: 10.96.0.0/12Service DNS IP: 10.96.0.10Kubernetes API VIP: 172.16.80.200节点信息操作系统可采用 Ubuntu Server 16.04 LTS 、Ubuntu Server 18.04 LTS和 CentOS 7.6由keepalived提供VIP由haproxy提供kube-apiserver四层负载均衡通过污点的方式防止工作负载被调度到master节点服务器配置请根据实际情况适当调整IP地址主机名操作系统内核版本角色CPU内存172.16.80.201k8s-m1CentOS-7.6.18103.10.0-957master+node48G172.16.80.202k8s-m2CentOS-7.6.18103.10.0-957master+node48G172.16.80.203k8s-m3CentOS-7.6.18103.10.0-957master+node48G172.16.80.204k8s-n1CentOS-7.6.18103.10.0-957node48G172.16.80.205k8s-n2CentOS-7.6.18103.10.0-957node48G目录说明/usr/local/bin/：存放kubernetes和etcd二进制文件/opt/cni/bin/： 存放cni-plugin二进制文件/etc/etcd/：存放etcd配置文件和SSL证书/etc/kubernetes/：存放kubernetes配置和SSL证书/etc/cni/net.d/：安装CNI插件后会在这里生成配置文件$HOME/.kube/：kubectl命令会在家目录下建立此目录，用于保存访问kubernetes集群的配置和缓存$HOME/.helm/：helm命令会建立此目录，用于保存helm缓存和repository信息事前准备事情准备在所有服务器上都需要完成部署过程以root用户完成所有服务器网络互通，k8s-m1可以通过SSH证书免密登录到其他master节点，用于分发文件编辑hosts文件123456789cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1 localhost172.16.80.200 k8s-vip172.16.80.201 k8s-m1172.16.80.202 k8s-m2172.16.80.203 k8s-m3172.16.80.204 k8s-n1172.16.80.205 k8s-n2EOF时间同步服务集群系统需要各节点时间同步参考链接：RHEL7官方文档这里使用公网对时，如果需要内网对时，请自行配置123yum install -y chronysystemctl enable chronydsystemctl start chronyd关闭firewalld1234systemctl stop firewalldsystemctl disable firewalld# 清空iptables规则iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X关闭无用服务请根据自己的环境针对性地关闭服务123456789101112# 禁用蓝牙systemctl disable bluetooth.service# 如果不用NFS可以禁用rpcbindsystemctl disable rpcbind.servicesystemctl disable rpcbind.socket# 禁用cockpit网页端管理工具systemctl disable cockpit.socket# 禁用CUPS打印机服务systemctl disable cups-browsed.servicesystemctl disable cups.path systemctl disable cups.service systemctl disable cups.socket关闭SELINUX12setenforce 0sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config禁用swap12swapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab配置sysctl参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152cat &gt; /etc/sysctl.d/99-centos.conf &lt;&lt;EOF # 最大文件句柄数fs.file-max=5242880# 最大文件打开数fs.nr_open=5242880# 在CentOS7.4引入了一个新的参数来控制内核的行为。 # /proc/sys/fs/may_detach_mounts 默认设置为0# 当系统有容器运行的时候，需要将该值设置为1。fs.may_detach_mounts = 1# 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤net.bridge.bridge-nf-call-arptables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# 关闭严格校验数据包的反向路径net.ipv4.conf.default.rp_filter=0net.ipv4.conf.all.rp_filter=0# 修改动态NAT跟踪记录参数net.netfilter.nf_conntrack_max = 655350net.netfilter.nf_conntrack_tcp_timeout_established = 1200# 加快系统关闭处于 FIN_WAIT2 状态的 TCP 连接net.ipv4.tcp_fin_timeout = 30# 系统中处于 SYN_RECV 状态的 TCP 连接数量net.ipv4.tcp_max_syn_backlog = 8192# 内核中管理 TIME_WAIT 状态的数量net.ipv4.tcp_max_tw_buckets = 5120# 指定重发 SYN/ACK 的次数net.ipv4.tcp_synack_retries = 2# 端口最大的监听队列的长度net.core.somaxconn = 4096# 打开ipv4数据包转发net.ipv4.ip_forward = 1# 允许应用程序能够绑定到不属于本地网卡的地址net.ipv4.ip_nonlocal_bind=1 # 内存耗尽才使用swap分区vm.swappiness = 0vm.panic_on_oom = 0# 允许overcommitvm.overcommit_memory = 1# 定义了进程能拥有的最多内存区域vm.max_map_count = 65536# 设置系统TCP连接keepalive的持续时间，默认7200net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10# 禁用ipv6net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv6.conf.lo.disable_ipv6 = 1EOF# 让sysctl参数生效sysctl --system更新软件包1yum update -y安装软件包123yum groups install base -yyum install epel-release -yyum install jq nc git redhat-lsb vim ipvsadm tree dstat iotop htop socat ipset conntrack bash-completion-extras -y配置开机加载ipvs模块123456789101112131415cat &gt; /etc/modules-load.d/ipvs.conf &lt;&lt;EOFip_vsip_vs_lcip_vs_wlcip_vs_rrip_vs_wrrip_vs_lblcip_vs_lblcrip_vs_dhip_vs_ship_vs_foip_vs_nqip_vs_sedip_vs_ftpEOF安装docker-ce12345678910# 删除旧版本dockeryum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine -y# 安装docker-ce依赖yum install -y yum-utils device-mapper-persistent-data lvm2 -y# 添加docker-ce软件源yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# 修改docker-ce软件源为阿里云sed -e 's,download.docker.com,mirrors.aliyun.com/docker-ce,g' -i /etc/yum.repos.d/docker-ce.repo# 安装docker-ce 18.09yum install 1:docker-ce-cli-18.09.6-3.el7.x86_64 3:docker-ce-18.09.6-3.el7.x86_64 containerd.io -y配置docker-ce参考【container runtimes】CentOS/RHEL 7.4+123456789101112131415161718mkdir -p /etc/dockercat&gt;/etc/docker/daemon.json&lt;&lt;EOF&#123; \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\"], \"insecure-registries\": [], \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"data-root\": \"/var/lib/docker\", \"max-concurrent-downloads\": 10&#125;EOFUbuntu 16.04123456789101112131415mkdir -p /etc/dockercat&gt;/etc/docker/daemon.json&lt;&lt;EOF&#123; \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\"], \"insecure-registries\": [], \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"data-root\": \"/var/lib/docker\", \"max-concurrent-downloads\": 10&#125;EOF配置docker命令补全1cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/配置docker服务开机自启动12systemctl enable docker.servicesystemctl start docker.service查看docker信息1docker info禁用docker源为避免yum update时更新docker，将docker源禁用1yum-config-manager --disable docker-ce-stable确保以最新的内核启动系统1reboot定义集群变量注意这里的变量只对当前会话生效，如果会话断开或者重启服务器，都需要重新定义变量HostArray定义集群中所有节点的主机名和IPMasterArray定义master节点的主机名和IPNodeArray定义node节点的主机名和IP，这里master也运行kubelet所以也需要加入到NodeArrayVIP_IFACE定义keepalived的VIP绑定在哪一个网卡ETCD_SERVERS以MasterArray的信息生成etcd集群服务器列表ETCD_INITIAL_CLUSTER以MasterArray信息生成etcd集群初始化列表POD_DNS_SERVER_IP定义Pod的DNS服务器IP地址123456789101112131415161718192021222324252627282930313233343536373839404142434445464748declare -A HostArray MasterArray NodeArray# 声明所有节点的信息HostArray=( ['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203 ['k8s-n1']=172.16.80.204 ['k8s-n2']=172.16.80.205 )# 声明master节点信息MasterArray=( ['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203 )# 声明node节点信息NodeArray=( ['k8s-n1']=172.16.80.204 ['k8s-n2']=172.16.80.205 )# 配置Kubernetes APIServer的VIP地址VIP=\"172.16.80.200\"KUBE_APISERVER=\"https://172.16.80.200:8443\"# 定义集群名字CLUSTER_NAME=\"kubernetes\"# etcd版本号# kubeadm-v1.14.3里面使用的是v3.2.24ETCD_VERSION=\"v3.2.24\"# kubernetes版本号KUBERNETES_VERSION=\"v1.14.3\"# cni-plugin版本号# kubernetes YUM源里用的是v0.7.5CNI_PLUGIN_VERSION=\"v0.7.5\"# 声明VIP所在的网卡名称，以ens33为例VIP_IFACE=\"ens33\"# 声明etcd_serverETCD_SERVERS=$( xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#^#https://#;s#$#:2379#;$s#\\n##' | paste -d, -s - )ETCD_INITIAL_CLUSTER=$( for i in $&#123;!MasterArray[@]&#125;;do echo $i=https://$&#123;MasterArray[$i]&#125;:2380; done | sort | paste -d, -s - )# 定义POD_CLUSTER_CIDRPOD_NET_CIDR=\"10.244.0.0/16\"# 定义SVC_CLUSTER_CIDRSVC_CLUSTER_CIDR=\"10.96.0.0/12\"# 定义POD_DNS_SERVER_IPPOD_DNS_SERVER_IP=\"10.96.0.10\"下载所需软件包创建工作目录12mkdir -p /root/softwarecd /root/software下载解压软件包kubernetes12345678910111213141516171819echo \"--- 下载kubernetes $&#123;KUBERNETES_VERSION&#125; 二进制包 ---\"wget https://dl.k8s.io/$&#123;KUBERNETES_VERSION&#125;/kubernetes-server-linux-amd64.tar.gztar xzf kubernetes-server-linux-amd64.tar.gz \\ kubernetes/server/bin/kube-controller-manager \\ kubernetes/server/bin/hyperkube \\ kubernetes/server/bin/mounter \\ kubernetes/server/bin/kubelet \\ kubernetes/server/bin/kubectl \\ kubernetes/server/bin/kube-scheduler \\ kubernetes/server/bin/kube-proxy \\ kubernetes/server/bin/kube-apiserver \\ kubernetes/server/bin/cloud-controller-manager \\ kubernetes/server/bin/kubeadm \\ kubernetes/server/bin/apiextensions-apiserverchown -R root:root kubernetes/server/bin/*chmod 0755 kubernetes/server/bin/*# 这里需要先拷贝kubectl到/usr/local/bin目录下，用于生成kubeconfig文件cp kubernetes/server/bin/kubectl /usr/local/bin/kubectletcd12345678# 下载etcd二进制包echo \"--- 下载etcd $&#123;ETCD_VERSION&#125; 二进制包 ---\"wget https://github.com/etcd-io/etcd/releases/download/$&#123;ETCD_VERSION&#125;/etcd-$&#123;ETCD_VERSION&#125;-linux-amd64.tar.gztar xzf etcd-$&#123;ETCD_VERSION&#125;-linux-amd64.tar.gz \\ etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl \\ etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdchown root:root etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdchmod 0755 etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdCNI-Plugin12345# 下载CNI-pluginecho \"--- 下载cni-plugins $&#123;CNI_PLUGIN_VERSION&#125; 二进制包 ---\"wget https://github.com/containernetworking/plugins/releases/download/$&#123;CNI_PLUGIN_VERSION&#125;/cni-plugins-amd64-$&#123;CNI_PLUGIN_VERSION&#125;.tgzmkdir /root/software/cni-pluginstar xzf cni-plugins-amd64-$&#123;CNI_PLUGIN_VERSION&#125;.tgz -C /root/software/cni-plugins/生成集群Certificates说明本次部署，需要为etcd-server、etcd-client、kube-apiserver、kube-controller-manager、kube-scheduler、kube-proxy生成证书。另外还需要生成sa、front-proxy-ca、front-proxy-client证书用于集群的其他功能。要注意CA JSON文件的CN(Common Name)与O(Organization)等内容是会影响Kubernetes组件认证的。CN Common Name，kube-apiserver会从证书中提取该字段作为请求的用户名（User Name）O Oragnization，kube-apiserver会从证书中提取该字段作为请求用户的所属组（Group）CA是自签名根证书，用来给后续各种证书签名kubernetes集群的所有状态信息都保存在etcd中，kubernetes组件会通过kube-apiserver读写etcd里面的信息etcd如果暴露在公网且没做SSL/TLS验证，那么任何人都能读写数据，那么很可能会无端端在kubernetes集群里面多了挖坑Pod或者肉鸡Pod本文使用CFSSL创建证书，证书有效期10年建立证书过程在k8s-m1上完成用于生成证书的JSON文件已经打包好在这里pki.zip下载CFSSL工具123456wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/local/bin/cfssl-certinfowget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljsonchmod 755 /usr/local/bin/cfssl-certinfo \\ /usr/local/bin/cfssl \\ /usr/local/bin/cfssljson创建工作目录12mkdir -p /root/pki /root/master /root/nodecd /root/pki创建用于生成证书的json文件ca-config.json12345678910111213141516171819202122232425262728293031323334353637cat &gt; ca-config.json &lt;&lt;EOF&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125;, \"etcd-server\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125;, \"etcd-client\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"client auth\" ], \"expiry\": \"87600h\" &#125; &#125; &#125;&#125;EOFca-csr.json123456789101112131415161718cat &gt; ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"Kubernetes\", \"OU\": \"System\" &#125; ]&#125;EOFetcd-ca-csr.json123456789101112131415161718cat &gt; etcd-ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"etcd\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"etcd\", \"OU\": \"Etcd Security\" &#125; ]&#125;EOFetcd-server-csr.json123456789101112131415161718cat &gt; etcd-server-csr.json &lt;&lt;EOF&#123; \"CN\": \"etcd-server\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"etcd\", \"OU\": \"Etcd Security\" &#125; ]&#125;EOFetcd-client-csr.json123456789101112131415161718192021cat &gt; etcd-client-csr.json &lt;&lt;EOF&#123; \"CN\": \"etcd-client\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"hosts\": [ \"\" ], \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"etcd\", \"OU\": \"Etcd Security\" &#125; ]&#125;EOFkube-apiserver-csr.json123456789101112131415161718cat &gt; kube-apiserver-csr.json &lt;&lt;EOF&#123; \"CN\": \"kube-apiserver\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"Kubernetes\", \"OU\": \"System\" &#125; ]&#125;EOFkube-manager-csr.json123456789101112131415161718cat &gt; kube-manager-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-controller-manager\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"system:kube-controller-manager\", \"OU\": \"System\" &#125; ]&#125;EOFkube-scheduler-csr.json123456789101112131415161718cat &gt; kube-scheduler-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-scheduler\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"system:kube-scheduler\", \"OU\": \"System\" &#125; ]&#125;EOFkube-proxy-csr.json123456789101112131415161718cat &gt; kube-proxy-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-proxy\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"system:kube-proxy\", \"OU\": \"System\" &#125; ]&#125;EOFkube-admin-csr.json123456789101112131415161718cat &gt; kube-admin-csr.json &lt;&lt;EOF&#123; \"CN\": \"admin\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125;EOFfront-proxy-ca-csr.json123456789cat &gt; front-proxy-ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;&#125;EOFfront-proxy-client-csr.json123456789cat &gt; front-proxy-client-csr.json &lt;&lt;EOF&#123; \"CN\": \"front-proxy-client\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;&#125;EOFsa-csr.json123456789101112131415161718cat &gt; sa-csr.json &lt;&lt;EOF&#123; \"CN\": \"service-accounts\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"Kubernetes\", \"OU\": \"System\" &#125; ]&#125;EOF创建etcd证书etcd-ca证书12echo '--- 创建etcd-ca证书 ---'cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-caetcd-server证书1234567echo '--- 创建etcd-server证书 ---'cfssl gencert \\ -ca=etcd-ca.pem \\ -ca-key=etcd-ca-key.pem \\ -config=ca-config.json \\ -hostname=127.0.0.1,$(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | paste -d, -s -) \\ -profile=etcd-server etcd-server-csr.json | cfssljson -bare etcd-serveretcd-client证书123456echo '--- 创建etcd-client证书 ---'cfssl gencert \\ -ca=etcd-ca.pem \\ -ca-key=etcd-ca-key.pem \\ -config=ca-config.json \\ -profile=etcd-client etcd-client-csr.json | cfssljson -bare etcd-client创建kubernetes证书kubernetes-CA 证书123echo '--- 创建kubernetes-ca证书 ---'# 创建kubernetes-ca证书cfssl gencert -initca ca-csr.json | cfssljson -bare kube-cakube-apiserver证书12345678910echo '--- 创建kube-apiserver证书 ---'# 创建kube-apiserver证书# 这里的hostname字段中的10.96.0.1要跟上文提到的service cluster ip cidr对应cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -hostname=10.96.0.1,127.0.0.1,localhost,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,$&#123;VIP&#125;,$(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | paste -d, -s -) \\ -profile=kubernetes \\ kube-apiserver-csr.json | cfssljson -bare kube-apiserverkube-controller-manager证书12345678echo '--- 创建kube-controller-manager证书 ---'# 创建kube-controller-manager证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-manager-csr.json | cfssljson -bare kube-controller-managerkube-scheduler证书12345678echo '--- 创建kube-scheduler证书 ---'# 创建kube-scheduler证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-schedulerkube-proxy证书12345678echo '--- 创建kube-proxy证书 ---'# 创建kube-proxy证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxykube-admin证书12345678echo '--- 创建kube-admin证书 ---'# 创建kube-admin证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-admin-csr.json | cfssljson -bare kube-adminFront Proxy证书123456789echo '--- 创建Front Proxy Certificate证书 ---'# 创建Front Proxy Certificate证书cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-cacfssl gencert \\ -ca=front-proxy-ca.pem \\ -ca-key=front-proxy-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ front-proxy-client-csr.json | cfssljson -bare front-proxy-clientService Account证书12345678echo '--- 创建service account证书 ---'# 创建创建service account证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ sa-csr.json | cfssljson -bare sabootstrap-token12345# 生成 Bootstrap TokenBOOTSTRAP_TOKEN_ID=$(head -c 6 /dev/urandom | md5sum | head -c 6)BOOTSTRAP_TOKEN_SECRET=$(head -c 16 /dev/urandom | md5sum | head -c 16)BOOTSTRAP_TOKEN=\"$&#123;BOOTSTRAP_TOKEN_ID&#125;.$&#123;BOOTSTRAP_TOKEN_SECRET&#125;\"echo \"Bootstrap Token: $&#123;BOOTSTRAP_TOKEN&#125;\"encryption.yaml1234567891011121314151617ENCRYPTION_TOKEN=$(head -c 32 /dev/urandom | base64)echo \"ENCRYPTION_TOKEN: $&#123;ENCRYPTION_TOKEN&#125;\"# 创建encryption.yaml文件cat &gt; encryption.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: $&#123;ENCRYPTION_TOKEN&#125; - identity: &#123;&#125;EOFaudit-policy.yaml这里使用最低限度的审计策略文件123456789echo '--- 创建创建高级审计配置 ---'# 创建高级审计配置cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF# Log all requests at the Metadata level.apiVersion: audit.k8s.io/v1kind: Policyrules:- level: MetadataEOF创建kubeconfig文件说明kubeconfig 文件用于组织关于集群、用户、命名空间和认证机制的信息。命令行工具 kubectl 从 kubeconfig 文件中得到它要选择的集群以及跟集群 API server 交互的信息。默认情况下，kubectl 会从 $HOME/.kube 目录下查找文件名为 config 的文件。注意： 用于配置集群访问信息的文件叫作 kubeconfig文件，这是一种引用配置文件的通用方式，并不是说它的文件名就是 kubeconfig。Components kubeconfig1234567891011121314151617for TARGET in kube-controller-manager kube-scheduler kube-admin; do kubectl config set-cluster kubernetes \\ --certificate-authority=kube-ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=$&#123;TARGET&#125;.kubeconfig kubectl config set-credentials system:$&#123;TARGET&#125; \\ --client-certificate=$&#123;TARGET&#125;.pem \\ --client-key=$&#123;TARGET&#125;-key.pem \\ --embed-certs=true \\ --kubeconfig=$&#123;TARGET&#125;.kubeconfig kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:$&#123;TARGET&#125; \\ --kubeconfig=$&#123;TARGET&#125;.kubeconfig kubectl config use-context default --kubeconfig=$&#123;TARGET&#125;.kubeconfigdoneBootstrap kubeconfig1234567891011121314151617181920# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=kube-ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context kubelet-bootstrap \\ --cluster=$&#123;CLUSTER_NAME&#125; \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置当前使用的上下文kubectl config use-context kubelet-bootstrap --kubeconfig=bootstrap.kubeconfig清理证书CSR文件12echo '--- 删除*.csr文件 ---'rm -rf *csr修改文件权限123chown root:root *pem *kubeconfig *yamlchmod 0444 *pem *kubeconfig *yamlchmod 0400 *key.pemEtcd Cluster说明Kubernetes集群数据需要存放在etcd中etcd集群部署在master节点上部署三节点的etcd clusteretcd集群启用基于TLS的客户端身份验证+集群节点身份认证准备环境查看集群部署的环境变量这里主要检查上面集群变量定义的时候，是否有遗漏或者不正确12echo \"ETCD_SERVERS=$&#123;ETCD_SERVERS&#125;\"echo \"ETCD_INITIAL_CLUSTER=$&#123;ETCD_INITIAL_CLUSTER&#125;\"添加用户etcd以普通用户运行12345echo '--- master节点添加etcd用户 ---'for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE ---\" ssh $&#123;NODE&#125; /usr/sbin/useradd -r -s /bin/false etcddone创建目录12345678910111213echo '--- master节点创建目录 ---'for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE ---\" echo \"--- 创建目录 ---\" ssh $&#123;NODE&#125; /usr/bin/mkdir -p /etc/etcd/ssl \\ /var/lib/etcd echo \"--- 修改目录权限 ---\" ssh $&#123;NODE&#125; /usr/bin/chmod 0755 /etc/etcd \\ /etc/etcd/ssl \\ /var/lib/etcd echo \"--- 修改目录属组 ---\" ssh $&#123;NODE&#125; chown -R etcd:etcd /etc/etcd/ /var/lib/etcddone创建工作目录1mkdir -p /root/master切换工作目录1cd /root/master部署etcd创建systemd服务脚本123456789101112131415161718cat &gt; etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServiceDocumentation=https://coreos.com/etcd/docs/latest/After=network.target[Service]User=etcdType=notifyExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yamlRestart=on-failureRestartSec=60LimitNOFILE=65536[Install]WantedBy=multi-user.targetAlias=etcd3.serviceEOF创建etcd配置模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100cat &gt; etcd.config.yaml.example &lt;&lt;EOF# This is the configuration file for the etcd server.# Human-readable name for this member.name: '&#123;HOSTNAME&#125;'# Path to the data directory.data-dir: '/var/lib/etcd/&#123;HOSTNAME&#125;.data/'# Path to the dedicated wal directory.wal-dir: '/var/lib/etcd/&#123;HOSTNAME&#125;.wal/'# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 5000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: 'https://&#123;PUBLIC_IP&#125;:2380'# List of comma separated URLs to listen on for client traffic.listen-client-urls: 'https://&#123;PUBLIC_IP&#125;:2379,http://127.0.0.1:2379'# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 3# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: 'https://&#123;PUBLIC_IP&#125;:2380'# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: 'https://&#123;PUBLIC_IP&#125;:2379'# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: '$&#123;ETCD_INITIAL_CLUSTER&#125;'# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-k8s-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: '/etc/etcd/ssl/etcd-server.pem' # Path to the client server TLS key file. key-file: '/etc/etcd/ssl/etcd-server-key.pem' # Enable client cert authentication. client-cert-auth: true # Path to the client server TLS trusted CA cert file. trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem' # Client TLS using generated certificates auto-tls: truepeer-transport-security: # Path to the peer server TLS cert file. cert-file: '/etc/etcd/ssl/etcd-server.pem' # Path to the peer server TLS key file. key-file: '/etc/etcd/ssl/etcd-server-key.pem' # Enable peer client cert authentication. client-cert-auth: true # Path to the peer server TLS trusted CA cert file. trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem' # Peer TLS using generated certificates. auto-tls: true# Enable debug-level logging for etcd.debug: falselogger: 'zap'# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [default]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: 'periodic'auto-compaction-retention: 1# Set level of detail for exported metrics, specify 'extensive' to include histogram metrics.# default is 'basic'metrics: 'extensive'EOF分发etcd集群文件12345678910111213141516171819202122232425for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"---- $NODE ----\" echo \"--- 分发etcd二进制文件 ---\" rsync -avpt /root/software/etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl \\ /root/software/etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcd \\ $&#123;NODE&#125;:/usr/local/bin/ echo '---- 分发etcd证书 ----' rsync -avpt /root/pki/etcd-ca-key.pem \\ /root/pki/etcd-ca.pem \\ /root/pki/etcd-client-key.pem \\ /root/pki/etcd-client.pem \\ /root/pki/etcd-server-key.pem \\ /root/pki/etcd-server.pem \\ $&#123;NODE&#125;:/etc/etcd/ssl/ echo '---- 创建etcd服务脚本 ----' sed -e \"s/&#123;HOSTNAME&#125;/$&#123;NODE&#125;/g\" \\ -e \"s/&#123;PUBLIC_IP&#125;/$&#123;MasterArray[$NODE]&#125;/g\" \\ etcd.config.yaml.example &gt; etcd.config.yaml.$&#123;NODE&#125; echo '---- 分发etcd服务脚本 ----' rsync -avpt etcd.config.yaml.$&#123;NODE&#125; $&#123;NODE&#125;:/etc/etcd/etcd.config.yaml rsync -avpt etcd.service $&#123;NODE&#125;:/usr/lib/systemd/system/etcd.service ssh $&#123;NODE&#125; \"chown -R etcd:etcd /etc/etcd &amp;&amp; systemctl daemon-reload\" # 清理配置文件 rm -rf etcd.config.yaml.$&#123;NODE&#125;done启动etcd clusteretcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 systemctl start etcd 会卡住一段时间，为正常现象启动之后可以通过etcdctl命令查看集群状态12345for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" ssh $&#123;NODE&#125; systemctl enable etcd.service ssh $&#123;NODE&#125; systemctl start etcd.service &amp;done为方便维护，可使用alias简化etcdctl命令1234cat &gt;&gt; /root/.bashrc &lt;&lt;EOFalias etcdctl2=\"export ETCDCTL_API=2;etcdctl --ca-file '/etc/etcd/ssl/etcd-ca.pem' --cert-file '/etc/etcd/ssl/etcd-client.pem' --key-file '/etc/etcd/ssl/etcd-client-key.pem' --endpoints $&#123;ETCD_SERVERS&#125;\"alias etcdctl3=\"export ETCDCTL_API=3;etcdctl --cacert=/etc/etcd/ssl/etcd-ca.pem --cert=/etc/etcd/ssl/etcd-client.pem --key=/etc/etcd/ssl/etcd-client-key.pem --endpoints=$&#123;ETCD_SERVERS&#125;\"EOF验证etcd集群状态etcd提供v2和v3两套APIkubernetes使用的是etcd v3 API应用环境变量应用上面定义的alias1source /root/.bashrc使用v2 API访问集群获取集群状态1etcdctl2 cluster-health示例输出1234member 222fd3b0bb4a5931 is healthy: got healthy result from https://172.16.80.203:2379member 8349ef180b115a83 is healthy: got healthy result from https://172.16.80.201:2379member f525d2d797a7c465 is healthy: got healthy result from https://172.16.80.202:2379cluster is healthy获取成员列表1etcdctl2 member list示例输出123222fd3b0bb4a5931: name=k8s-m3 peerURLs=https://172.16.80.203:2380 clientURLs=https://172.16.80.203:2379 isLeader=false8349ef180b115a83: name=k8s-m1 peerURLs=https://172.16.80.201:2380 clientURLs=https://172.16.80.201:2379 isLeader=falsef525d2d797a7c465: name=k8s-m2 peerURLs=https://172.16.80.202:2380 clientURLs=https://172.16.80.202:2379 isLeader=true使用v3 API访问集群获取集群endpoint状态1etcdctl3 endpoint health示例输出123https://172.16.80.201:2379 is healthy: successfully committed proposal: took = 2.879402mshttps://172.16.80.203:2379 is healthy: successfully committed proposal: took = 6.708566mshttps://172.16.80.202:2379 is healthy: successfully committed proposal: took = 7.187607ms获取集群成员列表1etcdctl3 member list --write-out=table示例输出1234567+------------------+---------+--------+----------------------------+----------------------------+| ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS |+------------------+---------+--------+----------------------------+----------------------------+| 222fd3b0bb4a5931 | started | k8s-m3 | https://172.16.80.203:2380 | https://172.16.80.203:2379 || 8349ef180b115a83 | started | k8s-m1 | https://172.16.80.201:2380 | https://172.16.80.201:2379 || f525d2d797a7c465 | started | k8s-m2 | https://172.16.80.202:2380 | https://172.16.80.202:2379 |+------------------+---------+--------+----------------------------+----------------------------+配置定时备份脚本123456789101112131415#!/bin/shTIME=$(date +%Y%m%d)HOUR=$(date +%H)BASE_DIR=\"/path/to/etcd_backup\"BACKUP_DIR=\"$&#123;BASE_DIR&#125;/$&#123;TIME&#125;\"mkdir -p $BACKUP_DIRexport ETCDCTL_API=3/usr/local/bin/etcdctl \\ --cacert=/etc/etcd/ssl/etcd-ca.pem \\ --cert=/etc/etcd/ssl/etcd-client.pem \\ --key=/etc/etcd/ssl/etcd-client-key.pem \\ --endpoints=https://172.16.80.201:2379,https://172.16.80.201:2379,https://172.16.80.201:2379 \\ snapshot save $BACKUP_DIR/etcd-snapshot-$&#123;HOUR&#125;.db# 清理15天前的etcd备份find $&#123;BASE_DIR&#125; -type d -mtime +15 -exec rm -rf &#123;&#125; \\;Kubernetes MastersKeepalived和HAProxy说明HAProxy提供多个 API Server 的负载均衡(Load Balance)监听VIP的8443端口负载均衡到三台master节点的6443端口Keepalived提供虚拟IP位址(VIP),来让vip落在可用的master主机上供所有组件访问master节点提供健康检查脚本用于切换VIP切换工作目录1cd /root/master安装Keepalived和HAProxy12345for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"---- $NODE ----\" echo \"---- 安装haproxy和keepalived ----\" ssh $NODE yum install keepalived haproxy -ydoneKeepalived创建配置模板12345678910111213141516171819202122232425262728cat &gt; keepalived.conf.example &lt;&lt;EOFvrrp_script haproxy-check &#123; script \"/bin/bash /etc/keepalived/check_haproxy.sh\" interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance haproxy-vip &#123; state BACKUP priority 101 interface &#123;&#123; VIP_IFACE &#125;&#125; virtual_router_id 47 advert_int 3 unicast_peer &#123; &#125; virtual_ipaddress &#123; &#123;&#123; VIP &#125;&#125; &#125; track_script &#123; haproxy-check &#125;&#125;EOF生成配置文件通过集群变量替换配置模板的字符串，然后重定向到新的配置文件1234sed -r -e \"s#\\&#123;\\&#123; VIP \\&#125;\\&#125;#$&#123;VIP&#125;#\" \\ -e \"s#\\&#123;\\&#123; VIP_IFACE \\&#125;\\&#125;#$&#123;VIP_IFACE&#125;#\" \\ -e '/unicast_peer/r '&lt;(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#^#\\t#') \\ keepalived.conf.example &gt; keepalived.conf创建keepalived服务检查脚本12345678910111213cat &gt; check_haproxy.sh &lt;&lt;EOF#!/bin/bashVIRTUAL_IP=$&#123;VIP&#125;errorExit() &#123; echo \"*** $*\" 1&gt;&amp;2 exit 1&#125;if ip addr | grep -q \\$VIRTUAL_IP ; then curl -s --max-time 2 --insecure https://\\$&#123;VIRTUAL_IP&#125;:8443/ -o /dev/null || errorExit \"Error GET https://\\$&#123;VIRTUAL_IP&#125;:8443/\"fiEOFHAproxy创建配置模板1234567891011121314151617181920212223242526272829303132333435363738394041424344454647cat &gt; haproxy.cfg.example &lt;&lt;EOFglobal maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30sdefaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15sfrontend monitor-in bind $&#123;VIP&#125;:33305 mode http option httplog monitor-uri /monitorlisten stats bind $&#123;VIP&#125;:8006 mode http stats enable stats hide-version stats uri /stats stats refresh 30s stats realm Haproxy\\ Statistics stats auth admin:adminfrontend k8s-api bind $&#123;VIP&#125;:8443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-apibackend k8s-api mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100EOF生成配置文件通过集群变量替换配置模板的字符串，然后重定向到新的配置文件1sed -e '$r '&lt;(paste &lt;( seq -f' server k8s-api-%g' $&#123;#MasterArray[@]&#125; ) &lt;( xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#$#:6443 check#')) haproxy.cfg.example &gt; haproxy.cfg分发配置文件1234567for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"---- $NODE ----\" rsync -avpt haproxy.cfg $NODE:/etc/haproxy/ rsync -avpt keepalived.conf \\ check_haproxy.sh \\ $NODE:/etc/keepalived/done启动keepalived和HAProxy服务1234for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"---- $NODE ----\" ssh $NODE systemctl enable --now keepalived.service haproxy.servicedone验证服务状态需要大约十秒的时间等待keepalived和haproxy服务起来这里由于后端的kube-apiserver服务还没启动，只测试是否能ping通VIP如果VIP没起来，就要去确认一下各master节点的keepalived服务是否正常1sleep 15 &amp;&amp; ping -c 4 $VIPKubernetes Master服务切换工作目录1cd /root/master添加用户12345echo '--- master节点添加用户 ---'for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE ---\" ssh $&#123;NODE&#125; /usr/sbin/useradd -r -s /bin/false kubedone创建程序运行目录12345678910111213141516171819202122echo '--- master节点创建目录 ---'for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE ---\" echo \"--- 创建目录 ---\" ssh $&#123;NODE&#125; /usr/bin/mkdir -p /etc/kubernetes \\ /etc/kubernetes/pki \\ /etc/kubernetes/manifests \\ /var/run/kubernetes \\ /var/log/kube-audit echo \"--- 修改目录权限 ---\" ssh $&#123;NODE&#125; /usr/bin/chmod 0755 /etc/kubernetes \\ /etc/kubernetes/pki \\ /etc/kubernetes/manifests \\ /var/log/kube-audit \\ /var/run/kubernetes echo \"--- 修改目录属组 ---\" ssh $&#123;NODE&#125; chown -R kube:kube /etc/kubernetes \\ /etc/kubernetes/pki \\ /etc/kubernetes/manifests \\ /var/run/kubernetes \\ /var/log/kube-auditdonekube-apiserver说明以 REST APIs 提供 Kubernetes 资源的 CRUD,如授权、认证、存取控制与 API 注册等机制。关闭默认非安全端口8080,在安全端口 6443 接收 https 请求严格的认证和授权策略 (x509、token、RBAC)开启 bootstrap token 认证，支持 kubelet TLS bootstrapping使用 https 访问 kubelet、etcd，加密通信配置参数--allow-privileged=true启用容器特权模式--apiserver-count=3指定集群运行模式，其它节点处于阻塞状态--audit-policy-file=/etc/kubernetes/audit-policy.yaml 基于audit-policy.yaml文件定义的内容启动审计功能--authorization-mode=Node,RBAC开启 Node 和 RBAC 授权模式，拒绝未授权的请求--disable-admission-plugins=和--enable-admission-plugins禁用和启用准入控制插件。准入控制插件会在请求通过认证和授权之后、对象被持久化之前拦截到达apiserver的请求。准入控制插件依次执行，因此需要注意顺序。如果插件序列中任何一个拒绝了请求，则整个请求将立刻被拒绝并返回错误给客户端。关于admission-plugins官方文档里面有推荐配置，这里直接采用官方配置。注意针对不同kubernetes版本都会有不一样的配置，具体可以看这里。官方说明： For Kubernetes version 1.10 and later, the recommended admission controllers are enabled by default.--enable-bootstrap-token-auth=true启用 kubelet bootstrap 的 token 认证--experimental-encryption-provider-config=/etc/kubernetes/encryption.yaml启用加密特性将Secret数据加密存储到etcd--insecure-port=0关闭监听非安全端口8080--runtime-config=api/all=true启用所有版本的 APIs--service-cluster-ip-range=10.96.0.0/12指定 Service Cluster IP 地址段--service-node-port-range=30000-32767指定 NodePort 的端口范围--target-ram-mb配置缓存大小，参考值为节点数*60--event-ttl配置kubernets events的保留时间，默认1h0m0s创建配置模板12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758cat &gt; kube-apiserver.conf.example &lt;&lt;EOFKUBE_APISERVER_ARGS=\" \\\\--advertise-address=&#123;PUBLIC_IP&#125; \\\\--allow-privileged=true \\\\--apiserver-count=3 \\\\--audit-log-format=json \\\\--audit-log-maxage=30 \\\\--audit-log-maxbackup=3 \\\\--audit-log-maxsize=1000 \\\\--audit-log-mode=blocking \\\\--audit-log-path=/var/log/kube-audit/audit.log \\\\--audit-policy-file=/etc/kubernetes/audit-policy.yaml \\\\--authorization-mode=Node,RBAC \\\\--bind-address=0.0.0.0 \\\\--client-ca-file=/etc/kubernetes/pki/kube-ca.pem \\\\--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,PersistentVolumeClaimResize,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota \\\\--enable-aggregator-routing=true \\\\--enable-bootstrap-token-auth=true \\\\--enable-garbage-collector=true \\\\--etcd-compaction-interval=0s \\\\--etcd-cafile=/etc/kubernetes/pki/etcd-ca.pem \\\\--etcd-certfile=/etc/kubernetes/pki/etcd-client.pem \\\\--etcd-keyfile=/etc/kubernetes/pki/etcd-client-key.pem \\\\--etcd-prefix=/registry \\\\--etcd-servers=$&#123;ETCD_SERVERS&#125; \\\\--encryption-provider-config=/etc/kubernetes/encryption.yaml \\\\--endpoint-reconciler-type=lease \\\\--event-ttl=24h0m0s \\\\--feature-gates=PodShareProcessNamespace=true,ExpandPersistentVolumes=true \\\\--insecure-port=0 \\\\--kubelet-client-certificate=/etc/kubernetes/pki/kube-apiserver.pem \\\\--kubelet-client-key=/etc/kubernetes/pki/kube-apiserver-key.pem \\\\--kubelet-https=true \\\\--kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP \\\\--kubelet-timeout=3s \\\\--logtostderr=true \\\\--max-mutating-requests-inflight=500 \\\\--max-requests-inflight=1500 \\\\--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\\\--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\\\--requestheader-allowed-names=aggregator,front-proxy-client \\\\--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\\\--requestheader-extra-headers-prefix=X-Remote-Extra- \\\\--requestheader-group-headers=X-Remote-Group \\\\--requestheader-username-headers=X-Remote-User \\\\--runtime-config=api/all=true \\\\--secure-port=6443 \\\\--service-account-key-file=/etc/kubernetes/pki/sa.pem \\\\--service-cluster-ip-range=10.96.0.0/12 \\\\--service-node-port-range=30000-32767 \\\\--storage-backend=etcd3 \\\\--storage-media-type=application/vnd.kubernetes.protobuf \\\\--target-ram-mb=300 \\\\--tls-cert-file=/etc/kubernetes/pki/kube-apiserver.pem \\\\--tls-private-key-file=/etc/kubernetes/pki/kube-apiserver-key.pem \\\\--v=2 \\\\\"EOF创建systemd服务脚本123456789101112131415161718cat &gt; kube-apiserver.service &lt;&lt;EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target etcd.service[Service]User=kubeType=simpleEnvironmentFile=-/etc/kubernetes/kube-apiserver.confExecStart=/usr/local/bin/kube-apiserver \\$KUBE_APISERVER_ARGSRestart=on-failureRestartSec=60LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFkube-controller-manager说明通过核心控制循环(Core Control Loop)监听 Kubernetes API的资源来维护集群的状态，这些资源会被不同的控制器所管理，如 Replication Controller、NamespaceController 等等。而这些控制器会处理着自动扩展、滚动更新等等功能。关闭非安全端口，在安全端口 10252 接收 https 请求使用 kubeconfig 访问 kube-apiserver 的安全端口配置参数--allocate-node-cidrs=true在cloud provider上分配和设置pod的CIDR--cluster-cidr集群内的pod的CIDR范围，需要 --allocate-node-cidrs设为true--experimental-cluster-signing-duration=8670h0m0s指定 TLS Bootstrap 证书的有效期--feature-gates=RotateKubeletServerCertificate=true开启 kublet server 证书的自动更新特性--horizontal-pod-autoscaler-use-rest-clients=true能够使用自定义资源（Custom Metrics）进行自动水平扩展--leader-elect=true集群运行模式，启用选举功能，被选为 leader 的节点负责处理工作，其它节点为阻塞状态--node-cidr-mask-size=24集群中node cidr的掩码--service-cluster-ip-range=10.96.0.0/16指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致--terminated-pod-gc-thresholdexit状态的pod超过多少会触发gc创建配置文件123456789101112131415161718192021222324252627282930313233cat &gt; kube-controller-manager.conf &lt;&lt;EOFKUBE_CONTROLLER_MANAGER_ARGS=\" \\\\--address=0.0.0.0 \\\\--allocate-node-cidrs=true \\\\--authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\--authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\--cluster-cidr=$POD_NET_CIDR \\\\--cluster-signing-cert-file=/etc/kubernetes/pki/kube-ca.pem \\\\--cluster-signing-key-file=/etc/kubernetes/pki/kube-ca-key.pem \\\\--concurrent-service-syncs=10 \\\\--concurrent-serviceaccount-token-syncs=20 \\\\--controllers=*,bootstrapsigner,tokencleaner \\\\--enable-garbage-collector=true \\\\--experimental-cluster-signing-duration=8670h0m0s \\\\--feature-gates=PodShareProcessNamespace=true,ExpandPersistentVolumes=true \\\\--horizontal-pod-autoscaler-sync-period=10s \\\\--horizontal-pod-autoscaler-use-rest-clients=true \\\\--kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\--leader-elect=true \\\\--logtostderr=true \\\\--node-cidr-mask-size=24 \\\\--node-monitor-grace-period=40s \\\\--node-monitor-period=5s \\\\--pod-eviction-timeout=2m0s \\\\--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\\\--root-ca-file=/etc/kubernetes/pki/kube-ca.pem \\\\--service-account-private-key-file=/etc/kubernetes/pki/sa-key.pem \\\\--service-cluster-ip-range=$&#123;SVC_CLUSTER_CIDR&#125; \\\\--terminated-pod-gc-threshold=12500 \\\\--use-service-account-credentials=true \\\\--v=2 \\\\\"EOF创建systemd服务脚本123456789101112131415161718cat &gt; kube-controller-manager.service &lt;&lt;EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target kube-apiserver.service[Service]User=kubeType=simpleEnvironmentFile=-/etc/kubernetes/kube-controller-manager.confExecStart=/usr/local/bin/kube-controller-manager \\$KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureRestartSec=60LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFkube-scheduler说明负责将一个(或多个)容器依据调度策略分配到对应节点上让容器引擎(如 Docker)执行。调度受到 QoS 要求、软硬性约束、亲和性(Affinity)等等因素影响。配置参数--algorithm-provider=DefaultProvider使用默认调度算法--leader-elect=true集群运行模式，启用选举功能，被选为 leader 的节点负责处理工作，其它节点为阻塞状态创建配置文件12345678910cat &gt; kube-scheduler.conf &lt;&lt;EOFKUBE_SCHEDULER_ARGS=\"\\\\--address=0.0.0.0 \\\\--algorithm-provider=DefaultProvider \\\\--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\--leader-elect=true \\\\--logtostderr=true \\\\--v=2 \\\\\"EOF创建systemd服务脚本123456789101112131415161718cat &gt; kube-scheduler.service &lt;&lt;EOF[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target kube-apiserver.service[Service]User=kubeType=simpleEnvironmentFile=-/etc/kubernetes/kube-scheduler.confExecStart=/usr/local/bin/kube-scheduler \\$KUBE_SCHEDULER_ARGSRestart=on-failureRestartSec=60LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF分发配置文件12345678910111213141516171819202122232425262728293031323334for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" echo '---- 分发kubeconfig文件 yaml文件 ----' rsync -avpt /root/pki/kube-admin.kubeconfig \\ /root/pki/kube-controller-manager.kubeconfig \\ /root/pki/kube-scheduler.kubeconfig \\ /root/pki/audit-policy.yaml \\ /root/pki/encryption.yaml \\ $NODE:/etc/kubernetes/ echo '---- 分发sa证书 kube证书 front-proxy证书 ----' rsync -avpt /root/pki/etcd-ca.pem \\ /root/pki/etcd-client-key.pem \\ /root/pki/etcd-client.pem \\ /root/pki/front-proxy-ca.pem \\ /root/pki/front-proxy-client-key.pem \\ /root/pki/front-proxy-client.pem \\ /root/pki/kube-apiserver-key.pem \\ /root/pki/kube-apiserver.pem \\ /root/pki/kube-ca.pem \\ /root/pki/kube-ca-key.pem \\ /root/pki/sa-key.pem \\ /root/pki/sa.pem \\ $NODE:/etc/kubernetes/pki/ echo '---- 分发kubernetes components服务脚本----' rsync -avpt kube*service $NODE:/usr/lib/systemd/system/ echo '---- 分发kubernetes components配置----' sed -e \"s/&#123;PUBLIC_IP&#125;/$&#123;MasterArray[$NODE]&#125;/g\" kube-apiserver.conf.example &gt; kube-apiserver.conf.$&#123;NODE&#125; rsync -avpt kube-apiserver.conf.$&#123;NODE&#125; $NODE:/etc/kubernetes/kube-apiserver.conf rsync -avpt kube-controller-manager.conf $NODE:/etc/kubernetes/kube-controller-manager.conf rsync -avpt kube-scheduler.conf $NODE:/etc/kubernetes/kube-scheduler.conf rm -rf kube-apiserver.conf.$&#123;NODE&#125; ssh $NODE systemctl daemon-reload ssh $NODE chown -R kube:kube /etc/kubernetesdone分发master组件二进制文件12345678910111213141516echo '--- 分发kubernetes和etcd二进制文件 ---'for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE ---\" rsync -avpt /root/software/kubernetes/server/bin/kube-controller-manager \\ /root/software/kubernetes/server/bin/hyperkube \\ /root/software/kubernetes/server/bin/mounter \\ /root/software/kubernetes/server/bin/kubelet \\ /root/software/kubernetes/server/bin/kubectl \\ /root/software/kubernetes/server/bin/kube-scheduler \\ /root/software/kubernetes/server/bin/kube-proxy \\ /root/software/kubernetes/server/bin/kube-apiserver \\ /root/software/kubernetes/server/bin/cloud-controller-manager \\ /root/software/kubernetes/server/bin/kubeadm \\ /root/software/kubernetes/server/bin/apiextensions-apiserver \\ $NODE:/usr/local/bin/done启动Kubernetes master服务12345678for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" ssh $NODE \"systemctl enable --now kube-apiserver.service\" # sleep 10秒，让apiserver先初始化完成 sleep 10 ssh $NODE \"systemctl enable --now kube-controller-manager.service\" ssh $NODE \"systemctl enable --now kube-scheduler.service\"done验证Kubernetes集群服务检查集群components状态1kubectl --kubeconfig=/etc/kubernetes/kube-admin.kubeconfig get cs输出示例123456NAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy &#123;\"health\":\"true\"&#125; etcd-0 Healthy &#123;\"health\":\"true\"&#125; etcd-1 Healthy &#123;\"health\":\"true\"&#125;检查集群endpoints状态1kubectl --kubeconfig=/etc/kubernetes/kube-admin.kubeconfig get endpoints输出示例12NAME ENDPOINTS AGEkubernetes 172.16.80.201:6443,172.16.80.202:6443,172.16.80.203:6443 12m查看ETCD的数据Kubernetes集群数据会写入到etcd，这里检查一下是否能正常写入到etcd检查方式，查看etcd里面的key123456export ETCDCTL_API=3etcdctl --cacert=/etc/etcd/ssl/etcd-ca.pem \\ --cert=/etc/etcd/ssl/etcd-client.pem \\ --key=/etc/etcd/ssl/etcd-client-key.pem \\ --endpoints=$&#123;ETCD_SERVERS&#125; \\ get / --prefix --keys-only输出示例1234567891011121314151617181920212223/registry/apiregistration.k8s.io/apiservices/v1./registry/apiregistration.k8s.io/apiservices/v1.apps/registry/apiregistration.k8s.io/apiservices/v1.authentication.k8s.io/registry/apiregistration.k8s.io/apiservices/v1.authorization.k8s.io/registry/apiregistration.k8s.io/apiservices/v1.autoscaling/registry/apiregistration.k8s.io/apiservices/v1.batch/registry/apiregistration.k8s.io/apiservices/v1.coordination.k8s.io/registry/apiregistration.k8s.io/apiservices/v1.networking.k8s.io/registry/apiregistration.k8s.io/apiservices/v1.rbac.authorization.k8s.io/registry/apiregistration.k8s.io/apiservices/v1.scheduling.k8s.io/registry/apiregistration.k8s.io/apiservices/v1.storage.k8s.io.../registry/serviceaccounts/kube-system/service-account-controller/registry/serviceaccounts/kube-system/service-controller/registry/serviceaccounts/kube-system/statefulset-controller/registry/serviceaccounts/kube-system/token-cleaner/registry/serviceaccounts/kube-system/ttl-controller/registry/services/endpoints/default/kubernetes/registry/services/endpoints/kube-system/kube-controller-manager/registry/services/endpoints/kube-system/kube-dns/registry/services/endpoints/kube-system/kube-scheduler/registry/services/specs/default/kubernetes/registry/services/specs/kube-system/kube-dns配置kubectl集群配置文件kubectl默认会加载~/.kube/config文件，作为默认连接Kubernetes集群的凭证12345for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" ssh $NODE mkdir -p /root/.kube rsync -avpt /root/pki/kube-admin.kubeconfig $NODE:/root/.kube/configdonekubectl命令补齐临时生效1source &lt;(kubectl completion bash)开机自动加载12345for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" echo \"--- kubectl命令自动补全 ---\" ssh $NODE \"kubectl completion bash &gt; /etc/bash_completion.d/kubectl\"done配置TLS Bootstrap当集群开启了 TLS 认证后，每个节点的 kubelet 组件都要使用由 apiserver 使用的 CA 签发的有效证书才能与apiserver 通讯如果节点多起来，为每个节点单独签署证书将是一件非常繁琐的事情TLS bootstrapping 功能就是让 kubelet 先使用一个预定的低权限用户连接到 apiserver，然后向 apiserver 申请证书，kubelet 的证书由 apiserver 动态签署；创建工作目录12mkdir -p /root/yaml/cd /root/yaml/创建TLS Bootstrap Token对象123456789kubectl -n kube-system create secret generic bootstrap-token-$&#123;BOOTSTRAP_TOKEN_ID&#125; \\ --type 'bootstrap.kubernetes.io/token' \\ --from-literal description=\"cluster bootstrap token\" \\ --from-literal token-id=$&#123;BOOTSTRAP_TOKEN_ID&#125; \\ --from-literal token-secret=$&#123;BOOTSTRAP_TOKEN_SECRET&#125; \\ --from-literal usage-bootstrap-authentication=true \\ --from-literal usage-bootstrap-signing=true \\ --dry-run -o yaml &gt; /root/yaml/tls-bootstrap-token.yamlkubectl apply -f /root/yaml/tls-bootstrap-token.yaml创建YAML文件12345678910111213141516171819202122232425262728293031323334353637383940414243cat &gt; /root/yaml/kubelet-tls-bootstrap-rbac.yaml &lt;&lt;EOF# 允许 kubelet tls bootstrap 创建 csr 请求apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: create-csrs-for-bootstrappingroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-bootstrappersubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappers---# 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: auto-approve-csrs-for-groupsubjects:- kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclient apiGroup: rbac.authorization.k8s.io---# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: auto-approve-renewals-for-nodessubjects:- kind: Group name: system:nodes apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient apiGroup: rbac.authorization.k8s.ioEOF创建RBAC规则1kubectl apply -f /root/yaml/kubelet-tls-bootstrap-rbac.yamlkube-apiserver获取node信息的权限说明本文部署的kubelet关闭了匿名访问，因此需要额外为kube-apiserver添加权限用于访问kubelet的信息若没添加此RBAC，则kubectl在执行logs、exec等指令的时候会提示401 Forbidden12kubectl -n kube-system logs calico-node-pc8lq Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log calico-node-pc8lq)参考文档：Kubelet的认证授权创建YAML文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465cat &gt; /root/yaml/apiserver-to-kubelet-rbac.yaml &lt;&lt;EOF---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubeletrules:- apiGroups: - \"\" resources: - nodes verbs: - get - list - watch- apiGroups: - \"\" resources: - nodes verbs: - proxy- apiGroups: - \"\" resources: - nodes/log - nodes/metrics - nodes/proxy - nodes/spec - nodes/stats verbs: - '*'---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:kube-apiserver namespace: \"\"roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubeletsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kube-apiserver---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: systemctl:kubelet-api-admin namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubelet-api-adminsubjects:- apiGroup: rbac.authorization.k8s.io kind: User name: kube-apiserver---EOF创建RBAC规则1kubectl apply -f /root/yaml/apiserver-to-kubelet-rbac.yamlKubernetes Nodes说明节点配置安装Docker-ce，配置与master节点一致即可安装cni-plugins、kubelet、kube-proxy关闭防火墙和SELINUXkubelet运行需要root权限这里是以k8s-m1、k8s-m2、k8s-m3、k8s-n1、k8s-n2作为Work节点加入集群kubelet管理容器生命周期、节点状态监控目前 kubelet 支持三种数据源来获取节点Pod信息：本地文件通过 url 从网络上某个地址来获取信息API Server：从 kubernetes master 节点获取信息使用kubeconfig与kube-apiserver通信这里启用TLS-Bootstrap实现kubelet证书动态签署证书，并自动生成kubeconfigkube-proxyKube-proxy是实现Service的关键插件，kube-proxy会在每台节点上执行，然后监听API Server的Service与Endpoint资源物件的改变，然后来依据变化调用相应的组件来实现网路的转发kube-proxy可以使用userspace（基本已废弃）、iptables（默认方式）和ipvs来实现数据报文的转发这里使用的是性能更好、适合大规模使用的ipvs以DaemonSet方式运行kubelet配置说明kubelet在1.10版本开始动态配置特性进入Beta阶段，因此绝大部分配置被标记为DEPRECATED，官方推荐使用--config指定配置文件，并在配置文件里面指定原来命令行中配置的内容。因此kubelet的配置实际被分割成两个部分，启动参数和动态配置参数创建目录1mkdir -p /root/node切换工作目录1cd /root/node配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101cat &gt; kubelet.config &lt;&lt;EOFapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0authentication: anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: # 这里写kubernetes-ca证书的路径 clientCAFile: /etc/kubernetes/pki/kube-ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s# cgroups的驱动，可选systemd和cgroupfscgroupDriver: systemdcgroupsPerQOS: true# 指定Pod的DNS地址，这里的地址需要指向CoreDNS的SVC地址clusterDNS:- 10.96.0.10# 集群域名clusterDomain: cluster.localconfigMapAndSecretChangeDetectionStrategy: CachecontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuCFSQuotaPeriod: 100mscpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5# 达到某些阈值之后，kubelet会驱逐Pod# A set of eviction thresholds (e.g. memory.available&lt;1Gi) that if met would trigger a pod eviction.evictionHard: imagefs.available: 15% memory.available: 100Mi nodefs.available: 10% nodefs.inodesFree: 5%evictionPressureTransitionPeriod: 5m0s# 检测到系统已启用swap分区时kubelet会启动失败failSwapOn: false# 定义feature gatesfeatureGates: # 禁用Alpha的功能 AllAlpha: false # 检查kubelet配置文件变更的间隔fileCheckFrequency: 20s# 允许endpoint在尝试访问自己的服务时会被负载均衡分发到自身# 可选值\"promiscuous-bridge\", \"hairpin-veth\" and \"none\"# 默认值为promiscuous-bridgehairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20s# 这里定义容器镜像触发回收空间的上限值和下限值imageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: true# kubelet进程最大能打开的文件数量maxOpenFiles: 1048576# 当前节点kubelet所能运行的最大Pod数量maxPods: 110nodeLeaseDurationSeconds: 40# node状态上报间隔nodeStatusReportFrequency: 1m0snodeStatusUpdateFrequency: 10soomScoreAdj: -999# Pod PID数量限制，默认-1，即无限制podPidsLimit: -1# kubelet服务端口port: 10250registryBurst: 10registryPullQPS: 5# 指定域名解析文件resolvConf: /etc/resolv.confrotateCertificates: truerotateServerCertificates: trueruntimeRequestTimeout: 2m0s# 拉镜像时，同一时间只拉取一个镜像，即串行化# We recommend *not* changing the default value on nodes that run docker daemon with version &lt; 1.9 or an Aufs storage backend. Issue #10959 has more details. (default true)serializeImagePulls: false# 静态Pod的manifest目录staticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0sEOFsystemd服务脚本--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig指定bootstrap启动时使用的kubeconfig--network-plugin=cni定义网络插件，Pod生命周期使用此网络插件--node-labels=node-role.kubernetes.io/node=&#39;&#39;kubelet注册当前Node时设置的Label，以key=value的格式表示，多个labe以逗号分隔--pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.1Pod的pause镜像12345678910111213141516171819202122232425262728cat &gt; kubelet.service &lt;&lt;EOF[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/usr/local/bin/kubelet \\\\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\\\ --cert-dir=/etc/kubernetes/ssl \\\\ --config=/etc/kubernetes/kubelet.config \\\\ --cni-conf-dir=/etc/cni/net.d \\\\ --cni-bin-dir=/opt/cni/bin \\\\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\ --logtostderr=true \\\\ --network-plugin=cni \\\\ --pod-infra-container-image=gcrxio/pause:3.1 \\\\ --v=2KillMode=processRestart=on-failureRestartSec=60LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF创建程序目录12345678910for NODE in \"$&#123;!HostArray[@]&#125;\";do echo \"--- $NODE ---\" echo \"--- 创建目录 ---\" ssh $NODE mkdir -p /opt/cni/bin \\ /etc/cni/net.d \\ /etc/kubernetes/pki \\ /etc/kubernetes/ssl \\ /etc/kubernetes/manifests \\ /var/lib/kubeletdone分发文件1234567891011121314151617181920for NODE in \"$&#123;!HostArray[@]&#125;\";do echo \"--- $&#123;NODE&#125; ---\" echo \"--- 分发kubernetes二进制文件 ---\" rsync -avpt /root/software/kubernetes/server/bin/kubelet \\ $&#123;NODE&#125;:/usr/local/bin/ echo \"--- 分发CNI-Plugins ---\" rsync -avpt /root/software/cni-plugins/* \\ $&#123;NODE&#125;:/opt/cni/bin/ echo \"--- 分发证书和kubeconfig文件 ---\" rsync -avpt /root/pki/bootstrap.kubeconfig \\ $&#123;NODE&#125;:/etc/kubernetes/ rsync -avpt /root/pki/kube-ca.pem \\ /root/pki/front-proxy-ca.pem \\ $&#123;NODE&#125;:/etc/kubernetes/pki/ echo \"--- 分发配置文件 ---\" rsync -avpt kubelet.config $&#123;NODE&#125;:/etc/kubernetes/ echo \"--- 分发systemd服务脚本 ---\" rsync -avpt kubelet.service $&#123;NODE&#125;:/usr/lib/systemd/system/ ssh $&#123;NODE&#125; systemctl daemon-reloaddone启动kubelet服务12345for NODE in \"$&#123;!HostArray[@]&#125;\";do echo \"--- $NODE ---\" ssh $NODE systemctl enable docker.service kubelet.service ssh $NODE systemctl start docker.service kubelet.servicedone获取集群节点信息此时由于未按照网络插件，所以节点状态为NotReady1kubectl get node -o wide输出示例123456NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-m1 NotReady node 12s v1.14.3 172.16.80.201 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://18.9.6k8s-m2 NotReady node 12s v1.14.3 172.16.80.202 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://18.9.6k8s-m3 NotReady node 12s v1.14.3 172.16.80.203 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://18.9.6k8s-n1 NotReady node 12s v1.14.3 172.16.80.204 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://18.9.6k8s-n2 NotReady node 12s v1.14.3 172.16.80.205 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.3.el7.x86_64 docker://18.9.6节点标签master节点声明污点，避免没有声明容忍该污点的Pod被调度到master节点12kubectl label node $&#123;!MasterArray[@]&#125; node-role.kubernetes.io/master=\"\"kubectl taint nodes $&#123;!MasterArray[@]&#125; node-role.kubernetes.io/master=\"\":NoSchedulenode节点1kubectl label nodes $&#123;!NodeArray[@]&#125; node-role.kubernetes.io/node=\"\"Kubernetes Core AddonsKube-Proxy说明Kube-proxy是实现Service的关键插件,kube-proxy会在每台节点上执行,然后监听APIServer的Service与Endpoint资源物件的改变,然后来依据变化执行iptables来实现网路的转发。本文使用DaemonSet来运行Kube-Proxy组件。创建工作目录1mkdir -p /root/yaml/CoreAddons/kube-proxy切换工作目录1cd /root/yaml/CoreAddons/kube-proxy创建YAML文件ServicAccount1234567cat &gt; /root/yaml/CoreAddons/kube-proxy/kube-proxy-sa.yaml &lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: kube-proxy namespace: kube-systemEOFConfigMap123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081cat &gt; /root/yaml/CoreAddons/kube-proxy/kube-proxy-cm.yaml &lt;&lt;EOFapiVersion: v1kind: ConfigMapmetadata: labels: app: kube-proxy name: kube-proxy namespace: kube-systemdata: config.conf: |- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: \"\" burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /var/lib/kube-proxy/kubeconfig.conf qps: 5 # 集群中pod的CIDR范围，从这个范围以外发送到服务集群IP的流量将被伪装 # 从POD发送到外部LoadBalanceIP的流量将被定向到各自的集群IP clusterCIDR: 10.244.0.0/16 # 来自apiserver配置的刷新频率 configSyncPeriod: 15m0s conntrack: max: null # 每个核心最大能跟踪的NAT连接数，默认32768 maxPerCore: 32768 min: 131072 # 处于CLOSE_WAIT状态的TCP连接超时时间 tcpCloseWaitTimeout: 1h0m0s # 已建立的 TCP 连接的空闲超时 tcpEstablishedTimeout: 24h0m0s # 通过Web接口/debug/pprof启用性能分析 enableProfiling: false featureGates: SupportIPVSProxyMode: true healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \"\" iptables: # SNAT所有通过服务集群ip发送的通信 masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s # ipvs 规则刷新的最大时间间隔 syncPeriod: 30s ipvs: excludeCIDRs: null minSyncPeriod: 0s # ipvs调度类型，默认是rr scheduler: rr strictARP: false syncPeriod: 30s metricsBindAddress: 127.0.0.1:10249 # 配置kube-proxy代理模式，可选iptables和ipvs，默认是iptables mode: ipvs nodePortAddresses: null oomScoreAdj: -999 portRange: \"\" resourceContainer: /kube-proxy udpIdleTimeout: 250ms kubeconfig.conf: |- apiVersion: v1 kind: Config clusters: - cluster: certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt server: $&#123;KUBE_APISERVER&#125; name: default contexts: - context: cluster: default namespace: default user: default name: default current-context: default users: - name: default user: tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/tokenEOFDaemonSet12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970cat &gt; /root/yaml/CoreAddons/kube-proxy/kube-proxy-ds.yaml &lt;&lt;EOFapiVersion: apps/v1kind: DaemonSetmetadata: labels: k8s-app: kube-proxy name: kube-proxy namespace: kube-systemspec: selector: matchLabels: k8s-app: kube-proxy template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" labels: k8s-app: kube-proxy spec: containers: - command: - /usr/local/bin/kube-proxy - --config=/var/lib/kube-proxy/config.conf - --hostname-override=\\$(NODE_NAME) env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName image: gcrxio/kube-proxy:v1.14.3 imagePullPolicy: IfNotPresent name: kube-proxy resources: &#123;&#125; securityContext: privileged: true volumeMounts: - mountPath: /var/lib/kube-proxy name: kube-proxy - mountPath: /run/xtables.lock name: xtables-lock - mountPath: /lib/modules name: lib-modules readOnly: true - mountPath: /etc/localtime name: timezone-volume readOnly: true hostNetwork: true priorityClassName: system-node-critical serviceAccountName: kube-proxy tolerations: - key: CriticalAddonsOnly operator: Exists - operator: Exists volumes: - configMap: name: kube-proxy name: kube-proxy - hostPath: path: /run/xtables.lock type: FileOrCreate name: xtables-lock - hostPath: path: /lib/modules name: lib-modules - hostPath: path: /usr/share/zoneinfo/Asia/Shanghai name: timezone-volume updateStrategy: type: RollingUpdateEOFClusterRoleBinding1234567891011121314cat &gt; /root/yaml/CoreAddons/kube-proxy/kube-proxy-rbac.yaml &lt;&lt;EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: kubeadm:node-proxierroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-proxiersubjects:- kind: ServiceAccount name: kube-proxy namespace: kube-systemEOF部署Kube-Proxy1kubectl apply -f /root/yaml/CoreAddons/kube-proxy/验证Kube-Proxy查看Pod1kubectl -n kube-system get pod -l k8s-app=kube-proxy查看IPVS规则1ipvsadm -ln查看Kube-Proxy代理模式1curl localhost:10249/proxyMode网络插件说明只要符合CNI规范的网络组件都可以给kubernetes使用网络组件清单可以在这里看到Network Plugins这里只列举kube-flannel和calico，flannel和calico的区别可以自己去找资料网络组件只能选一个来部署本文使用kube-flannel部署网络组件，calico已测试可用在k8s-m1上操作创建工作目录1mkdir -p /root/yaml/CoreAddons/network-plugin/&#123;kube-flannel,calico&#125;kube-flannel说明kube-flannel基于VXLAN的方式创建容器二层网络，使用端口8472/UDP通信flannel 第一次启动时，从 etcd 获取 Pod 网段信息，为本节点分配一个未使用的 /24 段地址，然后创建 flannel.1（也可能是其它名称，如 flannel1 等） 接口。官方提供yaml文件部署为DeamonSet若需要使用NetworkPolicy功能，可以关注这个项目canal架构图切换工作目录1cd /root/yaml/CoreAddons/network-plugin/kube-flannel下载yaml文件1wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml官方yaml文件包含多个平台的daemonset，包括amd64、arm64、arm、ppc64le、s390x这里以amd64作为例子，其他的可以自行根据需要修改或者直接删除不需要的daemonset官方yaml文件已经配置好容器网络为10.244.0.0/16，这里需要跟kube-controller-manager.conf里面的--cluster-cidr匹配如果在kube-controller-manager.conf里面把--cluster-cidr改成了其他地址段，例如192.168.0.0/16，用以下命令替换kube-flannel.yaml相应的字段1sed -e 's,\"Network\": \"10.244.0.0/16\",\"Network\": \"192.168.0.0/16,\" -i kube-flannel.yml如果服务器有多个网卡，需要指定网卡用于flannel通信，以网卡ens33为例在args下面添加一行- --iface=ens3312345678containers:- name: kube-flannel command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=ens33修改backendflannel支持多种后端实现，可选值为VXLAN、host-gw、UDP从性能上，host-gw是最好的，VXLAN和UDP次之默认值是VXLAN，这里以修改为host-gw为例，位置大概在75行左右1234567net-conf.json: | &#123; \"Network\": \"10.244.0.0/16\", \"Backend\": &#123; \"Type\": \"host-gw\" &#125; &#125;部署kube-flannel1kubectl apply -f kube-flannel.yml检查部署情况1kubectl -n kube-system get pod -l app=flannel输出示例123456NAME READY STATUS RESTARTS AGEkube-flannel-ds-amd64-4v8q7 1/1 Running 0 3m46skube-flannel-ds-amd64-6lq5q 1/1 Running 0 3m47skube-flannel-ds-amd64-cczp4 1/1 Running 0 3m45skube-flannel-ds-amd64-wjpkg 1/1 Running 0 3m44skube-flannel-ds-amd64-z2lm8 1/1 Running 0 3m46s如果等很久都没Running，可能是你所在的网络环境访问quay.io速度太慢了可以替换一下镜像，重新apply12sed -e 's,quay.io/coreos/,zhangguanzhang/quay.io.coreos.,g' -i kube-flannel.ymlkubectl apply -f kube-flannel.yamlCalico说明Calico 是一款纯 Layer 3 的网络，节点之间基于BGP协议来通信。这里以calico-v3.7.0来作为示例部署文档架构图切换工作目录1cd /root/yaml/CoreAddons/network-plugin/calico下载yaml文件这里选用的是【Installing with the Kubernetes API datastore—50 nodes or less】1wget https://docs.projectcalico.org/v3.7/manifests/calico.yaml修改YAML文件calico-node服务的主要参数如下CALICO_IPV4POOL_CIDR配置Calico IPAM的IP地址池，默认是192.168.0.0/1612345# The default IPv4 pool to create on startup if none exists. Pod IPs will be# chosen from this range. Changing this value after installation will have# no effect. This should fall within `--cluster-cidr`.- name: CALICO_IPV4POOL_CIDR value: \"192.168.0.0/16\"CALICO_IPV4POOL_IPIP 配置是否使用IPIP模式，默认是打开的123# Enable IPIP- name: CALICO_IPV4POOL_IPIP value: \"Always\"部署Calico1kubectl apply -f /root/yaml/CoreAddons/network-plugin/calico/检查部署情况检查calico-node1kubectl -n kube-system get pod -l k8s-app=calico-node输出示例123456NAME READY STATUS RESTARTS AGEcalico-node-fjcj4 2/2 Running 0 6mcalico-node-tzppt 2/2 Running 0 6mcalico-node-zdq64 2/2 Running 0 6mcalico-node-2hdqf 2/2 Running 0 6mcalico-node-jh6vd 2/2 Running 0 6m检查节点状态网络组件部署完成之后，可以看到node状态已经为Ready1234567kubectl get node NAME STATUS ROLES AGE VERSIONk8s-m1 Ready node 1d v1.14.3k8s-m2 Ready node 1d v1.14.3k8s-m3 Ready node 1d v1.14.3k8s-n1 Ready node 1d v1.14.3k8s-n2 Ready node 1d v1.14.3服务发现组件部署kubernetes从v1.11之后，已经使用CoreDNS取代原来的KUBE DNS作为服务发现的组件CoreDNS 是由 CNCF 维护的开源 DNS 方案，前身是 SkyDNS配置文件以kubeadm生成的YAML文件作为模板，再添加额外配置创建工作目录1mkdir -p /root/yaml/CoreAddons/coredns切换工作目录1cd /root/yaml/CoreAddons/corednsCoreDNS创建yaml文件ServiceAccount1234567cat &gt; /root/yaml/CoreAddons/coredns/coredns-sa.yaml &lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: coredns namespace: kube-systemEOFConfigMap123456789101112131415161718192021222324cat &gt; /root/yaml/CoreAddons/coredns/coredns-cm.yaml &lt;&lt;EOFapiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors log health kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload loadbalance &#125;EOFRBAC123456789101112131415161718192021222324252627282930313233343536cat &gt; /root/yaml/CoreAddons/coredns/coredns-rbac.yaml &lt;&lt;EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: system:corednsrules:- apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch- apiGroups: - \"\" resources: - nodes verbs: - get---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:corednsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:corednssubjects:- kind: ServiceAccount name: coredns namespace: kube-systemEOFDeployment123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899cat &gt; /root/yaml/CoreAddons/coredns/coredns-dp.yaml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: labels: k8s-app: kube-dns name: coredns namespace: kube-systemspec: replicas: 2 selector: matchLabels: k8s-app: kube-dns strategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate template: metadata: labels: k8s-app: kube-dns spec: affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: - kube-dns topologyKey: kubernetes.io/hostname containers: - args: - -conf - /etc/coredns/Corefile image: gcrxio/coredns:1.2.6 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 successThreshold: 1 timeoutSeconds: 5 name: coredns ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true volumeMounts: - mountPath: /etc/coredns name: config-volume readOnly: true - mountPath: /etc/localtime name: timezone-volume readOnly: true dnsPolicy: Default serviceAccountName: coredns tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/master volumes: - configMap: items: - key: Corefile path: Corefile name: coredns name: config-volume - hostPath: path: /usr/share/zoneinfo/Asia/Shanghai name: timezone-volumeEOFService123456789101112131415161718192021222324252627282930cat &gt; /root/yaml/CoreAddons/coredns/coredns-svc.yaml &lt;&lt;EOFapiVersion: v1kind: Servicemetadata: annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: KubeDNS name: kube-dns namespace: kube-systemspec: clusterIP: 10.96.0.10 ports: - name: dns port: 53 protocol: UDP targetPort: 53 - name: dns-tcp port: 53 protocol: TCP targetPort: 53 - name: metrics port: 9153 protocol: TCP selector: k8s-app: kube-dnsEOF修改yaml文件yaml文件里面定义了clusterIP这里需要与kubelet.config.file里面定义的cluster-dns一致如果kubelet.conf里面的--cluster-dns改成别的，例如x.x.x.x，这里也要做相应变动，不然Pod找不到DNS，无法正常工作这里定义静态的hosts解析，这样Pod可以通过hostname来访问到各节点主机用下面的命令根据HostArray的信息生成静态的hosts解析12345678sed -e '16r '&lt;(\\ echo ' hosts &#123;'; \\ for NODE in \"$&#123;!HostArray[@]&#125;\";do \\ echo \" $&#123;HostArray[$NODE]&#125; $NODE\"; \\ done;\\ echo ' fallthrough'; \\ echo ' &#125;';) \\-i /root/yaml/CoreAddons/coredns/coredns-cm.yaml上面的命令的作用是，通过HostArray的信息生成hosts解析配置，顺序是打乱的，可以手工调整顺序也可以手动修改coredns.yaml文件来添加对应字段123456789101112131415161718192021222324252627282930apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors log health kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; hosts &#123; 172.16.80.201 k8s-m1 172.16.80.202 k8s-m2 172.16.80.203 k8s-m3 172.16.80.204 k8s-n1 172.16.80.205 k8s-n2 fallthrough &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload loadbalance &#125;部署CoreDNS1kubectl apply -f /root/yaml/CoreAddons/coredns/检查部署状态1kubectl -n kube-system get pod -l k8s-app=kube-dns输出样例123NAME READY STATUS RESTARTS AGEcoredns-5566c96697-6gzzc 1/1 Running 0 45scoredns-5566c96697-q5slk 1/1 Running 0 45s验证集群DNS服务创建一个deployment测试DNS解析1234567891011121314151617181920212223242526272829cat &gt; /root/yaml/busybox-deployment.yaml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: labels: app: busybox name: busybox namespace: defaultspec: replicas: 1 selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: busybox imagePullPolicy: IfNotPresent image: busybox:1.26 command: - sleep - \"36000\"EOF# 基于文件创建deploymentkubectl apply -f /root/yaml/busybox-deployment.yaml检查deployment部署情况123kubectl get podNAME READY STATUS RESTARTS AGEbusybox-7b9bfb5658-872gj 1/1 Running 0 6s验证集群DNS解析上一个命令获取到pod名字为busybox-7b9bfb5658-872gj通过kubectl命令连接到Pod运行nslookup命令测试使用域名来访问kube-apiserver和各节点主机123456789101112131415161718192021222324252627282930313233echo \"--- 通过CoreDNS访问kubernetes ---\"kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup kubernetes# 示例输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.localecho \"--- 通过CoreDNS访问k8s-m1 ---\"# 示例输出kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup k8s-m1Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: k8s-m1Address 1: 172.16.80.201 k8s-m1echo \"--- 通过CoreDNS访问k8s-m2 ---\"kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup k8s-m2# 示例输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: k8s-n2Address 1: 172.16.80.202 k8s-m2echo \"--- 通过CoreDNS访问并不存在的k8s-n3 ---\"kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup k8s-n3# 示例输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localnslookup: can't resolve 'k8s-n3'Metrics ServerMetrics Server是实现了 Metrics API 的元件,其目标是取代 Heapster 作位 Pod 与 Node 提供资源的 Usagemetrics,该元件会从每个 Kubernetes 节点上的 Kubelet 所公开的 Summary API 中收集 MetricsHorizontal Pod Autoscaler（HPA）控制器用于实现基于CPU使用率进行自动Pod伸缩的功能。HPA控制器基于Master的kube-controller-manager服务启动参数–horizontal-pod-autoscaler-sync-period定义是时长（默认30秒）,周期性监控目标Pod的CPU使用率,并在满足条件时对ReplicationController或Deployment中的Pod副本数进行调整,以符合用户定义的平均PodCPU使用率。在新版本的kubernetes中 Pod CPU使用率不在来源于heapster,而是来自于metrics-server官网原话是 The –horizontal-pod-autoscaler-use-rest-clients is true or unset. Setting this to false switches to Heapster-based autoscaling, which is deprecated.额外参数设置kube-apiserver参数，这里在配置kube-apiserver阶段已经加进去了front-proxy证书，在证书生成阶段已经完成且已分发1234567--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem--requestheader-allowed-names=aggregator--requestheader-group-headers=X-Remote-Group--requestheader-extra-headers-prefix=X-Remote-Extra---requestheader-username-headers=X-Remote-User创建工作目录1mkdir -p /root/yaml/CoreAddons/metrics-server切换工作目录1cd /root/yaml/CoreAddons/metrics-server下载yaml文件123456wget https://raw.githubusercontent.com/kubernetes/kubernetes/v1.14.3/cluster/addons/metrics-server/auth-delegator.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/v1.14.3/cluster/addons/metrics-server/auth-reader.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/v1.14.3/cluster/addons/metrics-server/metrics-apiservice.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/v1.14.3/cluster/addons/metrics-server/metrics-server-service.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/v1.14.3/cluster/addons/metrics-server/resource-reader.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/v1.14.3/cluster/addons/metrics-server/metrics-server-deployment.yaml修改metrics-server-deployment修改镜像地址修改metrics-server启动参数1234567891011containers:- name: metrics-server image: gcrxio/metrics-server-amd64:v0.3.1 command: - /metrics-server - --kubelet-preferred-address-types=Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP - --kubelet-insecure-tls - --requestheader-extra-headers-prefix=x-remote-extra- - --requestheader-group-headers=x-remote-group - --requestheader-username-headers=x-remote-user - -v=2修改resource-reader默认配置的权限无法获取node节点信息，会提示403 Forbidden需要在ClusterRole里面的rules[0].resources增加nodes/stats1234567891011121314151617181920212223242526272829303132333435363738394041424344apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: system:metrics-server labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilerules:- apiGroups: - \"\" resources: - pods - nodes - nodes/stats - namespaces verbs: - get - list - watch- apiGroups: - \"extensions\" resources: - deployments verbs: - get - list - update - watch---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:metrics-server labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: ReconcileroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-serversubjects:- kind: ServiceAccount name: metrics-server namespace: kube-system部署metrics-server1kubectl apply -f .查看pod状态123kubectl -n kube-system get pod -l k8s-app=metrics-serverNAME READY STATUS RESTARTS AGEpod/metrics-server-86bd9d7667-5hbn6 1/1 Running 0 1m验证metrics完成后,等待一段时间(约 30s - 1m)收集 Metrics请求metrics api的结果1kubectl get --raw /apis/metrics.k8s.io/v1beta1示例输出123456789101112131415161718192021222324252627&#123; \"kind\": \"APIResourceList\", \"apiVersion\": \"v1\", \"groupVersion\": \"metrics.k8s.io/v1beta1\", \"resources\": [ &#123; \"name\": \"nodes\", \"singularName\": \"\", \"namespaced\": false, \"kind\": \"NodeMetrics\", \"verbs\": [ \"get\", \"list\" ] &#125;, &#123; \"name\": \"pods\", \"singularName\": \"\", \"namespaced\": true, \"kind\": \"PodMetrics\", \"verbs\": [ \"get\", \"list\" ] &#125; ]&#125;获取节点性能数据1kubectl top node输出示例123456NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-m1 695m 17% 914Mi 11% k8s-m2 360m 9% 553Mi 7% k8s-m3 492m 12% 533Mi 6% k8s-n1 144m 3% 311Mi 3% k8s-n2 149m 3% 321Mi 4%获取Pod性能数据1kubectl -n kube-system top pod输出示例1234567891011121314NAME CPU(cores) MEMORY(bytes) coredns-56c964478c-g7q8n 14m 14Mi coredns-56c964478c-zmt2j 13m 14Mi kube-flannel-ds-amd64-8g6f9 6m 15Mi kube-flannel-ds-amd64-hslkh 6m 21Mi kube-flannel-ds-amd64-ppqkb 8m 14Mi kube-flannel-ds-amd64-swj8m 7m 14Mi kube-flannel-ds-amd64-zs2sd 8m 17Mi kube-proxy-2dq5x 18m 23Mi kube-proxy-9lzmj 28m 23Mi kube-proxy-9xtjc 18m 23Mi kube-proxy-w7mtg 2m 23Mi kube-proxy-zvp8d 2m 21Mi metrics-server-v0.3.1-98bdfb766-s6jf6 8m 22Mi#############################################################################Kubernetes集群已基本可用#############################################################################Kubernetes ExtraAddons说明下面的部署流程，更多的是补充Kubernetes的能力只记录简单的部署过程，不保证ctrl+c和ctrl+v能直接跑！Dashboard说明Dashboard 是kubernetes社区提供的GUI界面，用于图形化管理kubernetes集群，同时可以看到资源报表。官方提供yaml文件直接部署，但是需要更改image以便国内部署创建工作目录1mkdir -p /root/yaml/ExtraAddons/kubernetes-dashboard切换工作目录1cd /root/yaml/ExtraAddons/kubernetes-dashboard获取yaml文件1wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml修改镜像地址1sed -e 's,k8s.gcr.io/kubernetes-dashboard-amd64,gcrxio/kubernetes-dashboard-amd64,g' -i kubernetes-dashboard.yaml创建kubernetes-Dashboard1kubectl apply -f /root/yaml/ExtraAddons/kubernetes-dashboard/kubernetes-dashboard.yaml创建ServiceAccount RBAC官方的yaml文件，ServiceAccount绑定的RBAC权限很低，很多资源无法查看需要创建一个用于管理全局的ServiceAccount12345678910111213141516171819202122232425cat&lt;&lt;EOF | kubectl apply -f ----# 在kube-system中创建名为admin-user的ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---# 将admin-user和cluster-admin绑定在一起# cluster-admin是kubernetes内置的clusterrole，具有集群管理员权限# 其他内置的clusterrole可以通过kubectl get clusterrole查看apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-systemEOF获取ServiceAccount的Token1kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '&#123;print $1&#125;')查看部署情况1kubectl get all -n kube-system --selector k8s-app=kubernetes-dashboard访问Dashboardkubernetes-dashborad的svc默认是clusterIP，需要修改为nodePort才能被外部访问随机分配NodePort，分配范围由kube-apiserver的--service-node-port-range参数指定1kubectl patch -n kube-system svc kubernetes-dashboard -p '&#123;\"spec\":&#123;\"type\":\"NodePort\"&#125;&#125;'修改完之后，通过以下命令获取访问kubernetes-Dashboard的端口123kubectl -n kube-system get svc --selector k8s-app=kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.106.183.192 &lt;none&gt; 443:30216/TCP 12s可以看到已经将节点的30216端口暴露出来IP地址不固定，只要运行了kube-proxy组件，都会在节点上添加30216端口规则用于转发请求到Podhttps://172.16.80.200:30216https://172.16.80.201:30216https://172.16.80.202:30216https://172.16.80.203:30216https://172.16.80.204:30216https://172.16.80.205:30216登录Dashboard，上面已经获取了token，这里只需要把token的值填入输入框，点击SIGN IN即可登录Dashboard UI预览图Ingress Controller说明Ingress 是 Kubernetes 中的一个抽象资源，其功能是通过 Web Server 的 Virtual Host概念以域名(Domain Name)方式转发到內部 Service，这避免了使用 Service 中的 NodePort 与LoadBalancer 类型所带來的限制(如 Port 数量上限)，而实现 Ingress 功能则是通过 Ingress Controller来达成，它会负责监听 Kubernetes API 中的 Ingress 与 Service 资源，并在发生资源变化时，根据资源预期的结果来设置 Web Server。Ingress Controller 有许多实现可以选择，这里只是列举一小部分Ingress NGINX：Kubernetes 官方维护的方案，本次安装使用此方案kubernetes-ingress：由nginx社区维护的方案，使用社区版nginx和nginx-plustreafik：一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置创建工作目录1mkdir -p /root/yaml/ExtraAddons/ingress/ingress-nginx切换工作目录1cd /root/yaml/ExtraAddons/ingress/ingress-nginx下载yaml文件12wget https://github.com/kubernetes/ingress-nginx/raw/nginx-0.24.1/deploy/mandatory.yamlwget https://github.com/kubernetes/ingress-nginx/raw/nginx-0.24.1/deploy/provider/baremetal/service-nodeport.yaml修改镜像地址如果访问quay.io比较缓慢的话，可以修改一下镜像源12sed -e 's,quay.io/kubernetes-ingress-controller/,zhangguanzhang/quay.io.kubernetes-ingress-controller.,g' \\ -i mandatory.yaml创建ingress-nginx1kubectl apply -f .检查部署情况1kubectl -n ingress-nginx get pod访问ingress获取ingres的nodeport端口1kubectl -n ingress-nginx get svc输出示例12NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.96.250.140 &lt;none&gt; 80:32603/TCP,443:30083/TCP 1m默认的backend会返回40412curl http://172.16.80.200:32603curl -k https://172.16.80.200:30083输出示例1default backend - 404注意这里部署之后，是deployment，且通过nodePort暴露服务也可以修改yaml文件，将Ingress-nginx部署为DaemonSet使用labels和nodeSelector来指定运行ingress-nginx的节点使用hostNetwork=true来共享主机网络命名空间，或者使用hostPort指定主机端口映射如果使用hostNetwork共享宿主机网络栈或者hostPort映射宿主机端口，记得要看看有没有端口冲突，否则无法启动修改监听端口可以在ingress-nginx启动命令中添加--http-port=8180和--https-port=8543，还有下面的端口定义也相应变更即可创建kubernetes-Dashboard的Ingresskubernetes-Dashboard默认是开启了HTTPS访问的ingress-nginx需要以HTTPS的方式反向代理kubernetes-Dashboard以HTTP方式访问kubernetes-Dashboard的时候会被重定向到HTTPS需要创建HTTPS证书，用于访问ingress-nginx的HTTPS端口创建HTTPS证书这里的CN=域名/O=域名需要跟后面的ingress主机名匹配1234567openssl req -x509 \\ -nodes \\ -days 3650 \\ -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \"/CN=dashboard.k8s.local/O=dashboard.k8s.local\"创建secret对象这里将HTTPS证书创建为kubernetes的secret对象dashboard-tlsingress创建的时候需要加载这个作为HTTPS证书1kubectl -n kube-system create secret tls dashboard-tls --key ./tls.key --cert ./tls.crt创建dashboard-ingress.yaml123456789101112131415161718192021apiVersion: extensions/v1beta1kind: Ingressmetadata: name: dashboard-ingress namespace: kube-system annotations: nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" nginx.ingress.kubernetes.io/secure-backends: \"true\"spec: tls: - hosts: - dashboard.k8s.local secretName: dashboard-tls rules: - host: dashboard.k8s.local http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443创建ingress1kubectl apply -f dashboard-ingress.yaml检查ingress123kubectl -n kube-system get ingressNAME HOSTS ADDRESS PORTS AGEdashboard-ingress dashboard.k8s.local 80, 443 16m访问kubernetes-Dashboard修改主机hosts静态域名解析，以本文为例在hosts文件里添加172.16.80.200 dashboard.k8s.local使用https://dashboard.k8s.local:30083访问kubernetesDashboard了添加了TLS之后，访问HTTP会被跳转到HTTPS端口，这里比较坑爹，没法自定义跳转HTTPS的端口此处使用的是自签名证书，浏览器会提示不安全，请忽略建议搭配external-DNS和LoadBalancer一起食用，效果更佳效果图HelmHelm是一个kubernetes应用的包管理工具，用来管理charts——预先配置好的安装包资源，有点类似于Ubuntu的APT和CentOS中的yum。Helm chart是用来封装kubernetes原生应用程序的yaml文件，可以在你部署应用的时候自定义应用程序的一些metadata，便与应用程序的分发。Helm和charts的主要作用：应用程序封装版本管理依赖检查便于应用程序分发环境要求kubernetes v1.6及以上的版本，启用RBAC集群可以访问到chart仓库helm客户端主机能访问kubernetes集群安装Helm安装方式二选一，需要科学上网直接脚本安装1curl -L https://git.io/get_helm.sh | bash下载二进制文件安装123wget -O - https://get.helm.sh/helm-v2.14.1-linux-amd64.tar.gz | tar xz linux-amd64/helmmv linux-amd64/helm /usr/local/bin/helmrm -rf linux-amd64创建RBAC规则12345678910111213141516171819202122cat &lt;&lt; EOF | kubectl apply -f -# 创建名为tiller的ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---# 给tiller绑定cluster-admin权限apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tiller-cluster-ruleroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: tiller namespace: kube-systemEOF安装服务端helm官方charts仓库 https://kubernetes-charts.storage.googleapis.com/ 需要翻墙访问这里指定了helm的stable repo国内镜像地址，使用的是微软Azure的源具体说明请看这里123helm init --tiller-image gcrxio/tiller:v2.14.1 \\ --service-account tiller \\ --stable-repo-url http://mirror.azure.cn/kubernetes/charts/检查安装情况查看Pod状态1kubectl -n kube-system get pod -l app=helm,name=tiller输出示例12NAME READY STATUS RESTARTS AGEtiller-deploy-84fc6cd5f9-nz4m7 1/1 Running 0 1m查看helm版本1helm version输出示例12Client: &amp;version.Version&#123;SemVer:\"v2.14.1\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.14.1\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;本文至此结束说明在【二进制部署 kubernetes v1.11.x 高可用集群】里面对于ExtraAddons有额外的一些内容，例如Rook、Prometheus-Operator、ExternalDNS、EFK等等重新做整理适配实在是太麻烦了。后面再考虑另起文章专门来记录这些内容。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://luanlengli.github.io/tags/Docker/"}]},{"title":"kubeadm部署kubernetes-v1.13.x集群","slug":"kubeadm部署kubernetes-1.13.x集群","date":"2019-06-12T08:56:50.000Z","updated":"2019-08-05T03:03:31.000Z","comments":true,"path":"2019/06/12/kubeadm部署kubernetes-1.13.x集群.html","link":"","permalink":"https://luanlengli.github.io/2019/06/12/kubeadm部署kubernetes-1.13.x集群.html","excerpt":"","text":"说明根据kubernetes v1.13的Release Note说明从Kubernetes v1.13.x开始，kubeadm的kubeadm.k8s.io/v1alpha3被标记为废弃并将于kubernetes v1.14版开始被移除，新的apiVersion为kubeadm.k8s.io/v1beta1这里是关于kubeadm.k8s.io/v1beta1的说明文档下面演示环境使用1个master节点+2个node节点部署kubernetes集群仅记录我的部署流程，不一定满足各种需求，自己看菜吃饭！服务器配置主机名IP地址角色操作系统Docker版本kubeadm版本k8s-master172.16.80.200master+nodeCentOS-7.6.181018.09.6v1.13.7k8s-node1172.16.80.201nodeCentOS-7.6.181018.09.6v1.13.7k8s-node2172.16.80.202nodeCentOS-7.6.181018.09.6v1.13.7服务器初始化参考CentOS-7.6(1810)虚拟机模板制作配置/etc/hosts12345127.0.0.1 localhost::1 localhost172.16.80.200 k8s-master172.16.80.201 k8s-node1172.16.80.202 k8s-node2禁用swap分区12swapoff -ased -ri '/^[^#]*swap/s@^@#@' /etc/fstab部署kubernetes集群创建YUM源使用阿里云的kubernetes YUM源1234567[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg刷新YUM缓存1yum makecache安装kubernetes-v1.13.71yum install kubeadm-1.13.7-0.x86_64 kubelet-1.13.7-0.x86_64 kubectl-1.13.7-0.x86_64 cri-tools-1.12.0-0.x86_64 kubernetes-cni-0.7.5-0.x86_64添加bash自动补全命令12kubeadm completion bash &gt; /etc/bash_completion.d/kubeadmkubectl completion bash &gt; /etc/bash_completion.d/kubectl配置文件说明kubeadm的配置文件被拆分成了以下几个类型InitConfigurationClusterConfigurationKubeletConfigurationKubeProxyConfigurationJoinConfiguration对应到配置文件里面的格式如下1234567891011121314apiVersion: kubeadm.k8s.io/v1beta1kind: InitConfiguration---apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfiguration---apiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfiguration---apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfiguration---apiVersion: kubeadm.k8s.io/v1beta1kind: JoinConfiguration创建配置文件kubeadm命令可以打印InitConfiguration、ClusterConfiguration和JoinConfiguration的默认配置init-defaults1kubeadm config print init-defaults输出示例123456789101112131415161718192021222324252627282930313233343536373839404142---apiVersion: kubeadm.k8s.io/v1beta1bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: advertiseAddress: 1.2.3.4 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-master taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta1certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrolPlaneEndpoint: \"\"controllerManager: &#123;&#125;dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfigurationkubernetesVersion: v1.13.0networking: dnsDomain: cluster.local podSubnet: \"\" serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;---join-defaults1kubeadm config print join-defaults输出示例12345678910111213apiVersion: kubeadm.k8s.io/v1beta1caCertPath: /etc/kubernetes/pki/ca.crtdiscovery: bootstrapToken: apiServerEndpoint: kube-apiserver:6443 token: abcdef.0123456789abcdef unsafeSkipCAVerification: true timeout: 5m0s tlsBootstrapToken: abcdef.0123456789abcdefkind: JoinConfigurationnodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-master修改配置文件对于没有设置或者配置了一部分的参数，kubeadm会使用默认值这里精简了很多配置，自己决定是否增加配置！kubeadm-init.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168---# InitConfiguration影响master节点初始化时，kubelet的配置apiVersion: kubeadm.k8s.io/v1beta1kind: InitConfiguration# bootstrapTokens:# - groups:# - system:bootstrappers:kubeadm:default-node-token# token: abcdef.0123456789abcdef# ttl: 24h0m0s# usages:# - signing# - authenticationnodeRegistration: # master节点默认会加taints taints: - effect: NoSchedule key: node-role.kubernetes.io/master kubeletExtraArgs: pod-infra-container-image: \"k8s.gcr.io/pause:3.1\" network-plugin: cni---# kubeadm生成证书、manifest目录、staticPod的配置apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfiguration# kube-apiserver配置apiServer: timeoutForControlPlane: 4m0s # 配置证书的SANs certSANs: - localhost - k8s-master - kubernetes - kubernetes.default - kubernetes.default.svc - kubernetes.default.svc.cluster.local - 127.0.0.1 - 10.96.0.1 - 172.16.80.200 # 配置apiserver启动参数 extraArgs: authorization-mode: \"Node,RBAC\" runtime-config: \"api/all=true\" # 配置apiserver容器挂载volume extraVolumes: - name: \"timezone-volume\" hostPath: \"/usr/share/zoneinfo/Asia/Shanghai\" mountPath: \"/etc/localtime\" readOnly: true pathType: File# 配置证书目录certificatesDir: /etc/kubernetes/pki# 配置集群名字clusterName: kubernetescontrolPlaneEndpoint: \"\"# kube-controller-manager配置controllerManager: # 配置controller-manager启动参数 extraArgs: address: \"0.0.0.0\" extraVolumes: - name: \"timezone-volume\" hostPath: \"/usr/share/zoneinfo/Asia/Shanghai\" mountPath: \"/etc/localtime\" readOnly: true pathType: File# kubernetes集群dns配置dns: # 类型可选 CoreDNS 或者 kube-dns type: CoreDNS imageRepository: \"k8s.gcr.io\" imageTag: \"1.2.6\"# etcd配置etcd: # local和external配置是冲突的，二选一 local: imageRepository: \"k8s.gcr.io\" imageTag: \"3.2.24\" # etcd数据目录 dataDir: \"/var/lib/etcd\" extraArgs: advertise-client-urls: \"https://172.16.80.200:2379\" listen-client-urls: \"https://127.0.0.1:2379,https://172.16.80.200:2379\" listen-peer-urls: \"https://172.16.80.200:2380\" # 配置etcd server证书的SAN serverCertSANs: - k8s-master - localhost - ::1 - 127.0.0.1 - 172.16.80.200 # 配置etcd peer证书的SAN peerCertSANs: - k8s-master - localhost - ::1 - 127.0.0.1 - 172.16.80.200 # local和external配置是冲突的，二选一 # external: # endpoints: # - \"https://etcd_1:2379\" # - \"https://etcd_2:2379\" # - \"https://etcd_3:2379\" # caFile: \"/path/to/etcd-ca.crt\" # certFile: \"/path/to/etcd-client.crt\" # keyFile: \"/path/to/etcd-client.key\"imageRepository: k8s.gcr.iokubernetesVersion: \"v1.13.7\"# kubernetes网络配置networking: # DNS域名 dnsDomain: cluster.local # Pod网络 podSubnet: \"10.244.0.0/16\" # Service网络 serviceSubnet: \"10.96.0.0/12\"# kube-scheduler配置scheduler: extraArgs: address: \"0.0.0.0\" extraVolumes: - name: \"timezone-volume\" hostPath: \"/usr/share/zoneinfo/Asia/Shanghai\" mountPath: \"/etc/localtime\" readOnly: true pathType: File# 是否使用HyperKube镜像# hyperkube包含所有kubernetes的二进制文件,实现单二进制文件运行所有kubernetes服务useHyperKubeImage: false---# 配置kubeletapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationcgroupDriver: cgroupfsclusterDNS:- 10.96.0.10clusterDomain: cluster.local# 默认配置下，kubelet检测到系统启用了swap会启动失败failSwapOn: falsemaxOpenFiles: 1048576maxPods: 110---# 配置kube-proxyapiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationconfigSyncPeriod: 15m0sipvs: scheduler: \"rr\"# 可选iptables或者ipvs# iptables性能好兼容性好# ipvs大规模场景下性能更好mode: \"ipvs\"---# 这里的配置影响kubeadm join的节点上运行的kubelet参数apiVersion: kubeadm.k8s.io/v1beta1kind: JoinConfiguration#caCertPath: /etc/kubernetes/pki/ca.crt#discovery:# bootstrapToken:# apiServerEndpoint: kube-apiserver:6443# token: abcdef.0123456789abcdef# unsafeSkipCAVerification: true# timeout: 5m0s# tlsBootstrapToken: abcdef.0123456789abcdefnodeRegistration: kubeletExtraArgs: pod-infra-container-image: \"k8s.gcr.io/pause:3.1\" network-plugin: cni检查配置文件1kubeadm init --config=kubeadm-init.yaml --dry-run命令执行完毕，可以在/tmp/kubeadm-init-dryrun*里面看到生成的配置文件可以检查一下是否符合预期！初始化集群1kubeadm init --config=kubeadm-init.yaml集群初始化完成之后，终端会打印出信息12345678910111213141516Your Kubernetes master has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join 172.16.80.200:6443 --token uqc4n7.mfs4ep1br9l9ztsi --discovery-token-ca-cert-hash sha256:b30dd4d598baf1a3a53fe794d02302788e71e4314947cf7df5a75e98dd2ed97a节点加入集群运行kubeadm init初始化集群之后的提供的join命令，将节点加入集群1kubeadm join 172.16.80.200:6443 --token uqc4n7.mfs4ep1br9l9ztsi --discovery-token-ca-cert-hash sha256:b30dd4d598baf1a3a53fe794d02302788e71e4314947cf7df5a75e98dd2ed97a查看节点信息还没部署CNI插件，所以节点状态为NotReady1kubectl get nodes -o wide输出示例1234NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-master NotReady master 4m45s v1.13.7 172.16.80.200 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.2.el7.x86_64 docker://18.9.6k8s-node1 NotReady &lt;none&gt; 4m18s v1.13.7 172.16.80.201 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.2.el7.x86_64 docker://18.9.6k8s-node2 NotReady &lt;none&gt; 4m17s v1.13.7 172.16.80.202 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.2.el7.x86_64 docker://18.9.6部署CNI插件这里用flannel1kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml查看集群状态查看节点信息1kubectl get node可以看到节点状态为Ready1234NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-master Ready master 7m13s v1.13.7 172.16.80.200 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.2.el7.x86_64 docker://18.9.6k8s-node1 Ready &lt;none&gt; 6m46s v1.13.7 172.16.80.201 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.2.el7.x86_64 docker://18.9.6k8s-node2 Ready &lt;none&gt; 6m45s v1.13.7 172.16.80.202 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.21.2.el7.x86_64 docker://18.9.6查看Pod信息1kubectl get pod --all-namespaces -o wide --sort-by=.spec.nodeName这里根据节点名字作为排序依据来输出结果12345678910111213NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkube-system etcd-k8s-master 1/1 Running 0 6m56s 172.16.80.200 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-apiserver-k8s-master 1/1 Running 0 7m13s 172.16.80.200 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-controller-manager-k8s-master 1/1 Running 0 7m5s 172.16.80.200 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-kccsf 1/1 Running 0 2m27s 172.16.80.200 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-proxy-m68d5 1/1 Running 0 7m56s 172.16.80.200 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-scheduler-k8s-master 1/1 Running 0 7m1s 172.16.80.200 k8s-master &lt;none&gt; &lt;none&gt;kube-system coredns-86c58d9df4-jv465 1/1 Running 0 7m56s 10.244.1.2 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system coredns-86c58d9df4-td2nl 1/1 Running 0 7m56s 10.244.1.3 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-s9pds 1/1 Running 0 2m27s 172.16.80.201 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-l67jr 1/1 Running 0 7m48s 172.16.80.201 k8s-node1 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-r6h8s 1/1 Running 0 2m27s 172.16.80.202 k8s-node2 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-pjz5w 1/1 Running 0 7m47s 172.16.80.202 k8s-node2 &lt;none&gt; &lt;none&gt;master节点参与负载kubeadm部署的kubernetes集群，出于安全考虑，在初始化集群后会给master节点打上node-role.kubernetes.io/master:NoSchedule污点，避免Pod被调度到master节点。作为实验环境，可以去除污点，让Pod可以调度到master节点1kubectl taint nodes k8s-master node-role.kubernetes.io/master-至此kubernetes集群已经搭建完成最简化的部署！！！！","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"Redis安装部署记录","slug":"Redis安装部署记录","date":"2019-06-03T09:19:48.000Z","updated":"2019-11-21T02:45:16.000Z","comments":true,"path":"2019/06/03/Redis安装部署记录.html","link":"","permalink":"https://luanlengli.github.io/2019/06/03/Redis安装部署记录.html","excerpt":"","text":"说明仅记录安装部署过程不保证ctrl+c和ctrl+v可以跑起来，自己判断哪些要改的！操作系统使用的CentOS-7.6.1810 x86_64Redis版本使用4.0.14【根据自己的环境灵活调整，生产环境请务必提供独立数据盘！】环境准备关闭SELINUX12sed -i 's,SELINUX=enforcing,SELINUX=disabled,' /etc/selinux/configsetenforce 0关闭防火墙12systemctl stop firewalldsystemctl disable firewalld添加sysctl参数12345678910111213141516171819202122232425262728cat &gt; /etc/sysctl.d/99-redis.conf &lt;&lt;EOF # 最大文件句柄数fs.file-max=1048576# 在CentOS7.4引入了一个新的参数来控制内核的行为。 # /proc/sys/fs/may_detach_mounts 默认设置为0# 当系统有容器运行的时候，需要将该值设置为1。fs.may_detach_mounts = 1# 最大文件打开数fs.nr_open=1024000# 修改动态NAT跟踪记录参数net.netfilter.nf_conntrack_max = 655350net.netfilter.nf_conntrack_tcp_timeout_established = 1200# 加快系统关闭处于 FIN_WAIT2 状态的 TCP 连接net.ipv4.tcp_fin_timeout = 30# 系统中处于 SYN_RECV 状态的 TCP 连接数量net.ipv4.tcp_max_syn_backlog = 8192# 内核中管理 TIME_WAIT 状态的数量net.ipv4.tcp_max_tw_buckets = 5000# 端口最大的监听队列的长度net.core.somaxconn=512# 允许应用程序能够绑定到不属于本地网卡的地址net.ipv4.ip_nonlocal_bind=1 # 内存耗尽才使用swap分区vm.swappiness = 0vm.panic_on_oom = 0# 内核允许超量使用内存直到用完为止vm.overcommit_memory = 1EOF修改limits参数123456cat &gt; /etc/security/limits.d/99-redis.conf &lt;&lt;EOF* soft nproc 1048576* hard nproc 1048576* soft nofile 1048576* hard nofile 1048576EOF修改journal设置12345sed -e 's,^#Compress=yes,Compress=yes,' \\ -e 's,^#SystemMaxUse=,SystemMaxUse=1G,' \\ -e 's,^#Seal=yes,Seal=yes,' \\ -e 's,^#RateLimitBurst=1000,RateLimitBurst=2000,' \\ -i /etc/systemd/journald.conf更新系统软件1yum update -y安装编译环境1yum install gcc make jemalloc tcl -y安装Redis创建用户12groupadd redisuseradd -g redis -s /bin/false redis创建目录1mkdir -p /opt/software /opt/redis-data下载Redis代码1wget http://download.redis.io/releases/redis-4.0.14.tar.gz -O - | tar xz --directory=/opt/software/创建软链接1ln -sv /opt/software/redis-4.0.14 /opt/software/redis切换目录1cd /opt/software/redis编译安装1make MALLOC=jemalloc &amp;&amp; make test &amp;&amp; make install创建配置文件1234567891011121314151617181920212223242526272829303132333435363738394041cat &gt; /opt/redis-data/redis.conf &lt;&lt;EOF# 监听地址bind 0.0.0.0# 监听端口port 6379tcp-backlog 2048timeout 0tcp-keepalive 300# 是否以守护进程的方式启动daemonize yesprotected-mode yessupervised no# 设置Redis有多少数据库databases 16save 900 1save 300 10save 60 10000# 定义数据目录dir \"/opt/redis-data\"dbfilename \"dump_6379.rdb\"pidfile \"/opt/redis-data/redis_6379.pid\"logfile \"/opt/redis-data/redis_6379.log\"loglevel notice# 定义访问redis的密码requirepass \"abcd1234\"appendonly noappendfilename \"redis_6379.aof\"appendfsync everysecno-appendfsync-on-rewrite nostop-writes-on-bgsave-error yesrdbcompression yesrdbchecksum yesauto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mbaof-load-truncated yeslua-time-limit 5000slowlog-log-slower-than 10000slowlog-max-len 128# 限制最大内存使用量maxmemory 4gbEOF修改目录权限1chown -R redis:redis /opt/redis-data启动Redis这里以前台方式运行Redis，查看输出日志，确认是否正常运行1su -s /bin/sh -c \"/usr/local/bin/redis-server /opt/redis-data/redis.conf --daemonize no\" redissystemd服务1234567891011121314151617cat &gt; /usr/lib/systemd/system/redis.service &lt;&lt;EOF[Unit]Description=Redis In-Memory Data StoreAfter=network.target[Service]User=redisGroup=redisExecStart=/usr/local/bin/redis-server /opt/redis-data/redis.conf --daemonize noExecStop=/usr/local/bin/redis-cli -a abcd1234 shutdownRestart=on-failureRestartSec=60sLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF设置开机自启动123systemctl daemon-reloadsystemctl enable redis.servicesystemctl start redis.service验证功能登录redis1redis-cli -h 127.0.0.1 -p 6379认证登录12redis&gt; auth abcd1234ok测试功能12redis&gt; ping\"PONG\"","categories":[],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://luanlengli.github.io/tags/Redis/"}]},{"title":"MySQL基于PXC方案实现数据库HA","slug":"MySQL基于PXC方案实现数据库HA","date":"2019-05-31T02:59:42.000Z","updated":"2019-06-04T06:35:46.000Z","comments":true,"path":"2019/05/31/MySQL基于PXC方案实现数据库HA.html","link":"","permalink":"https://luanlengli.github.io/2019/05/31/MySQL基于PXC方案实现数据库HA.html","excerpt":"","text":"说明仅记录安装部署过程不保证ctrl+c和ctrl+v可以跑起来，自己判断哪些要改的！操作系统使用的CentOS-7.6.1810 x86_64Percona XtraDB Cluster版本使用57-5.7.25-31.35.1.el7【根据自己的环境灵活调整，生产环境请务必提供独立数据盘！】集群架构模型多主复制模型环境准备服务器配置主机名IP地址db1172.16.80.201db2172.16.80.202db3172.16.80.203关闭SELINUX12sed -i 's,SELINUX=enforcing,SELINUX=disabled,' /etc/selinux/configsetenforce 0关闭防火墙123systemctl stop firewalldsystemctl disable firewalldsystemctl mask firewalld添加sysctl参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849cat &gt; /etc/sysctl.d/99-centos.conf &lt;&lt;EOF # 最大文件句柄数fs.file-max = 1048576# 最大文件打开数fs.nr_open = 1048576# 单个共享内存段的最大值，这里设置为物理内存大小的一半# 计算方式，以2G内存为例# 2048 / 2 * 1024 * 1024 = 1073741824kernel.shmmax = 1073741824# 最大的TCP数据接收窗口（字节）net.core.rmem_max = 4194304# 最大的TCP数据发送窗口（字节）net.core.wmem_max = 4194304# 默认的TCP数据接收窗口大小（字节）net.core.rmem_default = 262144# 默认的TCP数据发送窗口大小（字节）net.core.wmem_default = 262144# 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤net.bridge.bridge-nf-call-arptables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# 关闭严格校验数据包的反向路径net.ipv4.conf.default.rp_filter=0net.ipv4.conf.all.rp_filter=0# 修改动态NAT跟踪记录参数net.netfilter.nf_conntrack_max = 655350net.netfilter.nf_conntrack_tcp_timeout_established = 1200# 加快系统关闭处于 FIN_WAIT2 状态的 TCP 连接net.ipv4.tcp_fin_timeout = 30# 系统中处于 SYN_RECV 状态的 TCP 连接数量net.ipv4.tcp_max_syn_backlog = 8192# 内核中管理 TIME_WAIT 状态的数量net.ipv4.tcp_max_tw_buckets = 5000# 端口最大的监听队列的长度net.core.somaxconn=4096# 打开ipv4数据包转发net.ipv4.ip_forward=1# 允许应用程序能够绑定到不属于本地网卡的地址net.ipv4.ip_nonlocal_bind=1 # 内存耗尽才使用swap分区vm.swappiness = 0vm.panic_on_oom = 0# 配置主动发起连接时，端口范围，默认值32768-60000net.ipv4.ip_local_port_range = 9000 65535# 当系统pagecache脏页达到系统内存dirty_ratio的百分比值时，会阻塞新的写请求# 直到内存脏页落盘# 默认值为30vm.dirty_ratio = 80EOFlimits参数12345678910cat &gt; /etc/security/limits.d/99-mysql.conf &lt;&lt;EOF* soft nofile 655360* hard nofile 655360* soft nproc 655360* hard nproc 655360* soft stack unlimited* hard stack unlimited* soft memlock 250000000* hard memlock 250000000EOF更新系统软件1yum update -y重启服务器可选1reboot安装部署获取安装包可以通过配置YUM源或者直接下载RPM包安装配置YUM源YUM源配置方法可以看官方教程12yum install -y https://repo.percona.com/yum/percona-release-latest.noarch.rpmyum install -y Percona-XtraDB-Cluster-full-57下载安装包官方下载页面链接在这里12345678910wget https://www.percona.com/downloads/Percona-XtraDB-Cluster-LATEST/Percona-XtraDB-Cluster-5.7.25-31.35/binary/redhat/7/x86_64/Percona-XtraDB-Cluster-devel-57-5.7.25-31.35.1.el7.x86_64.rpmwget https://www.percona.com/downloads/Percona-XtraDB-Cluster-LATEST/Percona-XtraDB-Cluster-5.7.25-31.35/binary/redhat/7/x86_64/Percona-XtraDB-Cluster-57-5.7.25-31.35.1.el7.x86_64.rpmwget https://www.percona.com/downloads/Percona-XtraDB-Cluster-LATEST/Percona-XtraDB-Cluster-5.7.25-31.35/binary/redhat/7/x86_64/Percona-XtraDB-Cluster-client-57-5.7.25-31.35.1.el7.x86_64.rpmwget https://www.percona.com/downloads/Percona-XtraDB-Cluster-LATEST/Percona-XtraDB-Cluster-5.7.25-31.35/binary/redhat/7/x86_64/Percona-XtraDB-Cluster-full-57-5.7.25-31.35.1.el7.x86_64.rpmwget https://www.percona.com/downloads/Percona-XtraDB-Cluster-LATEST/Percona-XtraDB-Cluster-5.7.25-31.35/binary/redhat/7/x86_64/Percona-XtraDB-Cluster-test-57-5.7.25-31.35.1.el7.x86_64.rpmwget https://www.percona.com/downloads/Percona-XtraDB-Cluster-LATEST/Percona-XtraDB-Cluster-5.7.25-31.35/binary/redhat/7/x86_64/Percona-XtraDB-Cluster-server-57-5.7.25-31.35.1.el7.x86_64.rpmwget https://www.percona.com/downloads/Percona-XtraDB-Cluster-LATEST/Percona-XtraDB-Cluster-5.7.25-31.35/binary/redhat/7/x86_64/Percona-XtraDB-Cluster-shared-compat-57-5.7.25-31.35.1.el7.x86_64.rpmwget https://www.percona.com/downloads/Percona-XtraDB-Cluster-LATEST/Percona-XtraDB-Cluster-5.7.25-31.35/binary/redhat/7/x86_64/Percona-XtraDB-Cluster-shared-57-5.7.25-31.35.1.el7.x86_64.rpmwget https://www.percona.com/downloads/Percona-XtraBackup-2.4/Percona-XtraBackup-2.4.14/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.14-1.el7.x86_64.rpmwget http://repo.percona.com/percona/yum/release/7/RPMS/x86_64/qpress-11-1.el7.x86_64.rpm手动安装YUM包1yum localinstall -y *rpm初始化数据库注意！这里的初始化操作可以在集群任意节点进行，这里为了方便，在db1上进行操作1systemctl start mysql.service启动完成后之后，需要获取临时密码1grep 'temporary password' /var/log/mysqld.log使用mysql_secure_installation初始化MySQL数据库修改root密码修改密码复杂度要求禁止root远程登录删除匿名用户和测试数据刷新权限1mysql_secure_install登录数据库1mysql -u root -p创建SST用户并授权123CREATE USER 'sstuser'@'localhost' IDENTIFIED BY 'sstuser_password';GRANT RELOAD, LOCK TABLES, PROCESS, REPLICATION CLIENT ON *.* TO 'sstuser'@'localhost';FLUSH PRIVILEGES;关闭数据库1systemctl stop mysql.service配置Galera配置过程参照官方文档额外定义一些参数节点1清理一下配置文件12find /etc/my.cnf.d/ -type f -name *cnffind /etc/percona-xtradb-cluster.conf.d/ -type f -name *cnf将配置内容合并123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128# Template my.cnf for PXC# Edit to your requirements.[client]# 定义客户端从哪里获取mysql.sock的路径socket=/var/run/mysqld/mysql.sock[mysql]prompt=\"\\u@db1 \\R:\\m:\\s [\\d]&gt; \"no-auto-rehash[mysqld]# 监听地址bind-address = 0.0.0.0# 监听端口port = 3306# 默认数据库引擎default-storage-engine = innodb# 字符集character-set-server = utf8mb4# 排序规则collation-server = utf8mb4_general_ci# 最大连接数max_connections = 1000# 限制打开文件数open_files_limit = 65535# server-id每个服务器唯一即可，一般用IP末位数字server-id = 201# 默认时区default-time-zone = '+8:00'# 设置数据库事务隔离级别transaction_isolation = REPEATABLE-READ# 配置超时时间interactive_timeout = 600wait_timeout = 600connect_timeout = 20lock_wait_timeout = 3600# 限制数据库服务器接收数据包的大小max_allowed_packet = 64M# 配置排序和join的buffer大小sort_buffer_size = 4Mjoin_buffer_size = 4M# 这里定义数据目录和日志目录datadir = /var/lib/mysql/socket = /var/run/mysql/mysql.socklog-error = /var/log/mysql/error.logpid-file = /var/run/mysql/mysqld.pidlog-bin = /var/lib/mysql/mybinlog# 配置binlogbinlog_format = ROWexpire_logs_days = 7# 开启binlog校验功能binlog_checksum = 1binlog_cache_size = 4M# 每1次提交变更后立刻将binlog落盘sync_binlog = 1# 配置performance_schemaperformance_schema = onperformance_schema_instrument = '%memory%=on'performance_schema_instrument = '%lock%=on'skip-external-locking = on# 禁用域名解析skip-name-resolve = on# 禁用软链接symbolic-links = 0back_log = 1024# 慢日志查询配置slow_query_log = 1long_query_time = 1slow_query_log_file = /var/log/mysqld/slow.log# 配置每个EVENT都要执行刷盘操作sync_master_info = 1sync_relay_log_info = 1sync_relay_log = 1log_slave_updates = onrelay_log_recovery = 1relay_log_purge = 1master_info_repository = TABLErelay_log_info_repository = TABLE# InnoDB引擎配置innodb_file_per_table = oninnodb_checksums = 1innodb_checksum_algorithm = crc32innodb_status_file = 1innodb_status_output = 0innodb_status_output_locks = 0innodb_stats_on_metadata = 0innodb_open_files = 65535innodb_flush_method = O_DIRECTinnodb_flush_sync = 0innodb_flush_neighbors = 0# 每个事务提交后立即刷新binlog文件innodb_flush_log_at_trx_commit = 1innodb_monitor_enable=\"module_innodb\"innodb_monitor_enable=\"module_server\"innodb_monitor_enable=\"module_dml\"innodb_monitor_enable=\"module_ddl\"innodb_monitor_enable=\"module_trx\"innodb_monitor_enable=\"module_os\"innodb_monitor_enable=\"module_purge\"innodb_monitor_enable=\"module_log\"innodb_monitor_enable=\"module_lock\"innodb_monitor_enable=\"module_buffer\"innodb_monitor_enable=\"module_index\"innodb_monitor_enable=\"module_ibuf_system\"innodb_monitor_enable=\"module_buffer_page\"innodb_monitor_enable=\"module_adaptive_hash\"# SQL mode配置，这里是默认值sql_mode = ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION# GTID配置#gtid_mode = on#enforce_gtid_consistency = 1# wsrep配置innodb_autoinc_lock_mode = 2wsrep_cluster_name = pxc-clusterwsrep_cluster_address = gcomm://172.16.80.201:4567,172.16.80.202:4567,172.16.80.203:4567wsrep_provider = /usr/lib64/galera3/libgalera_smm.sowsrep_node_name = db1wsrep_node_address = 172.16.80.201:4567wsrep_sst_receive_address = 172.16.80.201:4444wsrep_sst_method = xtrabackup-v2wsrep_sst_auth = sstuser:sstuser_passwordwsrep_slave_threads = 4# 禁用Percona XtraDB Cluster中实验特性和不受支持的功能pxc_strict_mode = ENFORCING[mysqldump]quickmax_allowed_packet = 32M节点2配置过程跟节点1大体上是一样，区别在于要修改某些配置项1234567[mysql]prompt=\"\\u@db2 \\R:\\m:\\s [\\d]&gt; \"[mysqld]server-id = 202wsrep_node_name = db2wsrep_node_address = 172.16.80.202:4567wsrep_sst_receive_address = 172.16.80.202:4444节点3配置过程跟节点1一样，区别在于要修改某些配置项1234567[mysql]prompt=\"\\u@db3 \\R:\\m:\\s [\\d]&gt; \"[mysqld]server-id = 203wsrep_node_name = db3wsrep_node_address = 172.16.80.203:4567wsrep_sst_receive_address = 172.16.80.203:4444初始化PXC集群节点1作为集群第一个节点引导启动集群，此节点必须包含完整的数据！否则会出现数据不一致这里以节点1作为bootstrap node1systemctl start mysql@bootstrap.service启动完成之后，可以查看日志1tail -f /var/log/mysql/error.log在看到ready for connection就可以启动其他节点了1YYYY-MM-DDTHH:mm:ss.534955Z 0 [Note] /usr/sbin/mysqld: ready for connections.节点2启动数据库1systemctl start mysql.service节点3启动数据库1systemctl start mysql.service注意如果提示WSREP: Failed to prepare for &#39;xtrabackup-v2&#39; SST. Unrecoverable.那么可以将配置文件里面的wsrep_sst_method=xtrabackup-v2修改为wsrep_sst_method=rsync然后再重新初始化集群检查集群启动状态登录数据库1mysql -u root -p检查wsrep节点数量12345678910show global status like 'wsrep_cluster_size';# 示例输出+--------------------+-------+| Variable_name | Value |+--------------------+-------+| wsrep_cluster_size | 3 |+--------------------+-------+1 row in set (0.00 sec)检查wsrep集群信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980show global status like 'wsrep%';# 示例输出+----------------------------------+----------------------------------------------------------+| Variable_name | Value |+----------------------------------+----------------------------------------------------------+| wsrep_local_state_uuid | 77463f8f-82bf-11e9-b41f-92927bef6908 || wsrep_protocol_version | 9 || wsrep_last_applied | 25 || wsrep_last_committed | 25 || wsrep_replicated | 14 || wsrep_replicated_bytes | 3640 || wsrep_repl_keys | 15 || wsrep_repl_keys_bytes | 456 || wsrep_repl_data_bytes | 2237 || wsrep_repl_other_bytes | 0 || wsrep_received | 6 || wsrep_received_bytes | 1225 || wsrep_local_commits | 0 || wsrep_local_cert_failures | 0 || wsrep_local_replays | 0 || wsrep_local_send_queue | 0 || wsrep_local_send_queue_max | 1 || wsrep_local_send_queue_min | 0 || wsrep_local_send_queue_avg | 0.000000 || wsrep_local_recv_queue | 0 || wsrep_local_recv_queue_max | 1 || wsrep_local_recv_queue_min | 0 || wsrep_local_recv_queue_avg | 0.000000 || wsrep_local_cached_downto | 12 || wsrep_flow_control_paused_ns | 0 || wsrep_flow_control_paused | 0.000000 || wsrep_flow_control_sent | 0 || wsrep_flow_control_recv | 0 || wsrep_flow_control_interval | [ 173, 173 ] || wsrep_flow_control_interval_low | 173 || wsrep_flow_control_interval_high | 173 || wsrep_flow_control_status | OFF || wsrep_cert_deps_distance | 1.000000 || wsrep_apply_oooe | 0.000000 || wsrep_apply_oool | 0.000000 || wsrep_apply_window | 1.000000 || wsrep_commit_oooe | 0.000000 || wsrep_commit_oool | 0.000000 || wsrep_commit_window | 1.000000 || wsrep_local_state | 4 || wsrep_local_state_comment | Synced || wsrep_cert_index_size | 3 || wsrep_cert_bucket_count | 22 || wsrep_gcache_pool_size | 5520 || wsrep_causal_reads | 0 || wsrep_cert_interval | 0.000000 || wsrep_open_transactions | 0 || wsrep_open_connections | 0 || wsrep_ist_receive_status | || wsrep_ist_receive_seqno_start | 0 || wsrep_ist_receive_seqno_current | 0 || wsrep_ist_receive_seqno_end | 0 || wsrep_incoming_addresses | 172.16.80.201:3306,172.16.80.202:3306,172.16.80.203:3306 || wsrep_cluster_weight | 3 || wsrep_desync_count | 0 || wsrep_evs_delayed | || wsrep_evs_evict_list | || wsrep_evs_repl_latency | 0/0/0/0/0 || wsrep_evs_state | OPERATIONAL || wsrep_gcomm_uuid | 5ed853a2-834a-11e9-a850-17ef655007c8 || wsrep_cluster_conf_id | 11 || wsrep_cluster_size | 3 || wsrep_cluster_state_uuid | 77463f8f-82bf-11e9-b41f-92927bef6908 || wsrep_cluster_status | Primary || wsrep_connected | ON || wsrep_local_bf_aborts | 0 || wsrep_local_index | 0 || wsrep_provider_name | Galera || wsrep_provider_vendor | Codership Oy &lt;info@codership.com&gt; || wsrep_provider_version | 3.35(rddf9876) || wsrep_ready | ON |+----------------------------------+----------------------------------------------------------+71 rows in set (0.00 sec)简单验证集群在节点1创建数据库1mysql@db1&gt; CREATE DATABASE percona_test;在节点2上创建表12mysql@db2&gt; USE percona_test;mysql@db2&gt; CREATE TABLE example (node_id INT PRIMARY KEY, node_name VARCHAR(30));在节点2上插入数据1mysql@db2&gt; INSERT INTO percona_test.example VALUES (2, 'db2');在节点3上查数据1234567mysql@db3&gt; SELECT * FROM percona_test.example;+---------+-----------+| node_id | node_name |+---------+-----------+| 2 | db2 |+---------+-----------+1 row in set (0.00 sec)集群维护操作集群整体关机按顺序逐台关停，例如db1→db2→db3集群整体开机启动时，则按照db3→db2→db1其中db3启动时，需要作为bootstrap node引导集群systemctl start mysql@bootstrap.service集群少数节点维护PXC集群能容忍少数节点离线而不影响集群数据库服务这里的少数节点是指少于50%，即&lt; 50%少数节点重新上线之后，会自动完成数据同步，在完成同步之后，wsrep状态被设置为true，才对外提供服务","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://luanlengli.github.io/tags/MySQL/"}]},{"title":"Kong API网关搭建部署记录","slug":"Kong-API网关搭建部署记录","date":"2019-05-29T02:27:08.000Z","updated":"2019-07-03T02:43:09.000Z","comments":true,"path":"2019/05/29/Kong-API网关搭建部署记录.html","link":"","permalink":"https://luanlengli.github.io/2019/05/29/Kong-API网关搭建部署记录.html","excerpt":"","text":"说明仅记录安装部署过程操作系统使用的CentOS-7.6.1810 x86_64kong版本为1.1.2官方文档在这里环境准备PostgreSQL安装过程参考PostgreSQL10安装部署和初始化创建kong所需的数据库和用户登录PostgreSQL数据库1psql -U postgres创建用户1create user kong with password 'kong_password';创建数据库1create database kong owner kong;授权用户拥有数据库所有权限1grant all privileges on database kong to kong;Kong安装配置YUM源12345678cat &gt; /etc/yum.repos.d/kong.repo &lt;&lt;EOF[bintray--kong-kong-rpm]name=bintray--kong-kong-rpmbaseurl=https://kong.bintray.com/kong-rpm/centos/7gpgcheck=0repo_gpgcheck=0enabled=1EOF安装kong1yum install -y kong-1.1.2-1配置kong/etc/kong/kong.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109# -----------------------# Kong configuration file# -----------------------#------------------------------------------------------------------------------# GENERAL#------------------------------------------------------------------------------# 没啥好调的prefix = /usr/local/kong/log_level = noticeproxy_access_log = /var/log/kong/access.logproxy_error_log = /var/log/kong/error.logadmin_access_log = /var/log/kong/admin_access.logadmin_error_log = /var/log/kong/admin_error.log#plugins = bundledanonymous_reports = off#------------------------------------------------------------------------------# NGINX#------------------------------------------------------------------------------# proxy端口为对外提供服务的端口proxy_listen = 0.0.0.0:80, 0.0.0.0:443 http2 ssl#stream_listen = off# 管理端口，注意安全！# 此API接口是无权限认证的！admin_listen = 127.0.0.1:8001, 127.0.0.1:8444 sslnginx_user = nobody nobodynginx_worker_processes = autonginx_daemon = onmem_cache_size = 128m# SSL/TLS套件的说明可以看这里# https://wiki.mozilla.org/Security/Server_Side_TLSssl_cipher_suite = modern#ssl_ciphers = \"\"# 配置proxy端口的https证书#ssl_cert = /path/to/cert#ssl_cert_key = /path/to/key# 客户端证书认证，一般情况不用设置#client_ssl = off#client_ssl_cert = #client_ssl_cert_key = # 配置Admin API端口的https证书#admin_ssl_cert = #admin_ssl_cert_key = # 与upstream保持TCP连接，减少频繁建立TCP连接带来的性能损耗upstream_keepalive = 60#headers = server_tokens, latency_tokens#trusted_ips = # 经过CDN或者多级HTTP服务反向代理时，添加X-Real-IP请求头可以追溯到客户端真实IPreal_ip_header = X-Real-IP#real_ip_recursive = off# 客户端请求的body过大时会出现payload too large# 设置为0时，不限制client_max_body_size = 0client_body_buffer_size = 16kerror_default_type = text/plain#------------------------------------------------------------------------------# DATASTORE#------------------------------------------------------------------------------database = postgrespg_host = 127.0.0.1pg_port = 5432pg_timeout = 5000pg_user = kongpg_password = kong_passwordpg_database = uat_kongpg_schema = publicpg_ssl = off#pg_ssl_verify = off#------------------------------------------------------------------------------# DATASTORE CACHE#------------------------------------------------------------------------------# 使用数据存储更新缓存的频率(秒)，默认值5db_update_frequency = 5# 数据库节点的更新时间，对于最终一致性模型的数据库，需要配置各数据库节点之间数据同步的延迟# 整个kong同步配置的延迟时间 等于 db_update_frequency + db_update_propagation#db_update_propagation = 0# 缓存过期时间#db_cache_ttl = 0# 缓存刷新时间#db_resurrect_ttl = 30#------------------------------------------------------------------------------# DNS RESOLVER#------------------------------------------------------------------------------# 配置DNS服务器列表，仅提供给Kong使用，不会覆盖系统配置#dns_resolver = # 配置Kong的hosts文件，必须要重启Kong才能生效，仅提供给Kong使用，不会覆盖系统配置#dns_hostsfile = /etc/hosts# 解析不同记录类型的顺序。“LAST”类型表示最后一次成功查找的类型#dns_order = LAST,SRV,A,CNAME#dns_valid_ttl = # 配置DNS记录缓存过期时间#dns_stale_ttl = 4#dns_not_found_ttl = 30#dns_error_ttl = 1#dns_no_sync = off#------------------------------------------------------------------------------# DEVELOPMENT &amp; MISCELLANEOUS#------------------------------------------------------------------------------# 这个地方没啥好改的，默认值就好了#lua_ssl_trusted_certificate = #lua_ssl_verify_depth = 1#lua_package_path = ./?.lua;./?/init.lua;#lua_package_cpath = #lua_socket_pool_size = 30创建日志目录1mkdir -p /var/log/kong初始化kong数据库1/usr/local/bin/kong migrations bootstrap -c /etc/kong/kong.conf数据库初始化时输出示例12345678910111213141516171819202122232425262728293031323334353637bootstrapping database...migrating core on database 'kong'...core migrated up to: 000_base (executed)core migrated up to: 001_14_to_15 (executed)core migrated up to: 002_15_to_1 (executed)core migrated up to: 003_100_to_110 (executed)migrating oauth2 on database 'kong'...oauth2 migrated up to: 000_base_oauth2 (executed)oauth2 migrated up to: 001_14_to_15 (executed)oauth2 migrated up to: 002_15_to_10 (executed)migrating acl on database 'kong'...acl migrated up to: 000_base_acl (executed)acl migrated up to: 001_14_to_15 (executed)migrating jwt on database 'kong'...jwt migrated up to: 000_base_jwt (executed)jwt migrated up to: 001_14_to_15 (executed)migrating basic-auth on database 'kong'...basic-auth migrated up to: 000_base_basic_auth (executed)basic-auth migrated up to: 001_14_to_15 (executed)migrating key-auth on database 'kong'...key-auth migrated up to: 000_base_key_auth (executed)key-auth migrated up to: 001_14_to_15 (executed)migrating rate-limiting on database 'kong'...rate-limiting migrated up to: 000_base_rate_limiting (executed)rate-limiting migrated up to: 001_14_to_15 (executed)rate-limiting migrated up to: 002_15_to_10 (executed)rate-limiting migrated up to: 003_10_to_112 (executed)migrating hmac-auth on database 'kong'...hmac-auth migrated up to: 000_base_hmac_auth (executed)hmac-auth migrated up to: 001_14_to_15 (executed)migrating response-ratelimiting on database 'kong'...response-ratelimiting migrated up to: 000_base_response_rate_limiting (executed)response-ratelimiting migrated up to: 001_14_to_15 (executed)response-ratelimiting migrated up to: 002_15_to_10 (executed)24 migrations processed24 executeddatabase is up-to-date启动kong1/usr/local/bin/kong start -c /etc/kong/kong.conf查看监听端口80和443端口是业务端口8001和8444是Admin API端口123456Active Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 17493/nginx: mastertcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 17493/nginx: mastertcp 0 0 127.0.0.1:8444 0.0.0.0:* LISTEN 17493/nginx: mastertcp 0 0 127.0.0.1:8001 0.0.0.0:* LISTEN 17493/nginx: masterKong访问测试访问本机的8001端口1curl -i http://localhost:8001/输出示例1234567HTTP/1.1 200 OKDate: Wed, 29 May 2019 06:38:35 GMTContent-Type: application/json; charset=utf-8Connection: keep-aliveAccess-Control-Allow-Origin: *Server: kong/1.1.2Content-Length: 5777查看Nginx日志在本文中已经定义了日志路径为/var/log/kong/可以看到有四个日志文件12ls /usr/local/kong/logs/access.log admin_access.log admin_error.log error.log查看Nginx的PID文件kong的配置文件没法定义PID文件的路径默认在/usr/local/kong/pids/nginx.pid创建kong的systemd服务脚本/usr/lib/systemd/system/kong.service1234567891011121314151617[Unit]Description=Kong API GatewayAfter=network.target[Service]User=rootGroup=rootExecStart=/usr/local/bin/kong start -c /etc/kong/kong.confExecStop=/usr/local/bin/kong stopExecReload=/usr/local/bin/kong reload -c /etc/kong/kong.confType=forkingPIDFile=/usr/local/kong/pids/nginx.pidRestartSec=60Restart=on-failure[Install]WantedBy=default.targetKong集群单节点集群只有一个kong节点连接到数据库（cassandra或者PostgreSQL）时，通过kong的Admin API提交的变更都会立刻生效。kong的配置信息是缓存在内存中。多节点集群在多个kong节点组成的集群中，这些kong节点都连接到同一个数据库（cassandra或者PostgreSQL），即可组成集群。当其中一个节点提交了修改，那么其他节点是不会立刻感知到已经提交的变更。因此所有kong节点会在后台定期执行任务，从数据库中读取其他节点提交的变更。db_update_frequency 默认值5每5秒，集群中所有的kong节点都会从数据库中获取最新的配置信息，并缓存到内存中db_update_propagation默认值0如果使用了Cassandra数据库集群，那么如果数据库有更新，最多需要db_update_propagation时间来同步所有的数据库副本。如果使用PostgreSQL或者单数据库，这个值可以被设置为0缓存以下数据会被缓存在内存中，并通过后台任务的机制查询后端数据库进行更新ServicesRoutesPluginsConsumersCredentialsKonga安装konga是kong的web可视化管理工具这里在kong服务器上运行konga安装Docker-CE安装过程见官方文档启动Docker12systemctl enable docker.servicesystemctl start docker.service创建konga数据库登录PostgreSQL1psql -U postgres创建konga数据库1create database konga owner kong;授权kong用户拥有konga数据库所有权限1grant all privileges on database konga to kong;初始化konga数据库123456docker run --rm \\ --net=host \\ pantsel/konga:latest \\ -c prepare \\ -a postgres \\ -u postgresql://kong:kong_password@127.0.0.1:5432/konga输出示例123456789101112131415debug: Preparing database...Using postgres DB Adapter.Database exists. Continue...debug: Hook:api_health_checks:process() calleddebug: Hook:health_checks:process() calleddebug: Hook:start-scheduled-snapshots:process() calleddebug: Hook:upstream_health_checks:process() calleddebug: Hook:user_events_hook:process() calleddebug: Seeding User...debug: User seed planteddebug: Seeding Kongnode...debug: Kongnode seed planteddebug: Seeding Emailtransport...debug: Emailtransport seed planteddebug: Database migrations completed!运行konga共享宿主机网络命名空间12345678910docker run -d \\ --net=host \\ --name konga \\ --restart=always \\ -e \"PORT=1337\" \\ -e \"NODE_ENV=production\" \\ -e \"KONGA_LOG_LEVEL=info\" \\ -e \"DB_ADAPTER=postgres\" \\ -e \"DB_URI=postgresql://kong:kong_password@127.0.0.1:5432/konga\" \\ pantsel/konga:latest查看监听端口123Active Internet connections (servers and established)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp6 0 0 :::1337 :::* LISTEN 19341/node查看konga日志1docker logs konga输出示例12345678910111213141516171819202122232425262728293031Hook:api_health_checks:process() calledHook:health_checks:process() calledHook:start-scheduled-snapshots:process() calledHook:upstream_health_checks:process() calledHook:user_events_hook:process() calledUser had models, so no seed neededKongnode had models, so no seed neededEmailtransport seeds updated .-..-. Sails &lt;| .-..-. v0.12.14 |\\ /|.\\ / || \\ ,' |' \\ .-'.-==|/_--' `--'-------' __---___--___---___--___---___--___ ____---___--___---___--___---___--___-__Server lifted in `/app`To see your app, visit http://localhost:1337To shut down Sails, press &lt;CTRL&gt; + C at any time.-------------------------------------------------------:: Weekday Month Day Years HH:mm:ss GMT+0000 (UTC)Environment : productionPort : 1337-------------------------------------------------------访问konga使用浏览器访问http://konga服务器IP:1337第一次打开konga需要注册管理员用户，请保证密码强度足够高在弹出的New Connection窗口选择DefaultName填写kong的名字kong Admin URL填写http://127.0.0.1:8001Kong维护检查kong配置文件在更改配置文件之后，可以通过命令检查配置文件1kong check -v /etc/kong/kong.conf输出示例1234YYYY/MM/DD HH:mm:ss [verbose] Kong: 1.1.2YYYY/MM/DD HH:mm:ss [verbose] reading config file at /etc/kong/kong.confYYYY/MM/DD HH:mm:ss [verbose] prefix in use: /usr/local/kongYYYY/MM/DD HH:mm:ss [info] configuration at /etc/kong/kong.conf is validKong健康检查1kong health --v输出示例123456YYYY/MM/DD HH:mm:ss [verbose] Kong: 1.1.2YYYY/MM/DD HH:mm:ss [verbose] reading config file at /usr/local/kong/.kong_envYYYY/MM/DD HH:mm:ss [verbose] prefix in use: /usr/local/kongYYYY/MM/DD HH:mm:ss [info] nginx.......runningYYYY/MM/DD HH:mm:ss [info]YYYY/MM/DD HH:mm:ss [info] Kong is healthy at /usr/local/kongKong启动1kong start -c /etc/kong/kong.conf --v输出示例1234567891011YYYY/MM/DD HH:mm:ss [verbose] Kong: 1.1.2YYYY/MM/DD HH:mm:ss [verbose] reading config file at /etc/kong/kong.confYYYY/MM/DD HH:mm:ss [verbose] prefix in use: /usr/local/kongYYYY/MM/DD HH:mm:ss [verbose] retrieving database schema state...YYYY/MM/DD HH:mm:ss [verbose] schema state retrievedYYYY/MM/DD HH:mm:ss [verbose] preparing nginx prefix directory at /usr/local/kongYYYY/MM/DD HH:mm:ss [verbose] SSL enabled, no custom certificate set: using default certificateYYYY/MM/DD HH:mm:ss [verbose] default SSL certificate found at /usr/local/kong/ssl/kong-default.crtYYYY/MM/DD HH:mm:ss [verbose] Admin SSL enabled, no custom certificate set: using default certificateYYYY/MM/DD HH:mm:ss [verbose] admin SSL certificate found at /usr/local/kong/ssl/admin-kong-default.crtYYYY/MM/DD HH:mm:ss [info] Kong startedKong关闭1kong stop --v输出示例12345YYYY/MM/DD HH:mm:ss [verbose] Kong: 1.1.2YYYY/MM/DD HH:mm:ss [verbose] reading config file at /usr/local/kong/.kong_envYYYY/MM/DD HH:mm:ss [verbose] prefix in use: /usr/local/kongYYYY/MM/DD HH:mm:ss [verbose] sending TERM signal to nginx running at /usr/local/kong/pids/nginx.pidYYYY/MM/DD HH:mm:ss [info] Kong stoppedKong重启1kong restart -c /etc/kong/kong.conf --v输出示例123456789101112YYYY/MM/DD HH:mm:ss [verbose] Kong: 1.1.2YYYY/MM/DD HH:mm:ss [info] Kong stoppedYYYY/MM/DD HH:mm:ss [verbose] reading config file at /etc/kong/kong.confYYYY/MM/DD HH:mm:ss [verbose] prefix in use: /usr/local/kongYYYY/MM/DD HH:mm:ss [verbose] retrieving database schema state...YYYY/MM/DD HH:mm:ss [verbose] schema state retrievedYYYY/MM/DD HH:mm:ss [verbose] preparing nginx prefix directory at /usr/local/kongYYYY/MM/DD HH:mm:ss [verbose] SSL enabled, no custom certificate set: using default certificateYYYY/MM/DD HH:mm:ss [verbose] default SSL certificate found at /usr/local/kong/ssl/kong-default.crtYYYY/MM/DD HH:mm:ss [verbose] Admin SSL enabled, no custom certificate set: using default certificateYYYY/MM/DD HH:mm:ss [verbose] admin SSL certificate found at /usr/local/kong/ssl/admin-kong-default.crtYYYY/MM/DD HH:mm:ss [info] Kong startedKong重载配置1kong reload -c /etc/kong/kong.conf --v输出示例12345YYYY/MM/DD HH:mm:ss [verbose] Kong: 1.1.2YYYY/MM/DD HH:mm:ss [verbose] reading config file at /usr/local/kong/.kong_envYYYY/MM/DD HH:mm:ss [verbose] prefix in use: /usr/local/kongYYYY/MM/DD HH:mm:ss [verbose] preparing nginx prefix directory at /usr/local/kongYYYY/MM/DD HH:mm:ss [info] Kong reloaded日志轮转底层是Nginx，因此可以借助logrotate工具做日志轮转安装logrotate1yum install -y logrotate配置logrotate每天切割日志保留30天的日志记录被切割的日志会压缩打包/etc/logrotate.d/kong123456789101112/var/log/kong/*.log &#123;dailyrotate 30missingokdateextcompressdelaycompressnotifemptypostrotate [ -e /usr/local/kong/pids/nginx.pid ] &amp;&amp; kill -USR1 `cat /usr/local/kong/pids/nginx.pid`endscript&#125;测试logrotate配置1logrotate -d /etc/logrotate.d/kong输出示例123456789101112131415reading config file /etc/logrotate.d/kongAllocating hash table for state file, size 15360 BHandling 1 logsrotating pattern: /var/log/kong/*.log after 1 days (30 rotations)empty log files are not rotated, old logs are removedconsidering log /var/log/kong/access.log log does not need rotating (log has been already rotated)considering log /var/log/kong/admin_access.log log does not need rotating (log has been already rotated)considering log /var/log/kong/admin_error.log log does not need rotating (log has been already rotated)considering log /var/log/kong/error.log log does not need rotating (log has been already rotated)kong监控kong内置了prometheus标准的metrics，默认只有Nginx的信息开启了prometheus插件之后，可以为每个services都做监控注意！metrics数据默认只能从Admin端口获取，不能走Proxy端口因此想让Prometheus获取到kong的metric数据，需要修改kong.conf配置文件中admin_listen的值具体的可以看官方说明开启插件1curl -X POST http://localhost:8001/plugins --data \"name=prometheus\"访问metric数据1curl -X GET http://localhost:8001/metrics输出示例这里kong是没配置任何服务，因此输出内容很少123456789101112131415# HELP kong_datastore_reachable Datastore reachable from Kong, 0 is unreachable# TYPE kong_datastore_reachable gaugekong_datastore_reachable 1# HELP kong_nginx_http_current_connections Number of HTTP connections# TYPE kong_nginx_http_current_connections gaugekong_nginx_http_current_connections&#123;state=\"accepted\"&#125; 15kong_nginx_http_current_connections&#123;state=\"active\"&#125; 1kong_nginx_http_current_connections&#123;state=\"handled\"&#125; 15kong_nginx_http_current_connections&#123;state=\"reading\"&#125; 0kong_nginx_http_current_connections&#123;state=\"total\"&#125; 15kong_nginx_http_current_connections&#123;state=\"waiting\"&#125; 0kong_nginx_http_current_connections&#123;state=\"writing\"&#125; 1# HELP kong_nginx_metric_errors_total Number of nginx-lua-prometheus errors# TYPE kong_nginx_metric_errors_total counterkong_nginx_metric_errors_total 0监控数据可视化官方已经制作了grafana dashboard来展示Prometheus的数据Grafana Dashboard","categories":[{"name":"Kong","slug":"Kong","permalink":"https://luanlengli.github.io/categories/Kong/"}],"tags":[{"name":"Kong","slug":"Kong","permalink":"https://luanlengli.github.io/tags/Kong/"}]},{"title":"【施工中】PostgreSQL备份和恢复","slug":"PostgreSQL备份和恢复","date":"2019-05-28T15:32:05.000Z","updated":"2019-07-22T14:12:51.000Z","comments":true,"path":"2019/05/28/PostgreSQL备份和恢复.html","link":"","permalink":"https://luanlengli.github.io/2019/05/28/PostgreSQL备份和恢复.html","excerpt":"","text":"说明基于PostgreSQL 10.8版本做演示其他版本自行调整逻辑备份pg_dump备份的逻辑pg_dump的一次完整的备份是在一个事务中完成的, 事务隔离级别为serializable 或者 repeatable read。pg_dump在备份数据开始前, 需要对进行备份的对象加ACCESS SHARE锁。为了防止无休止的锁等待，可以使用--lock-wait-timeout设置锁超时时间一切准备就绪后，pg_dump开始备份数据备份的格式p或者plain默认格式，可读写的文本文件，里面是SQL语句c或者custom自定义格式，默认开启数据压缩，还原时可以调整对象还原顺序，必须使用pg_restore命令还原d或者directory目录归档格式，需要用pg_restore命令还原，目录归档格式下会创建一个目录, 然后每个表或者每个大对象对应一个备份输出文件，加上TOC文件名描述备份的详细信息, 这个格式默认支持压缩, 同时支持并行导出t或者tartar归档格式, 不支持压缩, 同时限制每个表最大不能超过8GB, 同样需要使用pg_restore还原全库一致性指定是集群中的单个数据库的一致性备份。因为备份不同的数据库需要切换连接，无法在不同的数据库之间共享snapshot，因此只能单库一致。非全库一致性备份如果单个库数据量越大，pg_dump备份时间就越长，持有锁的时间也就越长。这时候会阻塞DDL语句。未解决这个问题，可以将pg_dump的粒度缩小，只备份相关联且有一致性需求的数据表pg_dumpallpg_dumpall最主要的是用于备份全局数据, 例如表空间的DDL, 创建用户的DDL简单的备份脚本123456789101112131415161718192021222324252627#!/bin/bashset -ex# 定义PostgreSQL数据库连接export PGHOST='192.168.1.1'export PGPORT='5432'export PGUSER='postgres'export PGPASSWORD='postgres_passwd'# 定义目录BASE_DIR='/data/pgsql-dump'# 定义文件名FILENAME='pgsql-dump'# 定义时间格式，20190329-00DATE=$(date +%Y%m%d)TIME=$(date +%H%M)# 根据时间格式创建文件夹BACKUP_DIR=\"$&#123;BASE_DIR&#125;/$&#123;DATE&#125;/$&#123;TIME&#125;\"mkdir -p $&#123;BACKUP_DIR&#125;# 过滤需要备份的数据库DATABASES=$(psql --host=$&#123;PGHOST&#125; --port=$&#123;PGPORT&#125; --username=$&#123;PGUSER&#125; -c \"\\l\" | awk '&#123;print$1&#125;' | xargs)# 对每个数据库单独做逻辑备份for DB in $DATABASES;do pg_dump --host=$&#123;PGHOST&#125; --port=$&#123;PGPORT&#125; --username=$&#123;PGUSER&#125; --verbose --clean --create $&#123;DB&#125; | gzip -c &gt; $&#123;BACKUP_DIR&#125;/$&#123;FILENAME&#125;_$&#123;DB&#125;.gzdone# 备份整个数据库pg_dumpall --host=$&#123;PGHOST&#125; --port=$&#123;PGPORT&#125; --username=$&#123;PGUSER&#125; --verbose --clean | gzip -c &gt; $&#123;BACKUP_DIR&#125;/$&#123;FILENAME&#125;_all.gz# 查找备份目录修改时间大于30天的文件并清理find $&#123;BASE_DIR&#125;/ -mtime +30 -exec rm -rf &#123;&#125; \\;物理备份在线热备份pg_basebackupPostgreSQL自带的一个远程热备工具，可以将远程PostgreSQL热备到本地目录工作流程连接到一个远程PostgreSQL执行pg_start_backup将整个数据目录传输到本地执行pg_stop_backup命令命令示例使用postgres用户将192.168.1.1:5432PostgreSQL的数据备份到/path/to/data_dir目录1pg_basebackup -h 192.168.1.1 -p 5432 -U postgres -D /path/to/data_dirpg_rmanPostgreSQL的备份与恢复工具，支持全量、增量、归档三种备方式，支持数据压缩与备份集管理。pg_rman跑的不是流复制协议，而是文件拷贝，所以pg_rman必须与数据库服务器跑在同一台机器。具体用法可以看德哥的文档项目首页工作流程连接到本地PostgreSQL执行pg_start_backup全量备份文件或者通过比较数据文件块的lsn号进行增量备份执行pg_stop_backup命令备份归档日志前提条件开启归档archive_mode = onarchive_command = &#39;cp %p /data/pg-archlog/%f&#39;配置csvloglog_destination = csvloglog_directory = pg_log恢复","categories":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://luanlengli.github.io/categories/PostgreSQL/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://luanlengli.github.io/tags/PostgreSQL/"}]},{"title":"PostgreSQL10搭建时间序列数据库TimescaleDB","slug":"PostgreSQL10搭建时间序列数据库TimescaleDB","date":"2019-05-25T06:05:38.000Z","updated":"2019-05-29T03:33:40.000Z","comments":true,"path":"2019/05/25/PostgreSQL10搭建时间序列数据库TimescaleDB.html","link":"","permalink":"https://luanlengli.github.io/2019/05/25/PostgreSQL10搭建时间序列数据库TimescaleDB.html","excerpt":"","text":"说明这里只记录搭建和简单测试过程不保证ctrl+c和ctrl+v能完整跑起来操作系统使用的CentOS-7.6.1810 x86_64PostgreSQL版本号10.8TimescaleDB版本号1.3.0虚拟机配置1CPU 2G内存 20G系统盘postgres用户默认情况下PGDATA变量是/var/lib/pgsql/10/data这里没有使用数据盘，有需要的可以自行调整！TimescaleDB简介这段介绍是来自德哥的Github文档TimescaleDB介绍TimescaleDB是基于PostgreSQL数据库打造的一款时序数据库，插件化的形式，随着PostgreSQL的版本升级而升级。架构图环境准备安装PostgreSQL这个参考PostgreSQL10安装部署和初始化TimescaleDB安装TimescaleDB的YUM包已经被集成到PostgreSQL社区源里面，所以直接装就是了安装软件包1yum install -y timescaledb_10配置PostgreSQL切换到postgres用户1su - postgres在$PGDATA/postgresql.conf添加配置1shared_preload_libraries = 'timescaledb'重启PostgreSQL切换到postgres用户12su - postgrespg_ctl restart -D $PGDATA验证TimescaleDB功能登录PostgreSQL1psql -U postgres创建名为tutorial的数据库1CREATE database tutorial;切换到tutorial数据库1\\c tutorial加载TimescaleDB的extensions1CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;示例输出12345678910111213141516171819WARNING:WELCOME TO _____ _ _ ____________|_ _(_) | | | _ \\ ___ \\ | | _ _ __ ___ ___ ___ ___ __ _| | ___| | | | |_/ / | | | | _ ` _ \\ / _ \\/ __|/ __/ _` | |/ _ \\ | | | ___ \\ | | | | | | | | | __/\\__ \\ (_| (_| | | __/ |/ /| |_/ / |_| |_|_| |_| |_|\\___||___/\\___\\__,_|_|\\___|___/ \\____/ Running version 1.3.0For more information on TimescaleDB, please visit the following links: 1. Getting started: https://docs.timescale.com/getting-started 2. API reference documentation: https://docs.timescale.com/api 3. How TimescaleDB is designed: https://docs.timescale.com/introduction/architectureNote: TimescaleDB collects anonymous reports to better understand and assist our users.For more information and how to disable, please see our docs https://docs.timescaledb.com/using-timescaledb/telemetry.CREATE EXTENSION创建一个普通SQL标准的表123456CREATE TABLE conditions ( time TIMESTAMPTZ NOT NULL, location TEXT NOT NULL, temperature DOUBLE PRECISION NULL, humidity DOUBLE PRECISION NULL);查看表结构12345678tutorial=# \\d conditions Table \"public.conditions\" Column | Type | Collation | Nullable | Default-------------+--------------------------+-----------+----------+--------- time | timestamp with time zone | | not null | location | text | | not null | temperature | double precision | | | humidity | double precision | | |使用create_hypertable创建hypertable12345SELECT create_hypertable('conditions', 'time'); create_hypertable------------------------- (1,public,conditions,t)(1 row)这时再看表结构的时候，会发现不一样了123456789101112tutorial=# \\d conditions Table \"public.conditions\" Column | Type | Collation | Nullable | Default-------------+--------------------------+-----------+----------+--------- time | timestamp with time zone | | not null | location | text | | not null | temperature | double precision | | | humidity | double precision | | |Indexes: \"conditions_time_idx\" btree (\"time\" DESC)Triggers: ts_insert_blocker BEFORE INSERT ON conditions FOR EACH ROW EXECUTE PROCEDURE _timescaledb_internal.insert_blocker()插入数据1tutorial=# INSERT INTO conditions(time, location, temperature, humidity) VALUES (NOW(), 'office', 70.0, 50.0);查询数据12345tutorial=# SELECT * FROM conditions ORDER BY time DESC LIMIT 100; time | location | temperature | humidity-------------------------------+----------+-------------+---------- YYYY-MM-DD HH:mm:SS.354351+08 | office | 70 | 50(1 row)TimescaleDB Tutorials这里使用TimescaleDB官方的测试样例地址在这里纽约TAXI数据透视分析测试数据这里直接下载的同时解压压缩包1wget -O - https://timescaledata.blob.core.windows.net/datasets/nyc_data.tar.gz | tar xz解压出来有三个文件nyc_data_contagg.sqlnyc_data.sqlnyc_data_rides.csv创建数据库123CREATE DATABASE nyc_data;\\c nyc_dataCREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;创建表结构1psql -U postgres -d nyc_data -h localhost &lt; nyc_data.sql导入数据1psql -U postgres -d nyc_data -h localhost -c \"\\COPY rides FROM nyc_data_rides.csv CSV\"查看表12345678nyc_data=# \\dt List of relations Schema | Name | Type | Owner--------+---------------+-------+---------- public | payment_types | table | postgres public | rates | table | postgres public | rides | table | postgres(3 rows)查看表结构payments123456nyc_data=# \\d payment_types Table \"public.payment_types\" Column | Type | Collation | Nullable | Default--------------+---------+-----------+----------+--------- payment_type | integer | | | description | text | | |rates123456nyc_data=# \\d rates Table \"public.rates\" Column | Type | Collation | Nullable | Default-------------+---------+-----------+----------+--------- rate_code | integer | | | description | text | | |rides1234567891011121314151617181920212223242526272829303132333435363738394041postgres=# \\d+ rides; Table \"public.rides\" Column | Type | Collation | Nullable | Default | Storage | Stats target | Description-----------------------+-----------------------------+-----------+----------+---------+----------+--------------+------------- vendor_id | text | | | | extended | | pickup_datetime | timestamp without time zone | | not null | | plain | | dropoff_datetime | timestamp without time zone | | not null | | plain | | passenger_count | numeric | | | | main | | trip_distance | numeric | | | | main | | pickup_longitude | numeric | | | | main | | pickup_latitude | numeric | | | | main | | rate_code | integer | | | | plain | | dropoff_longitude | numeric | | | | main | | dropoff_latitude | numeric | | | | main | | payment_type | integer | | | | plain | | fare_amount | numeric | | | | main | | extra | numeric | | | | main | | mta_tax | numeric | | | | main | | tip_amount | numeric | | | | main | | tolls_amount | numeric | | | | main | | improvement_surcharge | numeric | | | | main | | total_amount | numeric | | | | main | |Indexes: \"rides_passenger_count_pickup_datetime_idx\" btree (passenger_count, pickup_datetime DESC) \"rides_pickup_datetime_vendor_id_idx\" btree (pickup_datetime DESC, vendor_id) \"rides_rate_code_pickup_datetime_idx\" btree (rate_code, pickup_datetime DESC) \"rides_vendor_id_pickup_datetime_idx\" btree (vendor_id, pickup_datetime DESC)Triggers: ts_insert_blocker BEFORE INSERT ON rides FOR EACH ROW EXECUTE PROCEDURE _timescaledb_internal.insert_blocker()Child tables: _timescaledb_internal._hyper_2_10_chunk, _timescaledb_internal._hyper_2_11_chunk, _timescaledb_internal._hyper_2_12_chunk, _timescaledb_internal._hyper_2_1_chunk, _timescaledb_internal._hyper_2_2_chunk, _timescaledb_internal._hyper_2_3_chunk, _timescaledb_internal._hyper_2_4_chunk, _timescaledb_internal._hyper_2_5_chunk, _timescaledb_internal._hyper_2_6_chunk, _timescaledb_internal._hyper_2_7_chunk, _timescaledb_internal._hyper_2_8_chunk, _timescaledb_internal._hyper_2_9_chunk查询数据普通查询1月1日至1月10日，同车超过两人，每天平均计费是多少1234567891011121314151617nyc_data=# SELECT date_trunc('day', pickup_datetime) as day, avg(fare_amount)nyc_data-# FROM ridesnyc_data-# WHERE passenger_count &gt; 1 AND (pickup_datetime &gt;= '2016-01-01' AND pickup_datetime &lt;= '2016-01-10')nyc_data-# GROUP BY day ORDER BY day; day | avg---------------------+--------------------- 2016-01-01 00:00:00 | 13.3990821679715529 2016-01-02 00:00:00 | 13.0224687415181399 2016-01-03 00:00:00 | 13.5382068607068607 2016-01-04 00:00:00 | 12.9618895561740149 2016-01-05 00:00:00 | 12.6614611935518309 2016-01-06 00:00:00 | 12.5775245695086098 2016-01-07 00:00:00 | 12.5868802584437019 2016-01-08 00:00:00 | 12.4288630909742120 2016-01-09 00:00:00 | 11.8049625897430078 2016-01-10 00:00:00 | 27.2500000000000000(10 rows)查询每天交易了多少笔，只显示头五个记录1234567891011nyc_data=# SELECT date_trunc('day', pickup_datetime) as day, COUNT(*) FROM ridesnyc_data-# GROUP BY day ORDER BY daynyc_data-# LIMIT 5; day | count---------------------+-------- 2016-01-01 00:00:00 | 345037 2016-01-02 00:00:00 | 312831 2016-01-03 00:00:00 | 302878 2016-01-04 00:00:00 | 316171 2016-01-05 00:00:00 | 343251(5 rows)使用TimescaleDB内置函数每5分钟间隔为一个BUCKET，输出每个间隔产生了多少笔订单1234567891011121314151617181920212223242526272829303132nyc_data=# SELECT time_bucket('5 minute', pickup_datetime) as five_min, count(*) nyc_data=# FROM rides nyc_data=# WHERE pickup_datetime &lt; '2016-01-01 02:00' nyc_data=# GROUP BY five_min ORDER BY five_min; five_min | count---------------------+------- 2016-01-01 00:00:00 | 703 2016-01-01 00:05:00 | 1482 2016-01-01 00:10:00 | 1959 2016-01-01 00:15:00 | 2200 2016-01-01 00:20:00 | 2285 2016-01-01 00:25:00 | 2291 2016-01-01 00:30:00 | 2349 2016-01-01 00:35:00 | 2328 2016-01-01 00:40:00 | 2440 2016-01-01 00:45:00 | 2372 2016-01-01 00:50:00 | 2388 2016-01-01 00:55:00 | 2473 2016-01-01 01:00:00 | 2395 2016-01-01 01:05:00 | 2510 2016-01-01 01:10:00 | 2412 2016-01-01 01:15:00 | 2482 2016-01-01 01:20:00 | 2428 2016-01-01 01:25:00 | 2433 2016-01-01 01:30:00 | 2337 2016-01-01 01:35:00 | 2366 2016-01-01 01:40:00 | 2325 2016-01-01 01:45:00 | 2257 2016-01-01 01:50:00 | 2316 2016-01-01 01:55:00 | 2250(24 rows)每个城市的TAXI交易量12345678910111213nyc_data=# SELECT rates.description, COUNT(vendor_id) as num_trips FROM ridesnyc_data-# JOIN rates on rides.rate_code = rates.rate_codenyc_data-# WHERE pickup_datetime &lt; '2016-01-08'nyc_data-# GROUP BY rates.description ORDER BY rates.description; description | num_trips-----------------------+----------- group ride | 17 JFK | 54832 Nassau or Westchester | 967 negotiated fare | 7193 Newark | 4126 standard rate | 2266401(6 rows)查看select执行计划1234567891011121314151617postgres=# EXPLAIN SELECT * FROM rides; QUERY PLAN---------------------------------------------------------------------------------- Append (cost=0.00..282354.03 rows=5963403 width=258) -&gt; Seq Scan on _hyper_2_1_chunk (cost=0.00..23485.78 rows=781178 width=113) -&gt; Seq Scan on _hyper_2_2_chunk (cost=0.00..36322.06 rows=1187506 width=115) -&gt; Seq Scan on _hyper_2_3_chunk (cost=0.00..32909.67 rows=1075467 width=116) -&gt; Seq Scan on _hyper_2_4_chunk (cost=0.00..15601.72 rows=518772 width=112) -&gt; Seq Scan on _hyper_2_5_chunk (cost=0.00..20396.28 rows=281328 width=472) -&gt; Seq Scan on _hyper_2_6_chunk (cost=0.00..41670.68 rows=574768 width=472) -&gt; Seq Scan on _hyper_2_7_chunk (cost=0.00..20921.76 rows=288576 width=472) -&gt; Seq Scan on _hyper_2_8_chunk (cost=0.00..42966.40 rows=592640 width=472) -&gt; Seq Scan on _hyper_2_9_chunk (cost=0.00..15160.04 rows=209104 width=472) -&gt; Seq Scan on _hyper_2_10_chunk (cost=0.00..32896.44 rows=453744 width=472) -&gt; Seq Scan on _hyper_2_11_chunk (cost=0.00..11.60 rows=160 width=472) -&gt; Seq Scan on _hyper_2_12_chunk (cost=0.00..11.60 rows=160 width=472)(13 rows)配合PostGIS插件实现时间+空间数据库安装PostGIS1yum install -y postgis25_10加载PostGIS12postgres=# create extension postgis;CREATE EXTENSION修改表结构12345678postgres=# ALTER TABLE rides ADD COLUMN pickup_geom geometry(POINT,2163);ALTER TABLEpostgres=# ALTER TABLE rides ADD COLUMN dropoff_geom geometry(POINT,2163);ALTER TABLEpostgres=# UPDATE rides SET pickup_geom = ST_Transform(ST_SetSRID(ST_MakePoint(pickup_longitude,pickup_latitude),4326),2163);UPDATE 10906860postgres=# UPDATE rides SET dropoff_geom = ST_Transform(ST_SetSRID(ST_MakePoint(dropoff_longitude,dropoff_latitude),4326),2163);UPDATE 10906860查询数据查询在(lat, long) (40.7589,-73.9851)附近400米范围内，每30分钟有多少辆的士被乘坐123456789101112131415161718192021222324252627282930313233343536postgres=# SELECT time_bucket('30 minutes', pickup_datetime) AS thirty_min, COUNT(*) AS near_times_sqpostgres-# FROM ridespostgres-# WHERE ST_Distance(pickup_geom, ST_Transform(ST_SetSRID(ST_MakePoint(-73.9851,40.7589),4326),2163)) &lt; 400postgres-# AND pickup_datetime &lt; '2016-01-01 14:00'postgres-# GROUP BY thirty_min ORDER BY thirty_min; thirty_min | near_times_sq---------------------+--------------- 2016-01-01 00:00:00 | 74 2016-01-01 00:30:00 | 102 2016-01-01 01:00:00 | 120 2016-01-01 01:30:00 | 98 2016-01-01 02:00:00 | 112 2016-01-01 02:30:00 | 109 2016-01-01 03:00:00 | 163 2016-01-01 03:30:00 | 181 2016-01-01 04:00:00 | 214 2016-01-01 04:30:00 | 185 2016-01-01 05:00:00 | 158 2016-01-01 05:30:00 | 113 2016-01-01 06:00:00 | 102 2016-01-01 06:30:00 | 91 2016-01-01 07:00:00 | 88 2016-01-01 07:30:00 | 58 2016-01-01 08:00:00 | 72 2016-01-01 08:30:00 | 94 2016-01-01 09:00:00 | 115 2016-01-01 09:30:00 | 118 2016-01-01 10:00:00 | 135 2016-01-01 10:30:00 | 160 2016-01-01 11:00:00 | 212 2016-01-01 11:30:00 | 229 2016-01-01 12:00:00 | 244 2016-01-01 12:30:00 | 230 2016-01-01 13:00:00 | 235 2016-01-01 13:30:00 | 238(28 rows)TimescaleDB集群由于TimescaleDB是PostgreSQL的一个插件，因此可以借助PostgreSQL自身的特性实现集群功能基于流复制实现主从数据同步基于PGPool实现读写分离+主从自动切换基于patroni实现高可用集群基于PostgreSQL-X2实现多主多读+横向扩展的分布式集群","categories":[],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://luanlengli.github.io/tags/PostgreSQL/"}]},{"title":"PostgreSQL10流复制配合PGPool-II实现数据库HA主备切换","slug":"PostgreSQL10流复制-PGPool-II实现数据库HA主备切换","date":"2019-05-24T03:01:59.000Z","updated":"2019-05-27T03:23:00.000Z","comments":true,"path":"2019/05/24/PostgreSQL10流复制-PGPool-II实现数据库HA主备切换.html","link":"","permalink":"https://luanlengli.github.io/2019/05/24/PostgreSQL10流复制-PGPool-II实现数据库HA主备切换.html","excerpt":"","text":"说明这里只记录搭建和简单测试过程不保证ctrl+c和ctrl+v能完整跑起来操作系统使用的CentOS-7.6.1810 x86_64PostgreSQL版本号10.8PGPool版本号4.0.5虚拟机配置1CPU 2G内存 20G系统盘postgres用户默认情况下PGDATA变量是/var/lib/pgsql/10/data这里没有使用数据盘，有需要的可以自行调整！由watchdog通过ARP协议提供的VIP切换来保证服务可用性，这里不涉及负载均衡！注意！在某些公有云环境不一定支持基于ARP协议做VIP架构图环境准备主机清单主机名IP地址角色监听端口vip172.16.80.200vippg1172.16.80.201master5432pg2172.16.80.202slave5432准备基于stream的主从集群这里可以看这个链接PostgreSQL10基于stream复制搭建主从集群修改hosts文件把每个服务器的主机IP地址做静态解析123172.16.80.200 vip172.16.80.201 pg1172.16.80.202 pg2配置系统命令权限切换脚本需要使用root权限运行1chmod u+s /usr/sbin/ip /usr/sbin/arping配置SSH密钥修改postgres用户的密码1passwd postgres切换到postgres用户1su - postgres生成SSH密钥1ssh-keygen -t ecdsa -b 521 -N '' -f ~/.ssh/id_ecdsa配置SSH免密登录12ssh-copy-id pg1ssh-copy-id pg2PGPool创建PGPool健康检查用户12create user pgpool_check with password 'pgpool_password';grant all privileges on database postgres to pgpool_check;安装PGPoolPGPool在PostgreSQL社区提供的YUM源里面有，直接装就是了1yum install -y pgpool-II-10-4.0.5-1.rhel7 pgpool-II-10-extensions-4.0.5-1.rhel7PGPool也提供了WebUI方便管理，按需安装1yum install -y pgpoolAdmin配置PGPool配置文件的目录在/etc/pgpool-II-10配置pcp.confpcp.conf是配置pgpool-II自己的用户名和密码使用pg_md5命令加密密码1pg_md5 pgpool_password输出示例14aa0cb9673e84b06d4c8a848c80eb5d0添加到pcp.conf1postgres:4aa0cb9673e84b06d4c8a848c80eb5d0配置pool_hba.confpool_hba.conf跟PostgreSQL里面的pg_hba.conf作用一样，可以拷贝pg_hba.conf的内容过来123456# TYPE DATABASE USER ADDRESS METHOD# 默认配置数据库主机以socket、127.0.0.1、::1的方式连接数据库可以跳过认证阶段直接登录数据库local all all trusthost all all 127.0.0.1/32 trusthost all all ::1/128 trusthost all all 172.16.80.0/24 md5配置pgpool.confpgpool.conf可以参考/etc/pgpool-II-10/pgpool.conf.sample的配置需要注意的几个点！master节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250# ----------------------------# pgPool-II configuration file# ----------------------------#------------------------------------------------------------------------------# CONNECTIONS#------------------------------------------------------------------------------listen_addresses = '*'port = 9999socket_dir = '/tmp'listen_backlog_multiplier = 2serialize_accept = offpcp_listen_addresses = '*'pcp_port = 9898pcp_socket_dir = '/tmp'# - Backend Connection Settings -backend_hostname0 = 'pg1'backend_port0 = 5432# load_balance_mode为off时不生效backend_weight0 = 1backend_data_directory0 = '/var/lib/pgsql/10/data'backend_flag0 = 'ALLOW_TO_FAILOVER'backend_hostname1 = 'pg2'backend_port1 = 5432# load_balance_mode为off时不生效backend_weight1 = 1backend_data_directory1 = '/var/lib/pgsql/10/data'backend_flag1 = 'ALLOW_TO_FAILOVER'# - Authentication -enable_pool_hba = on# 定义pool_hba.conf读取password文件pool_passwd = 'pool_passwd'authentication_timeout = 30allow_clear_text_frontend_auth = off# - SSL Connections -ssl = offssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL'ssl_prefer_server_ciphers = off#------------------------------------------------------------------------------# POOLS#------------------------------------------------------------------------------# - Concurrent session and pool size -# num_init_children * max_pool的值为PGPool可以接收多少客户端并发连接，计算方式# (max_pool * num_init_children) &lt; (max_connections - superuser_reserved_connections)num_init_children = 32max_pool = 4# - Life time -child_life_time = 600child_max_connections = 0connection_life_time = 0client_idle_limit = 0#------------------------------------------------------------------------------# LOGS#------------------------------------------------------------------------------# - Where to log -log_destination = 'stderr'# - What to log -log_line_prefix = '%t PID=%p: 'log_connections = onlog_hostname = onlog_statement = onlog_per_node_statement = onlog_client_messages = onlog_standby_delay = 'none'# - Syslog specific -syslog_facility = 'LOCAL0'syslog_ident = 'pgpool'# - Debug -log_error_verbosity = verbose#client_min_messages = notice#log_min_messages = warning#------------------------------------------------------------------------------# FILE LOCATIONS#------------------------------------------------------------------------------pid_file_name = '/var/run/pgpool-II-10/pgpool.pid'logdir = '/var/log/pgpool-II-10'#------------------------------------------------------------------------------# CONNECTION POOLING#------------------------------------------------------------------------------connection_cache = onreset_query_list = 'ABORT; DISCARD ALL'#reset_query_list = 'ABORT; RESET ALL; SET SESSION AUTHORIZATION DEFAULT'#------------------------------------------------------------------------------# REPLICATION MODE#------------------------------------------------------------------------------replication_mode = offreplicate_select = offinsert_lock = onlobj_lock_table = ''# - Degenerate handling -replication_stop_on_mismatch = offfailover_if_affected_tuples_mismatch = off#------------------------------------------------------------------------------# LOAD BALANCING MODE#------------------------------------------------------------------------------load_balance_mode = offignore_leading_white_space = onwhite_function_list = ''black_function_list = 'currval,lastval,nextval,setval'black_query_pattern_list = ''database_redirect_preference_list = ''app_name_redirect_preference_list = ''allow_sql_comments = offdisable_load_balance_on_write = 'transaction'#------------------------------------------------------------------------------# MASTER/SLAVE MODE#------------------------------------------------------------------------------master_slave_mode = onmaster_slave_sub_mode = 'stream'# - Streaming -sr_check_period = 3sr_check_user = 'repluser'sr_check_password = 'repluser_password'sr_check_database = 'postgres'delay_threshold = 'always'# - Special commands -follow_master_command = ''#------------------------------------------------------------------------------# HEALTH CHECK GLOBAL PARAMETERS#------------------------------------------------------------------------------health_check_period = 10health_check_timeout = 3health_check_user = 'pgpool_check'health_check_password = 'pgpool_password'health_check_database = 'postgres'health_check_max_retries = 3health_check_retry_delay = 1connect_timeout = 3#------------------------------------------------------------------------------# HEALTH CHECK PER NODE PARAMETERS (OPTIONAL)#------------------------------------------------------------------------------#health_check_period0 = 0#health_check_timeout0 = 20#health_check_user0 = 'nobody'#health_check_password0 = ''#health_check_database0 = ''#health_check_max_retries0 = 0#health_check_retry_delay0 = 1#connect_timeout0 = 10000#------------------------------------------------------------------------------# FAILOVER AND FAILBACK#------------------------------------------------------------------------------failover_command = '/etc/pgpool-II-10/failover_stream.sh -d %d -h %h -p %p -D %D -M %M -m %m -H %H -P %P -r %r -R %R &gt; /var/lib/pgsql/10/data/log/failover.log'failback_command = ''failover_on_backend_error = ondetach_false_primary = offsearch_primary_node_timeout = 60#------------------------------------------------------------------------------# ONLINE RECOVERY#------------------------------------------------------------------------------recovery_user = 'nobody'recovery_password = ''recovery_1st_stage_command = ''recovery_2nd_stage_command = ''recovery_timeout = 90client_idle_limit_in_recovery = 0#------------------------------------------------------------------------------# WATCHDOG#------------------------------------------------------------------------------# - Enabling -use_watchdog = on# -Connection to up stream servers -trusted_servers = ''ping_path = '/bin'# - Watchdog communication Settings -wd_hostname = 'pg1'wd_port = 9000wd_priority = 1wd_authkey = 'pgpool_watchdog'wd_ipc_socket_dir = '/tmp'# - Virtual IP control Setting -delegate_IP = '172.16.80.200'if_cmd_path = '/usr/sbin'if_up_cmd = 'ip addr add $_IP_$/24 dev ens33 label ens33:0'if_down_cmd = 'ip addr del $_IP_$/24 dev ens33'arping_path = '/usr/sbin'arping_cmd = 'arping -U $_IP_$ -w 1'# - Behaivor on escalation Setting -clear_memqcache_on_escalation = onwd_escalation_command = ''wd_de_escalation_command = ''# - Watchdog consensus settings for failover -failover_when_quorum_exists = onfailover_require_consensus = onallow_multiple_failover_requests_from_node = off# - Lifecheck Setting -# -- common --wd_monitoring_interfaces_list = 'ens33'wd_lifecheck_method = 'heartbeat'wd_interval = 3# -- heartbeat mode --wd_heartbeat_port = 9694wd_heartbeat_keepalive = 2wd_heartbeat_deadtime = 30heartbeat_destination0 = 'pg2'heartbeat_destination_port0 = 9694heartbeat_device0 = 'ens33'# -- query mode --wd_life_point = 3wd_lifecheck_query = 'SELECT 1'wd_lifecheck_dbname = 'template1'wd_lifecheck_user = 'nobody'wd_lifecheck_password = ''# - Other pgpool Connection Settings -other_pgpool_hostname0 = 'pg2'other_pgpool_port0 = 9999other_wd_port0 = 9000#------------------------------------------------------------------------------# OTHERS#------------------------------------------------------------------------------relcache_expire = 0relcache_size = 256check_temp_table = oncheck_unlogged_table = on#------------------------------------------------------------------------------# IN MEMORY QUERY MEMORY CACHE#------------------------------------------------------------------------------memory_cache_enabled = offmemqcache_method = 'shmem'memqcache_memcached_host = 'localhost'memqcache_memcached_port = 11211memqcache_total_size = 67108864memqcache_max_num_cache = 1000000memqcache_expire = 0memqcache_auto_cache_invalidation = onmemqcache_maxcache = 409600memqcache_cache_block_size = 1048576memqcache_oiddir = '/var/log/pgpool/oiddir'white_memqcache_table_list = ''black_memqcache_table_list = ''slave节点可以照抄master节点的配置注意以下几个地方要做对应变更123wd_hostname = 'pg2'heartbeat_destination0 = 'pg1'other_pgpool_hostname0 = 'pg1'生成pool_passwd文件在/etc/pgpool-II-10/pool_passwd添加连接到后端PostgreSQL数据库的用户密码会提示输入密码，这里的密码请填写PostgreSQL数据库用户对应的密码12pg_md5 -p -m -u postgres pool_passwdpg_md5 -p -m -u appuser pool_passwd修改权限1chmod a+r /etc/pgpool-II-10/pool_passwd创建切换脚本/etc/pgpool-II-10/failover_stream.sh12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/bashset -ex# 获取参数while getopts 'd:D:h:H:m:M:p:P:r:R:' OPT; do case $OPT in d) NODE_ID=\"$OPTARG\" ;; D) DATABASE_CLUSTER_PATH=\"$OPTARG\" ;; h) HOSTNAME=\"$OPTARG\" ;; H) NEW_MASTER_HOSTNAME=\"$OPTARG\" ;; m) NEW_MASTER_NODE_ID=\"$OPTARG\" ;; M) OLD_MASTER_NODE_ID=\"$OPTARG\" ;; p) PORT_NUM=\"$OPTARG\" ;; P) OLD_PRIMARY_NODE_ID=\"$OPTARG\" ;; r) NEW_MASTER_PORT_NUM=\"$OPTARG\" ;; R) NEW_MASTER_DATABASE_CLUSTER_PATH=\"$OPTARG\" ;; ?) echo \"Usage: `basename $0` -d NODE_ID -D DATABASE_CLUSTER_PATH -h HOSTNAME -H NEW_MASTER_NODE_HOSTNAME -m NEW_MASTER_NODE_ID -M OLD_MASTER_NODE_ID -p PORT_NUM -P OLD_PRIMARY_NODE_ID -r NEW_MASTER_PORT_NUM -R NEW_MASTER_DATABASE_PATH\" esacdone# 大型变量打印现场echo \"HOSTNAME: $&#123;HOSTNAME&#125;\"echo \"NODE_ID: $&#123;NODE_ID&#125;\"echo \"PORT_NUM: $&#123;PORT_NUM&#125;\"echo \"DATABASE_CLUSTER_PATH: $&#123;DATABASE_CLUSTER_PATH&#125;\"echo \"OLD_MASTER_NODE_ID: $&#123;OLD_MASTER_NODE_ID&#125;\"echo \"OLD_PRIMARY_NODE_ID: $&#123;OLD_PRIMARY_NODE_ID&#125;\"echo \"NEW_MASTER_HOSTNAME: $&#123;NEW_MASTER_HOSTNAME&#125;\"echo \"NEW_MASTER_NODE_ID: $&#123;NEW_MASTER_NODE_ID&#125;\"echo \"NEW_MASTER_PORT_NUM: $&#123;NEW_MASTER_PORT_NUM&#125;\"echo \"NEW_MASTER_DATABASE_CLUSTER_PATH: $&#123;NEW_MASTER_DATABASE_CLUSTER_PATH&#125;\"# 定义PostgreSQL家目录，默认是/usr/pgsql-10PGHOME='/usr/pgsql-10'# 定义PGDATA，默认是/var/lib/pgsql/10/data，这里使用pgpool.conf定义的backend_data_directoryPGDATA=$&#123;NEW_MASTER_DATABASE_CLUSTER_PATH&#125;# 拼凑字符串命令TRIGGER_COMMAND=\"$PGHOME/bin/pg_ctl promote -D $PGDATA\"# pg_ctl promote的功能是让备份服务器退出恢复进程并且转为读写模式/usr/bin/ssh -T $&#123;NEW_MASTER_HOSTNAME&#125; $&#123;TRIGGER_COMMAND&#125;exit 0;增加执行权限1chmod a+rx /etc/pgpool-II-10/failover_stream.sh启动PGPool12systemctl enable pgpool-II-10.servicesystemctl start pgpool-II-10.service验证PGPool使用psql连接PGPool1psql -U postgres -h vip -p 9999查看PGPool节点123456postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role | select_cnt | load_balance_node | replication_delay | last_status_change ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+--------------------- 0 | pg1 | 5432 | up | 0.500000 | primary | 2 | true | 0 | 2019-05-26 20:40:44 1 | pg2 | 5432 | up | 0.500000 | standby | 0 | false | 0 | 2019-05-26 20:40:44(2 rows)PGPool节点重启查看watchdog的VIP所在主机1ip address重启前查看PGPool节点信息1234567891011121314151617181920212223postgres=# show pool_nodes;-[ RECORD 1 ]------+--------------------node_id | 0hostname | pg1port | 5432status | uplb_weight | 0.500000role | primaryselect_cnt | 7load_balance_node | truereplication_delay | 0last_status_change | 2019-05-26 20:40:44-[ RECORD 2 ]------+--------------------node_id | 1hostname | pg2port | 5432status | uplb_weight | 0.500000role | standbyselect_cnt | 0load_balance_node | falsereplication_delay | 0last_status_change | 2019-05-26 20:40:44登录VIP所在主机直接重启1reboot再次查看节点信息可以看到VIP所在主机重启之后SQL连接会提示连接不可用在watchdog的作用下，VIP会自动切换到另一个PGPool节点SQL连接再次重新连接成功！12345678910111213141516171819202122232425262728postgres=# show pool_nodes;server closed the connection unexpectedly This probably means the server terminated abnormally before or while processing the request.The connection to the server was lost. Attempting reset: Succeeded.postgres=# show pool_nodes;-[ RECORD 1 ]------+--------------------node_id | 0hostname | pg1port | 5432status | uplb_weight | 0.500000role | primaryselect_cnt | 0load_balance_node | truereplication_delay | 0last_status_change | 2019-05-26 20:46:48-[ RECORD 2 ]------+--------------------node_id | 1hostname | pg2port | 5432status | downlb_weight | 0.500000role | standbyselect_cnt | 0load_balance_node | falsereplication_delay | 0last_status_change | 2019-05-26 20:46:42PGPool进程被杀查看watchdog的VIP所在主机1ip address查看PGPool节点信息12345678910111213141516171819202122232425postgres=# \\xExpanded display is on.postgres=# show pool_nodes;-[ RECORD 1 ]------+--------------------node_id | 0hostname | pg1port | 5432status | uplb_weight | 0.500000role | primaryselect_cnt | 0load_balance_node | truereplication_delay | 0last_status_change | 2019-05-26 20:46:48-[ RECORD 2 ]------+--------------------node_id | 1hostname | pg2port | 5432status | downlb_weight | 0.500000role | standbyselect_cnt | 0load_balance_node | falsereplication_delay | 0last_status_change | 2019-05-26 20:46:42找PGPool的进程123ps -ef | grep pgpool# 示例输出postgres 12641 1 0 20:39 ? 00:00:00 /usr/pgpool-10/bin/pgpool -f /etc/pgpool-II-10/pgpool.conf -n -D杀PGPool进程1kill -9 12641PGPool进程日志可以看到PGPool的watchdog几乎立刻就反应过来了1234567891011121314151617181920212223242526272829May 26 20:53:14 pg2 pgpool[9607]: LOG: watchdog node state changed from [STANDBY] to [JOINING]May 26 20:53:14 pg2 pgpool[9607]: LOCATION: watchdog.c:6360May 26 20:53:18 pg2 pgpool[9607]: LOG: watchdog node state changed from [JOINING] to [INITIALIZING]May 26 20:53:18 pg2 pgpool[9607]: LOCATION: watchdog.c:6360May 26 20:53:19 pg2 pgpool[9607]: LOG: I am the only alive node in the watchdog clusterMay 26 20:53:19 pg2 pgpool[9607]: HINT: skipping stand for coordinator stateMay 26 20:53:19 pg2 pgpool[9607]: LOCATION: watchdog.c:5231May 26 20:53:19 pg2 pgpool[9607]: LOG: watchdog node state changed from [INITIALIZING] to [MASTER]May 26 20:53:19 pg2 pgpool[9607]: LOCATION: watchdog.c:6360May 26 20:53:19 pg2 pgpool[9607]: LOG: I am announcing my self as master/coordinator watchdog nodeMay 26 20:53:19 pg2 pgpool[9607]: LOCATION: watchdog.c:5420May 26 20:53:23 pg2 pgpool[9607]: LOG: I am the cluster leader nodeMay 26 20:53:23 pg2 pgpool[9607]: DETAIL: our declare coordinator message is accepted by all nodesMay 26 20:53:23 pg2 pgpool[9607]: LOCATION: watchdog.c:5454May 26 20:53:23 pg2 pgpool[9607]: LOG: setting the local node \"pg2:9999 Linux pg2\" as watchdog cluster masterMay 26 20:53:23 pg2 pgpool[9607]: LOCATION: watchdog.c:7087May 26 20:53:23 pg2 pgpool[9607]: LOG: I am the cluster leader node. Starting escalation processMay 26 20:53:23 pg2 pgpool[9607]: LOCATION: watchdog.c:5473May 26 20:53:23 pg2 pgpool[9607]: LOG: escalation process started with PID:10118May 26 20:53:23 pg2 pgpool[9607]: LOCATION: watchdog.c:6000May 26 20:53:23 pg2 pgpool[9607]: LOG: new IPC connection receivedMay 26 20:53:23 pg2 pgpool[9607]: LOCATION: watchdog.c:3147May 26 20:53:23 pg2 pgpool[9607]: LOG: watchdog: escalation startedMay 26 20:53:23 pg2 pgpool[9607]: LOCATION: wd_escalation.c:93May 26 20:53:27 pg2 pgpool[9607]: LOG: successfully acquired the delegate IP:\"172.16.80.200\"May 26 20:53:27 pg2 pgpool[9607]: DETAIL: 'if_up_cmd' returned with successMay 26 20:53:27 pg2 pgpool[9607]: LOCATION: wd_if.c:169May 26 20:53:27 pg2 pgpool[9607]: LOG: watchdog escalation process with pid: 10118 exit with SUCCESS.May 26 20:53:27 pg2 pgpool[9607]: LOCATION: watchdog.c:2976模拟master节点宕机查看节点信息123456postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role | select_cnt | load_balance_node | replication_delay | last_status_change ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+--------------------- 0 | pg1 | 5432 | up | 0.500000 | primary | 0 | true | 0 | 2019-05-26 20:58:11 1 | pg2 | 5432 | up | 0.500000 | standby | 0 | false | 0 | 2019-05-26 20:59:22(2 rows)master节点直接关机1shutdown -h now查看PGPool日志1234567891011121314151617181920212223242526272829303132333435363738394041May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOG: new IPC connection receivedMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOCATION: watchdog.c:3147May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOG: watchdog received the failover command from local pgpool-II on IPC interfaceMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOCATION: watchdog.c:2570May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOG: watchdog is processing the failover command [DEGENERATE_BACKEND_REQUEST] received from local pgpool-II on IPC interfaceMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOCATION: watchdog.c:2491May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOG: we have got the consensus to perform the failoverMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: DETAIL: 1 node(s) voted in the favorMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOCATION: watchdog.c:2363May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10800 USER=[No Connection] DB=[No Connection]: LOG: Pgpool-II parent process has received failover requestMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10800 USER=[No Connection] DB=[No Connection]: LOCATION: pgpool_main.c:1594May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOG: new IPC connection receivedMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOCATION: watchdog.c:3147May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOG: received the failover indication from Pgpool-II on IPC interfaceMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOCATION: watchdog.c:2716May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOG: watchdog is informed of failover start by the main processMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10801 USER=[No Connection] DB=[No Connection]: LOCATION: watchdog.c:2784May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10800 USER=[No Connection] DB=[No Connection]: LOG: starting degeneration. shutdown host pg1(5432)May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10800 USER=[No Connection] DB=[No Connection]: LOCATION: pgpool_main.c:1873May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10800 USER=[No Connection] DB=[No Connection]: LOG: Restart all childrenMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10800 USER=[No Connection] DB=[No Connection]: LOCATION: pgpool_main.c:2023May 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10800 USER=[No Connection] DB=[No Connection]: LOG: execute command: /etc/pgpool-II-10/failover_stream.sh 0 pg1 5432 /var/lib/pgsql/10/data 0 1 pg2 0 5432 /var/lib/pgsql/10/data &gt; /var/lib/pgsql/10/data/log/failover.logMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10800 USER=[No Connection] DB=[No Connection]: LOCATION: pgpool_main.c:3070May 26 21:06:02 pg2 pgpool[10800]: + getopts d:D:h:H:m:M:p:P:r:R: OPTMay 26 21:06:02 pg2 pgpool[10800]: + echo 'HOSTNAME: pg2'May 26 21:06:02 pg2 pgpool[10800]: + echo 'NODE_ID: 'May 26 21:06:02 pg2 pgpool[10800]: + echo 'PORT_NUM: 'May 26 21:06:02 pg2 pgpool[10800]: + echo 'DATABASE_CLUSTER_PATH: 'May 26 21:06:02 pg2 pgpool[10800]: + echo 'OLD_MASTER_NODE_ID: 'May 26 21:06:02 pg2 pgpool[10800]: + echo 'OLD_PRIMARY_NODE_ID: 'May 26 21:06:02 pg2 pgpool[10800]: + echo 'NEW_MASTER_HOSTNAME: 'May 26 21:06:02 pg2 pgpool[10800]: + echo 'NEW_MASTER_NODE_ID: 'May 26 21:06:02 pg2 pgpool[10800]: + echo 'NEW_MASTER_PORT_NUM: 'May 26 21:06:02 pg2 pgpool[10800]: + echo 'NEW_MASTER_DATABASE_CLUSTER_PATH: 'May 26 21:06:02 pg2 pgpool[10800]: + PGHOME=/usr/pgsql-10May 26 21:06:02 pg2 pgpool[10800]: + PGDATA=May 26 21:06:02 pg2 pgpool[10800]: + TRIGGER_COMMAND='/usr/pgsql-10/bin/pg_ctl promote -D 'May 26 21:06:02 pg2 pgpool[10800]: + /usr/bin/ssh -T /usr/pgsql-10/bin/pg_ctl promote -DMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10871 USER=postgres DB=postgres: WARNING: failover/failback is in progressMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10871 USER=postgres DB=postgres: DETAIL: executing failover or failback on backendMay 26 21:06:02 pg2 pgpool[10800]: 2019-05-26 21:06:02 PID=10871 USER=postgres DB=postgres: HINT: In a moment you should be able to reconnect to the database查看PGPool节点可以看到PGPool帮我们自动将standby节点切换为primary123456postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role | select_cnt | load_balance_node | replication_delay | last_status_change ---------+----------+------+--------+-----------+---------+------------+-------------------+-------------------+--------------------- 0 | pg1 | 5432 | down | 0.500000 | standby | 0 | false | 0 | 2019-05-26 21:09:59 1 | pg2 | 5432 | up | 0.500000 | primary | 0 | true | 0 | 2019-05-26 21:12:56(2 rows)启动master节点在master登录到自己的PostgreSQL1psql -U postgres查看pg_is_in_recovery()发现master节点上面的PostgreSQL状态为f，没有自动转换为pg2的从库12345postgres=# select pg_is_in_recovery(); pg_is_in_recovery ------------------- f(1 row)手动处理一下切换用户12su - postgrescd $PGDATA创建recovery.conf文件12345678910# 指定timeline为latest，保证主从数据差异尽可能小recovery_target_timeline = 'latest'# 此选项会让PostgreSQL一直处于数据恢复状态# 不断去请求primary_conninfo定义的主节点的WAL日志# 并将这些WAL日志恢复到本地数据库中standby_mode = on# 定义主库的连接方式primary_conninfo = 'host=pg2 port=5432 user=repluser password=repluser_password'# 这个文件是用于触发停止流复制的操作！不需要手动创建trigger_file = '/var/lib/pgsql/10/data/pg.trigger'重启PostgreSQL1pg_ctl restart登录数据库查看状态，可以看到状态变成了t12345postgres=# select pg_is_in_recovery(); pg_is_in_recovery ------------------- t(1 row)手动加入PGPool集群这里输入之前pcp.conf定义的用户密码参数解析如下-h这里指定pcp的IP地址，这里用VIP-p这里指定pcp的端口，默认是9898-U这里指定登录pcp的用户，在/etc/pgpool-II-10/pcp.conf里面定义的-n这里指定节点ID，可以在show pool_nodes里面查到节点对应的ID号-d输出debug日志12345678pcp_attach_node -U postgres -h vip -p 9898 -n 0 -dPassword: DEBUG: recv: tos=\"m\", len=8DEBUG: recv: tos=\"r\", len=21DEBUG: send: tos=\"C\", len=6DEBUG: recv: tos=\"c\", len=20pcp_attach_node -- Command SuccessfulDEBUG: send: tos=\"X\", len=4查看PGPool节点状态，可以看到pg1状态是waiting，过一阵之后就变成up了123456postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role | select_cnt | load_balance_node | replication_delay | last_status_change ---------+----------+------+---------+-----------+---------+------------+-------------------+-------------------+--------------------- 0 | pg1 | 5432 | up | 0.500000 | standby | 0 | false | 4200 | 2019-05-26 21:19:05 1 | pg2 | 5432 | up | 0.500000 | primary | 0 | true | 0 | 2019-05-26 21:12:56(2 rows)维护操作PGPool节点down检查PostgreSQL是否正常启动查看$PGDATA目录是否有recovery.conf或者recovery.done，一般情况下，只存在一个文件其中recovery.done会让数据库启动时以主库形式启动recovery.conf会让数据库作为从库启动在发生主从切换的时候，recovery.conf会被重命名为recovery.done如确认当前down节点是要作为从库启动，则重命名recovery.done为recovery.conf，然后重启数据库需要确认一下是否能从新的主库同步数据PGPool节点waiting节点还在做主从同步PGPool还在创建子进程用于连接此节点，子进程数量多，耗时会相应变长如果登录很久都是waiting，可以尝试重启一下PGPool的服务PGPool重新添加节点确保主从数据库都已正常，但是节点状态还是down，就需要手工添加到集群中了使用pcp_attach_node命令，输入pcp.conf定义的密码之后即可将节点重新加入PGPool参数解析如下-h这里指定pcp的IP地址，这里用VIP-p这里指定pcp的端口，默认是9898-U这里指定登录pcp的用户，在/etc/pgpool-II-10/pcp.conf里面定义的-n这里指定节点ID，可以在show pool_nodes里面查到节点对应的ID号-d输出debug日志12345678pcp_attach_node -h vip -p 9898 -U postgres -n NODE_ID -dPassword:DEBUG: recv: tos=\"m\", len=8DEBUG: recv: tos=\"r\", len=21DEBUG: send: tos=\"C\", len=6DEBUG: recv: tos=\"c\", len=20pcp_attach_node -- Command SuccessfulDEBUG: send: tos=\"X\", len=4","categories":[],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://luanlengli.github.io/tags/PostgreSQL/"}]},{"title":"PostgreSQL10基于stream复制搭建主从集群","slug":"PostgreSQL10基于stream复制搭建主从集群","date":"2019-05-21T03:00:47.000Z","updated":"2019-06-03T12:42:52.000Z","comments":true,"path":"2019/05/21/PostgreSQL10基于stream复制搭建主从集群.html","link":"","permalink":"https://luanlengli.github.io/2019/05/21/PostgreSQL10基于stream复制搭建主从集群.html","excerpt":"","text":"说明这里只记录搭建和简单测试过程不保证ctrl+c和ctrl+v能完整跑起来操作系统使用的CentOS-7.6.1810 x86_64PostgreSQL版本号10.8虚拟机配置1CPU 2G内存 20G系统盘postgres用户默认情况下PGDATA变量是/var/lib/pgsql/10/data这里没有使用数据盘，有需要的可以自行调整！环境准备主机清单主机名IP地址角色监听端口pg1172.16.80.201master5432pg2172.16.80.202slave5432配置/etc/hosts12345cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1 localhost172.16.80.201 pg1172.16.80.202 pg2EOF安装PostgreSQL参照此链接在两个节点上面安装好PostgreSQL，但是先不初始化数据库！！！！PostgreSQL10安装部署和初始化基于stream主从复制配置master节点创建数据库如果是有业务在跑，已经有数据了就不要做这一步操作！直接从创建复制用户那一步开始1/usr/pgsql-10/bin/postgresql-10-setup initdb启动数据库1systemctl enable --now postgres-10.service登录数据库1psql -U postgres创建复制用户1CREATE USER repluser REPLICATION LOGIN CONNECTION LIMIT 2 ENCRYPTED PASSWORD 'repluser_password';创建.pgpass文件12345su - postgrescat &gt; ~/.pgpass &lt;&lt;EOFpg2:5432:replication:repluser:repluser_passwordEOFchmod 0600 ~/.pgpass编辑pg_hba.conf增加配置如下1host replication repluser 172.16.80.0/24 md5编辑postgres.conf增加配置参数如下12345678910wal_level = replica# 有多少个slave节点，max_wal_senders就设置为多少max_wal_senders = 1# wal_keep_segments可以设置大一点，每个segments大小16MB# 这样在备库不会因为wal receiver启动太慢导致所需wal在主库被删除wal_keep_segments = 64# 加上这个synchronous_standby_names可以开启同步复制# 从库不在线或者无法执行写操作时，本地执行完之后会卡住无法返回写入成功# 请自行斟酌之后再决定是否配置同步复制# synchronous_standby_names = 'pg2'重启数据库12su - postgresql/usr/pgsql-10/bin/pg_ctl restart -D $PGDATAslave节点创建.pgpass文件12345su - postgrescat &gt; ~/.pgpass &lt;&lt;EOFpg1:5432:replication:repluser:repluser_passwordEOFchmod 0600 ~/.pgpasspg_basebackup生成备库文件把master节点的数据以物理备份的方式，保存到slave节点。备份过程不影响其他客户端访问。备份出来的内容是整个数据库的所有数据，包括配置文件！-D指定PostgreSQL数据目录-Fp指定输出格式为plain，这里是缩写，相当于--format=plain-Xs指定传输WAL日志的方法为stream，这里是缩写，相当于--wal-method=stream-h指定PostgreSQL master节点-p指定端口-U指定用户12su - postgrespg_basebackup -D $PGDATA -Fp -Xs -v -P -h pg1 -p 5432 -U repluser配置recovery.conf从master节点拷贝过来的数据文件是没有recovery.conf的需要手动创建好在PostgreSQL安装的时候是有这个配置文件的模板里面都是注释12su - postgrescp /usr/pgsql-10/share/recovery.conf.sample $PGDATA/recovery.conf修改之后，内容如下12345678910# 指定timeline为latest，保证主从数据差异尽可能小recovery_target_timeline = 'latest'# 此选项会让PostgreSQL一直处于数据恢复状态# 不断去请求primary_conninfo定义的主节点的WAL日志# 并将这些WAL日志恢复到本地数据库中standby_mode = on# 定义主库的连接方式primary_conninfo = 'host=pg1 port=5432 user=repluser password=repluser_password application_name=pg2'# 这个文件是用于触发停止流复制的操作！不需要手动创建trigger_file = '/var/lib/pgsql/10/data/pg.trigger'启动备库1systemctl enable --now postgresql-10.service验证master节点查看进程1ps -ef | grep postgres查看是否有postgres: wal sender process查看recovery状态登录数据库1psql -U postgres -h pg1输入sql语句，主库返回f12345postgres=# select pg_is_in_recovery(); pg_is_in_recovery ------------------- f(1 row)查看主备同步状态登录数据库1psql -U postgres -h pg1修改一下输出格式，将每列数据以键值对的方式显示，类似于MySQL的\\G这里是输入一次，当前会话有效，想取消就再输入一次1\\x查看同步状态这里可以看到sync_state为async，即异步流复制123456789101112131415161718192021postgres=# select * from pg_stat_replication ;-[ RECORD 1 ]----+------------------------------pid | 8154usesysid | 16384usename | repluserapplication_name | pg2client_addr | 172.16.80.202client_hostname |client_port | 17024backend_start | YYYY-MM-DD HH:mm:ss.800271+08backend_xmin |state | streamingsent_lsn | 0/3000108write_lsn | 0/3000108flush_lsn | 0/3000108replay_lsn | 0/3000108write_lag |flush_lag |replay_lag |sync_priority | 1sync_state | asyncslave节点查看进程1ps -ef | grep postgres查看是否有postgres: wal receiver process查看recovery状态登录数据库1psql -U postgres -h pg2输入sql语句，从库返回t12345postgres=# select pg_is_in_recovery(); pg_is_in_recovery ------------------- t(1 row)","categories":[],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://luanlengli.github.io/tags/PostgreSQL/"}]},{"title":"PostgreSQL10安装部署和初始化","slug":"PostgreSQL10安装部署和初始化","date":"2019-05-20T03:03:26.000Z","updated":"2019-05-29T13:47:14.000Z","comments":true,"path":"2019/05/20/PostgreSQL10安装部署和初始化.html","link":"","permalink":"https://luanlengli.github.io/2019/05/20/PostgreSQL10安装部署和初始化.html","excerpt":"","text":"说明仅记录安装部署和初始化过程操作系统使用的CentOS-7.6.1810 x86_64PostgreSQL版本号为10.8虚拟机配置1CPU 2G内存 20G系统盘 30G数据盘【根据自己的环境灵活调整，生产环境请务必提供独立数据盘！】环境准备关闭防火墙1systemctl disable --now firewalld.service关闭SELINUX12setenforce 0sed -e 's,^SELINUX=.*,SELINUX=disabled,' -i /etc/selinux/configsysctl参数12345678910111213141516171819202122232425262728293031323334cat &gt; /etc/sysctl.d/99-pgsql.conf &lt;&lt;EOF# 表示最大限度使用物理内存，物理内存用满再用swapvm.swappiness = 0# 端口最大的监听队列的长度，默认值128net.core.somaxconn=4096# 最大文件句柄数fs.file-max = 1048576# 最大文件打开数fs.nr_open = 1048576# 单个共享内存段的最大值，这里设置为物理内存大小的一半# 计算方式 2048 / 2 * 1024 * 1024 = 1073741824kernel.shmmax = 1073741824# kernel.sem对应四个值分别为SEMMSL、SEMMNS、SEMOPM、SEMMNI# SEMMSL 每个信号集的最大信号数量# SEMMNS 用于控制整个 Linux 系统中信号（而不是信号集）的最大数# SEMOPM 内核参数用于控制每个 semop 系统调用可以执行的信号操作的数量# SEMMNI 内核参数用于控制整个 Linux 系统中信号集的最大数量# 这里套用Oracle 11gR2的最小要求值kernel.sem = 250 32000 100 128# 最大的TCP数据接收窗口（字节）net.core.rmem_max = 4194304# 最大的TCP数据发送窗口（字节）net.core.wmem_max = 4194304# 默认的TCP数据接收窗口大小（字节）net.core.rmem_default = 262144# 默认的TCP数据发送窗口大小（字节）net.core.wmem_default = 262144# 配置主动发起连接时，端口范围，默认值32768-60000net.ipv4.ip_local_port_range = 9000 65535# 当系统pagecache脏页达到系统内存dirty_ratio的百分比值时，会阻塞新的写请求# 直到内存脏页落盘# 默认值为30vm.dirty_ratio = 80EOFlimits参数12345678910cat &gt; /etc/security/limits.d/99-pgsql.conf &lt;&lt;EOF* soft nofile 655360* hard nofile 655360* soft nproc 655360* hard nproc 655360* soft stack unlimited* hard stack unlimited* soft memlock 250000000* hard memlock 250000000EOF准备数据盘确认数据盘1234567lsblk# 输出示例NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 20G 0 disk├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 19G 0 part /sdb 8:16 0 30G 0 disk分区使用fdisk /dev/sdb格式化硬盘，默认已做4K对齐1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950fdisk /dev/sdb# 输出示例Welcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.Device does not contain a recognized partition tableBuilding a new DOS disklabel with disk identifier 0xa5fd5a2b.Command (m for help): pDisk /dev/sdb: 32.2 GB, 32212254720 bytes, 62914560 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xa5fd5a2b Device Boot Start End Blocks Id SystemCommand (m for help): nPartition type: p primary (0 primary, 0 extended, 4 free) e extendedSelect (default p): pPartition number (1-4, default 1): 1First sector (2048-62914559, default 2048):Using default value 2048Last sector, +sectors or +size&#123;K,M,G&#125; (2048-62914559, default 62914559):Using default value 62914559Partition 1 of type Linux and of size 30 GiB is setCommand (m for help): pDisk /dev/sdb: 32.2 GB, 32212254720 bytes, 62914560 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0xa5fd5a2b Device Boot Start End Blocks Id System/dev/sdb1 2048 62914559 31456256 83 LinuxCommand (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.12345678lsblk# 输出示例NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsda 8:0 0 20G 0 disk├─sda1 8:1 0 1G 0 part /boot└─sda2 8:2 0 19G 0 part /sdb 8:16 0 30G 0 disk└─sdb1 8:17 0 30G 0 part格式化分区1234567891011mkfs.xfs -f -b size=4096 /dev/sdb1# 输出示例meta-data=/dev/sdb1 isize=512 agcount=4, agsize=1966016 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0data = bsize=4096 blocks=7864064, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=3839, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0创建数据目录1mkdir -p /data挂载数据盘1mount -t xfs -o nolargeio,noatime,nodiratime /dev/sdb1 /data修改/etc/fstab追加一行记录1/dev/sdb1 /data xfs nolargeio,noatime,nodiratime 0 0安装PostgreSQL准备YUM源社区配置YUM源的文档1yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm由于默认的源服务器地址在国外，修改为国内镜像地址可以改善下载速度1yum install -y https://mirrors.tuna.tsinghua.edu.cn/postgresql/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm默认源服务器地址是PostgreSQL社区的服务器，这里替换成清华大学的镜像源地址1sed -e 's,download.postgresql.org/pub,mirrors4.tuna.tsinghua.edu.cn/postgresql,g' -i /etc/yum.repos.d/pgdg-redhat-all.repo安装PostgreSQL10必装1yum install -y postgresql10 postgresql10-server选装1yum install -y postgresql10-contrib postgresql10-testPostgreSQL也有基于WebUI的图形管理工具pgAdmin，具体怎么用，自己看文档windows版的看这里容器化部署看这里1yum install -y pgadmin4初始化数据库使用默认数据目录默认数据目录在/var/lib/pgsql/10/data1/usr/pgsql-10/bin/postgresql-10-setup initdb使用自定义数据目录这里自定义数据目录为/data1234567891011121314151617181920# 修改数据目录权限chown -R postgres:postgres /data# 修改服务启动脚本的数据目录# 服务脚本默认的Environment=PGDATA=/var/lib/pgsql/10/data# 不改这个会导致服务启动失败sed -e 's,^Environment=PGDATA=.*,Environment=PGDATA=/data,' -i /usr/lib/systemd/system/postgresql-10.servicesystemctl daemon-reload# 切换到postgres用户su - postgres# 修改默认PGDATA环境变量sed -e 's,^PGDATA.*,PGDATA=/data,' -i ~/.bash_profile# 应用变量source ~/.bash_profile# 初始化数据库/usr/pgsql-10/bin/initdb --encoding=UTF-8 \\ --local=en_US.UTF8 \\ --username=postgres \\ --pwprompt \\ --pgdata=$PGDATA \\ --data-checksums启动PostgreSQL12systemctl enable postgresql-10.servicesystemctl start postgresql-10.service验证数据库查看数据库进程12345678910ps -ef | grep postgres# 输出示例postgres 13558 1 0 13:25 ? 00:00:00 /usr/pgsql-10/bin/postmaster -D /datapostgres 13560 13558 0 13:25 ? 00:00:00 postgres: logger processpostgres 13562 13558 0 13:25 ? 00:00:00 postgres: checkpointer processpostgres 13563 13558 0 13:25 ? 00:00:00 postgres: writer processpostgres 13564 13558 0 13:25 ? 00:00:00 postgres: wal writer processpostgres 13565 13558 0 13:25 ? 00:00:00 postgres: autovacuum launcher processpostgres 13566 13558 0 13:25 ? 00:00:00 postgres: stats collector processpostgres 13567 13558 0 13:25 ? 00:00:00 postgres: bgworker: logical replication launcher查看数据库监听端口1234netstat -antupl | grep 5432# 输出示例tcp 0 0 127.0.0.1:5432 0.0.0.0:* LISTEN 13558/postmastertcp6 0 0 ::1:5432 :::* LISTEN 13558/postmaster切换用户1su - postgres登录数据库在postgres用户下可以直接免密码登录到数据库1psql -U postgres查看数据库12345678910111213postgres=# \\conninfoYou are connected to database \"postgres\" as user \"postgres\" via socket in \"/var/run/postgresql\" at port \"5432\".postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges-----------+----------+----------+------------+------------+----------------------- postgres | postgres | UTF8 | zh_CN.UTF8 | zh_CN.UTF8 | template0 | postgres | UTF8 | zh_CN.UTF8 | zh_CN.UTF8 | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | zh_CN.UTF8 | zh_CN.UTF8 | =c/postgres + | | | | | postgres=CTc/postgres(3 rows)配置PostgreSQL数据库配置文件路径配置文件是放在数据目录，懒得记路径可以这么做123456789su - postgresqlcd $PGDATAls# 输出示例base pg_dynshmem pg_notify pg_stat_tmp pg_wal postmaster.pidcurrent_logfiles pg_hba.conf pg_replslot pg_subtrans pg_xactglobal pg_ident.conf pg_serial pg_tblspc postgresql.auto.conflog pg_logical pg_snapshots pg_twophase postgresql.confpg_commit_ts pg_multixact pg_stat PG_VERSION postmaster.opts配置文件说明postgresql.conf数据库服务的配置文件默认配置文件条目非常多，可以根据注释来配置对应选项也可以查看在线版本这里简单做一下配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406# -----------------------------# PostgreSQL configuration file# -----------------------------## This file consists of lines of the form:## name = value## (The \"=\" is optional.) Whitespace may be used. Comments are introduced with# \"#\" anywhere on a line. The complete list of parameter names and allowed# values can be found in the PostgreSQL documentation.## The commented-out settings shown in this file represent the default values.# Re-commenting a setting is NOT sufficient to revert it to the default value;# you need to reload the server.## This file is read on server startup and when the server receives a SIGHUP# signal. If you edit the file on a running system, you have to SIGHUP the# server for the changes to take effect, run \"pg_ctl reload\", or execute# \"SELECT pg_reload_conf()\". Some parameters, which are marked below,# require a server shutdown and restart to take effect.## Any parameter can also be given as a command-line option to the server, e.g.,# \"postgres -c log_connections=on\". Some parameters can be changed at run time# with the \"SET\" SQL command.## Memory units: kB = kilobytes Time units: ms = milliseconds# MB = megabytes s = seconds# GB = gigabytes min = minutes# TB = terabytes h = hours# d = days#------------------------------------------------------------------------------# FILE LOCATIONS#------------------------------------------------------------------------------#data_directory = 'ConfigDir'#hba_file = 'ConfigDir/pg_hba.conf'#ident_file = 'ConfigDir/pg_ident.conf'#external_pid_file = ''#------------------------------------------------------------------------------# CONNECTIONS AND AUTHENTICATION#------------------------------------------------------------------------------# - Connection Settings -listen_addresses = '*'port = 5432# max_connections默认是100max_connections = 500superuser_reserved_connections = 3# - TCP settings -# 默认值为0，即使用系统默认值#tcp_keepalives_idle = 0#tcp_keepalives_interval = 0#tcp_keepalives_count = 0#tcp_user_timeout = 0# - Authentication -# 认证超时时间，默认1minauthentication_timeout = 30s# 密码加密算法，默认md5password_encryption = md5# GSSAPI using Kerberos#krb_server_keyfile = ''#krb_caseins_users = off# - SSL -ssl = off#ssl_ca_file = ''#ssl_cert_file = 'server.crt'#ssl_crl_file = ''#ssl_key_file = 'server.key'#ssl_ciphers = 'HIGH:MEDIUM:+3DES:!aNULL' # allowed SSL ciphers#ssl_prefer_server_ciphers = on#ssl_ecdh_curve = 'prime256v1'#ssl_min_protocol_version = 'TLSv1'#ssl_max_protocol_version = ''#ssl_dh_params_file = ''#ssl_passphrase_command = ''#ssl_passphrase_command_supports_reload = off#------------------------------------------------------------------------------# RESOURCE USAGE (except WAL)#------------------------------------------------------------------------------# - Memory -shared_buffers = 128MB#huge_pages = try#temp_buffers = 8MB#max_prepared_transactions = 0#work_mem = 4MB#maintenance_work_mem = 64MB#autovacuum_work_mem = -1#max_stack_depth = 2MB#shared_memory_type = mmapdynamic_shared_memory_type = posix# - Disk -#temp_file_limit = -1# - Kernel Resources -#max_files_per_process = 1000#shared_preload_libraries = ''# - Cost-Based Vacuum Delay -#vacuum_cost_delay = 0#vacuum_cost_page_hit = 1#vacuum_cost_page_miss = 10#vacuum_cost_page_dirty = 20#vacuum_cost_limit = 200# - Background Writer -#bgwriter_delay = 200ms#bgwriter_lru_maxpages = 100#bgwriter_lru_multiplier = 2.0#bgwriter_flush_after = 0# - Asynchronous Behavior -#effective_io_concurrency = 1#max_worker_processes = 8#max_parallel_maintenance_workers = 2#max_parallel_workers_per_gather = 2#parallel_leader_participation = on#max_parallel_workers = 8#old_snapshot_threshold = -1#backend_flush_after = 0#------------------------------------------------------------------------------# WRITE AHEAD LOG#------------------------------------------------------------------------------# - Settings -wal_level = replicafsync = onsynchronous_commit = onwal_sync_method = fdatasync#full_page_writes = on#wal_compression = off# 如需要用pg_rewind修复WAL的timeline, 需要打开wal_log_hints# 但是开启它会导致写wal变多, 请斟酌wal_log_hints = off#wal_init_zero = on#wal_recycle = on#wal_buffers = -1#wal_writer_delay = 200ms#wal_writer_flush_after = 1MB#commit_delay = 0#commit_siblings = 5# - Checkpoints -#checkpoint_timeout = 5minmax_wal_size = 1GBmin_wal_size = 80MB#checkpoint_completion_target = 0.5#checkpoint_flush_after = 0#checkpoint_warning = 30s# - Archiving -#archive_mode = off#archive_command = ''#archive_timeout = 0#------------------------------------------------------------------------------# REPLICATION#------------------------------------------------------------------------------# - Sending Server(s) -max_wal_senders = 10# wal_keep_segments可以设置大一点，每个segments大小16MB# 这样在备库不会因为wal receiver启动太慢导致所需wal在主库被删除wal_sender_timeout = 60s#max_replication_slots = 10#track_commit_timestamp = off# - Master Server -#synchronous_standby_names = ''#vacuum_defer_cleanup_age = 0# - Standby Servers -#hot_standby = on#max_standby_archive_delay = 30s#max_standby_streaming_delay = 30s#wal_receiver_status_interval = 10s#hot_standby_feedback = off#wal_receiver_timeout = 60ss#wal_retrieve_retry_interval = 5s# - Subscribers -#max_logical_replication_workers = 4#max_sync_workers_per_subscription = 2#------------------------------------------------------------------------------# QUERY TUNING#------------------------------------------------------------------------------# - Planner Method Configuration -#enable_bitmapscan = on#enable_hashagg = on#enable_hashjoin = on#enable_indexscan = on#enable_indexonlyscan = on#enable_material = on#enable_mergejoin = on#enable_nestloop = on#enable_seqscan = on#enable_sort = on#enable_tidscan = on# - Planner Cost Constants -#seq_page_cost = 1.0#random_page_cost = 4.0#cpu_tuple_cost = 0.01#cpu_index_tuple_cost = 0.005#cpu_operator_cost = 0.0025#parallel_tuple_cost = 0.1#parallel_setup_cost = 1000.0#min_parallel_table_scan_size = 8MB#min_parallel_index_scan_size = 512kB#effective_cache_size = 4GB# - Genetic Query Optimizer -#geqo = on#geqo_threshold = 12#geqo_effort = 5#geqo_pool_size = 0#geqo_generations = 0#geqo_selection_bias = 2.0#geqo_seed = 0.0# - Other Planner Options -#default_statistics_target = 100#constraint_exclusion = partition#cursor_tuple_fraction = 0.1#from_collapse_limit = 8#join_collapse_limit = 8#force_parallel_mode = off#------------------------------------------------------------------------------# ERROR REPORTING AND LOGGING#------------------------------------------------------------------------------# - Where to Log -log_destination = 'stderr'logging_collector = onlog_directory = 'log'log_filename = 'postgresql-%Y%m%d.log'#log_file_mode = 0600log_truncate_on_rotation = offlog_rotation_age = 1dlog_rotation_size = 0# These are relevant when logging to syslog:#syslog_facility = 'LOCAL0'#syslog_ident = 'postgres'#syslog_sequence_numbers = on#syslog_split_messages = on# - When to Log -#log_min_messages = warning #log_min_error_statement = errorlog_min_duration_statement = 0 # - What to Log -#debug_print_parse = off#debug_print_rewritten = off#debug_print_plan = off#debug_pretty_print = onlog_checkpoints = on log_connections = on log_disconnections = on log_duration = on log_error_verbosity = verbose#log_hostname = offlog_line_prefix = '%m [%p]: user=%u,db=%d 'log_lock_waits = on#log_statement = 'none'#log_replication_commands = off#log_temp_files = -1log_timezone = 'PRC'# - Process Title -#cluster_name = ''#update_process_title = on#------------------------------------------------------------------------------# RUNTIME STATISTICS#------------------------------------------------------------------------------# - Query/Index Statistics Collector -track_activities = ontrack_counts = ontrack_io_timing = ontrack_functions = nonetrack_activity_query_size = 1024stats_temp_directory = 'pg_stat_tmp'# - Statistics Monitoring -log_parser_stats = onlog_planner_stats = onlog_executor_stats = onlog_statement_stats = off#------------------------------------------------------------------------------# AUTOVACUUM PARAMETERS#------------------------------------------------------------------------------#autovacuum = on#log_autovacuum_min_duration = -1#autovacuum_max_workers = 3#autovacuum_naptime = 1min#autovacuum_vacuum_threshold = 50#autovacuum_analyze_threshold = 50#autovacuum_vacuum_scale_factor = 0.2#autovacuum_analyze_scale_factor = 0.1#autovacuum_freeze_max_age = 200000000#autovacuum_multixact_freeze_max_age = 400000000#autovacuum_vacuum_cost_delay = 20ms#autovacuum_vacuum_cost_limit = -1#------------------------------------------------------------------------------# CLIENT CONNECTION DEFAULTS#------------------------------------------------------------------------------# - Statement Behavior -#client_min_messages = notice#search_path = '\"$user\", public'#default_tablespace = ''#temp_tablespaces = ''#check_function_bodies = on#default_transaction_isolation = 'read committed'#default_transaction_read_only = off#default_transaction_deferrable = off#session_replication_role = 'origin'#statement_timeout = 0#lock_timeout = 0#idle_in_transaction_session_timeout = 0#vacuum_freeze_min_age = 50000000#vacuum_freeze_table_age = 150000000#vacuum_multixact_freeze_min_age = 5000000#vacuum_multixact_freeze_table_age = 150000000#bytea_output = 'hex'#xmlbinary = 'base64'#xmloption = 'content'#gin_fuzzy_search_limit = 0#gin_pending_list_limit = 4MB# - Locale and Formatting -datestyle = 'iso, ymd'#intervalstyle = 'postgres'timezone = 'PRC'#timezone_abbreviations = 'Default'#extra_float_digits = 0#client_encoding = sql_ascii# These settings are initialized by initdb, but they can be changed.lc_messages = 'en_US.UTF-8' # locale for system error message stringslc_monetary = 'en_US.UTF-8' # locale for monetary formattinglc_numeric = 'en_US.UTF-8' # locale for number formattinglc_time = 'en_US.UTF-8' # locale for time formatting# default configuration for text searchdefault_text_search_config = 'pg_catalog.english'# - Other Defaults -#dynamic_library_path = '$libdir'#local_preload_libraries = ''#session_preload_libraries = ''#------------------------------------------------------------------------------# LOCK MANAGEMENT#------------------------------------------------------------------------------#deadlock_timeout = 1s#max_locks_per_transaction = 64#max_pred_locks_per_transaction = 64#max_pred_locks_per_relation = -2#max_pred_locks_per_page = 2#------------------------------------------------------------------------------# VERSION/PLATFORM COMPATIBILITY#------------------------------------------------------------------------------# - Previous PostgreSQL Versions -#array_nulls = on#backslash_quote = safe_encoding#default_with_oids = off#escape_string_warning = on#lo_compat_privileges = off#operator_precedence_warning = off#quote_all_identifiers = off#standard_conforming_strings = on#synchronize_seqscans = on# - Other Platforms and Clients -#transform_null_equals = off#------------------------------------------------------------------------------# ERROR HANDLING#------------------------------------------------------------------------------#exit_on_error = off#restart_after_crash = on#data_sync_retry = off#------------------------------------------------------------------------------# CONFIG FILE INCLUDES#------------------------------------------------------------------------------#include_dir = ''#include_if_exists = ''#include = ''#------------------------------------------------------------------------------# CUSTOMIZED OPTIONS#------------------------------------------------------------------------------# Add settings for extensions herepg_hba.conf基于主机认证的配置文件默认的配置文件好多注释，看着不方便简化之后如下123456789# TYPE DATABASE USER ADDRESS METHOD# 默认配置数据库主机以socket、127.0.0.1、::1的方式连接数据库可以跳过认证阶段直接登录数据库local all all trusthost all all 127.0.0.1/32 trusthost all all ::1/128 trust# 配置app_user可以基于用户名密码+MD5加密算法从0.0.0.0/0访问数据库服务，并且是能访问所有databaseshost all all 0.0.0.0/0 md5# 配置app_user可以基于用户名密码+MD5加密算法从*.exmpale.com访问数据库服务，只能访问app_db1和app_db2#host app_db1,app_db2 app_user .exmpale.com md5pg_hba.conf的配置选项非常多，更详细的资料可以查看配置文件自带的注释或者查看pgsql中文社区PostgreSQL10的中文版手册，链接在这激活数据库配置注释里没提及change requires restart，可以通过reload的方式加载配置12su - postgresql/usr/pgsql-10/bin/pg_ctl reload -D $PGDATA注释里明确列明了change requires restart，只能restart的方式重启数据库才能生效12su - postgresql/usr/pgsql-10/bin/pg_ctl restart -D $PGDATAPostgreSQL日志日志路径默认路径在数据目录的log目录里面，%PGDATA/log1log_filename = 'postgresql-%a.log'其中%a是指星期几的缩写作为标识，例如postgresql-Mon.log、postgresql-Tue.log自定义文件名postgresql.conf里面的参数控制日志的文件名1log_filename = 'postgresql-%Y%m%d.log'日志轮转在postgresql.conf可以定义这些参数每天生成一个日志文件1234log_filename = 'postgresql-%Y%m%d.log'log_truncate_on_rotation = offlog_rotation_age = 1dlog_rotation_size = 0每天一个日志文件，只保留7天日志，循环覆盖之前的日志1234log_filename = 'postgresql-%a.log'log_truncate_on_rotation = onlog_rotation_age = 1dlog_rotation_size = 0增加PATH变量切换到postgres用户1su - postgres修改~/.bash_profile12PGHOME='/usr/pgsql-10'export PATH=$&#123;PGHOME&#125;/bin:$&#123;PATH&#125;","categories":[],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://luanlengli.github.io/tags/PostgreSQL/"}]},{"title":"Prometheus搭建简单的监控告警系统","slug":"Prometheus搭建简单的监控告警系统","date":"2019-05-01T02:26:45.000Z","updated":"2019-08-08T09:14:23.000Z","comments":true,"path":"2019/05/01/Prometheus搭建简单的监控告警系统.html","link":"","permalink":"https://luanlengli.github.io/2019/05/01/Prometheus搭建简单的监控告警系统.html","excerpt":"","text":"说明使用的操作系统为CentOS 7.6.1810，其他系统请自己根据差异做对应调整仅用于记录部署过程告警通知对接了邮件和钉钉机器人安装在监控告警系统主机上安装prometheus、alertmanager和grafana被监控的主机安装各种各样的exporter，每个exporter只监控自身的服务状态环境准备123456# 创建prometheus配置目录mkdir -p /etc/prometheus /etc/prometheus/rules.d# 创建prometheus用户useradd prometheus# 修改属主属组chown -R prometheus:prometheus /etc/prometheus软件包prometheusprometheus-v2.9.2123456789101112# 下载解压wget -O - https://github.com/prometheus/prometheus/releases/download/v2.9.2/prometheus-2.9.2.linux-amd64.tar.gz | tar xz# 创建数据目录mkdir -p /var/lib/prometheuschwon -R prometheus:prometheus /var/lib/prometheus# 拷贝文件到对应位置cd prometheus-2.9.2.linux-amd64chown -R prometheus:prometheus prometheus promtool console_libraries consoles prometheus.ymlmv prometheus promtool /usr/local/bin/mv console_libraries consoles prometheus.yml /etc/prometheus/alertmanageralertmanager-v0.17.0123456789101112# 下载解压wget -O - https://github.com/prometheus/alertmanager/releases/download/v0.17.0/alertmanager-0.17.0.linux-amd64.tar.gz | tar xz# 创建数据目录mkdir -p /var/lib/prometheuschwon -R prometheus:prometheus /var/lib/prometheus# 拷贝文件cd alertmanager-0.17.0.linux-amd64chown -R prometheus:prometheus alertmanager alertmanager.yml amtoolmv alertmanager amtool /usr/local/bin/mv alertmanager.yml /etc/prometheusnode_exporternode_exporter-v0.18.0123456# 下载解压wget -O - https://github.com/prometheus/node_exporter/releases/download/v0.18.0/node_exporter-0.18.0.linux-amd64.tar.gz | tar xzcd node_exporter-0.18.0.linux-amd64chown prometheus:prometheus node_exporter# 拷贝文件mv node_exporter /usr/local/bin/mysqld_exportermysqld_exporter-v0.11.0123456# 下载解压wget -O - https://github.com/prometheus/mysqld_exporter/releases/download/v0.11.0/mysqld_exporter-0.11.0.linux-amd64.tar.gz | tar xzcd mysqld_exporter-0.11.0.linux-amd64chown prometheus:prometheus mysqld_exporter# 拷贝文件mv mysqld_exporter /usr/local/bin/postgresql_exporter123456# 下载解压wget -O - https://github.com/wrouesnel/postgres_exporter/releases/download/v0.4.7/postgres_exporter_v0.4.7_linux-amd64.tar.gz | tar xzcd postgres_exporter_v0.4.7_linux-amd64chown prometheus:prometheus postgres_exporter# 拷贝文件mv postgres_exporter /usr/local/bin/blackbox_exporterblackbox_exporter-v0.14.01234567# 下载解压wget -O - https://github.com/prometheus/blackbox_exporter/releases/download/v0.14.0/blackbox_exporter-0.14.0.linux-amd64.tar.gz | tar xzcd blackbox_exporter-0.14.0.linux-amd64chown prometheus:prometheus blackbox_exporter blackbox.yml# 拷贝文件mv blackbox_exporter /usr/local/bin/mv blackbox.yml /etc/prometheus/grafanagrafana-v6.1.6123# 下载wget https://dl.grafana.com/oss/release/grafana-6.1.6-1.x86_64.rpmyum localinstall grafana-6.1.6-1.x86_64.rpm配置Prometheus/etc/prometheus/prometheus.yml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950global: scrape_interval: 15s scrape_timeout: 10s evaluation_interval: 15salerting: alertmanagers: - static_configs: - targets: - localhost:9093 scheme: http timeout: 10srule_files:- /etc/prometheus/rules.d/*.rulesscrape_configs:- job_name: prometheus honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9090- job_name: node-exporter honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9100- job_name: mysqld-exporter honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9104- job_name: postgresql-exporter honor_timestamps: true scrape_interval: 5s scrape_timeout: 5s metrics_path: /metrics scheme: http static_configs: - targets: - localhost:9187/etc/prometheus/rules.d/host-status.rules12345678910111213141516171819202122232425262728293031323334353637383940414243groups: - name: host-status-rule rules: - alert: NodeFilesystemSpaceUsage expr: ( 1 - (node_filesystem_avail_bytes&#123;fstype=~\"ext[234]|btrfs|xfs|zfs\"&#125; / node_filesystem_size_bytes&#123;fstype=~\"ext[234]|btrfs|xfs|zfs\"&#125;) ) * 100 &gt; 85 for: 1m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: 文件系统空间使用率过高\" description: \"&#123;&#123;$labels.instance&#125;&#125;: 文件系统空间使用率超过 85% (当前使用率: &#123;&#123; $value &#125;&#125;)\" - alert: NodeFilesystemInodeUsage expr: ( 1 - (node_filesystem_files_free&#123;fstype=~\"ext[234]|btrfs|xfs|zfs\"&#125; / node_filesystem_files&#123;fstype=~\"ext[234]|btrfs|xfs|zfs\"&#125;) ) * 100 &gt; 80 for: 1m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: 文件系统inode使用率过高\" description: \"&#123;&#123;$labels.instance&#125;&#125;: 文件系统inode使用率超过 80% (当前使用率: &#123;&#123; $value &#125;&#125;)\" - alert: NodeFilesystemReadOnly expr: node_filesystem_readonly&#123;job=\"node-exporter\",device!~'rootfs'&#125; == 1 for: 1m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: 文件系统只读状态\" description: \"&#123;&#123;$labels.instance&#125;&#125;: 文件系统只读状态\" - alert: NodeMemoryUsage expr: (node_memory_MemTotal - (node_memory_MemFree+node_memory_Buffers+node_memory_Cached )) / node_memory_MemTotal * 100 &gt; 80 for: 1m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: 内存使用率过高\" description: \"&#123;&#123;$labels.instance&#125;&#125;: 内存使用率过高超过80% (当前使用率: &#123;&#123; $value &#125;&#125;)\" - alert: NodeCPUUsage expr: (100 - (avg by (instance) (irate(node_cpu_seconds_total&#123;mode='idle',job=\"node-exporter\"&#125;[1m])) * 100)) &gt; 80 for: 1m labels: team: node annotations: summary: \"&#123;&#123;$labels.instance&#125;&#125;: CPU使用率过高\" description: \"&#123;&#123;$labels.instance&#125;&#125;: CPU使用率超过80% (当前使用率: &#123;&#123; $value &#125;&#125;)\"/etc/prometheus/rules.d/mysql-status.rules123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131groups: - name: MySQLStatsAlert rules: - alert: MySQL is down expr: mysql_up == 0 for: 1m labels: severity: critical annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; MySQL is down\" description: \"MySQL database is down. This requires immediate action!\" - alert: open files high expr: mysql_global_status_innodb_num_open_files &gt; (mysql_global_variables_open_files_limit) * 0.75 for: 1m labels: severity: warning annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; open files high\" description: \"Open files is high. Please consider increasing open_files_limit.\" - alert: Read buffer size is bigger than max. allowed packet size expr: mysql_global_variables_read_buffer_size &gt; mysql_global_variables_slave_max_allowed_packet for: 1m labels: severity: warning annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Read buffer size is bigger than max. allowed packet size\" description: \"Read buffer size (read_buffer_size) is bigger than max. allowed packet size (max_allowed_packet).This can break your replication.\" - alert: Sort buffer possibly missconfigured expr: mysql_global_variables_innodb_sort_buffer_size &lt;256*1024 or mysql_global_variables_read_buffer_size &gt; 4*1024*1024 for: 1m labels: severity: warning annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Sort buffer possibly missconfigured\" description: \"Sort buffer size is either too big or too small. A good value for sort_buffer_size is between 256k and 4M.\" - alert: Thread stack size is too small expr: mysql_global_variables_thread_stack &lt;196608 for: 1m labels: severity: warning annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Thread stack size is too small\" description: \"Thread stack size is too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.\" - alert: Used more than 80% of max connections limited expr: mysql_global_status_max_used_connections &gt; mysql_global_variables_max_connections * 0.8 for: 1m labels: severity: warning annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Used more than 80% of max connections limited\" description: \"Used more than 80% of max connections limited\" - alert: InnoDB Force Recovery is enabled expr: mysql_global_variables_innodb_force_recovery != 0 for: 1m labels: severity: warning annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Force Recovery is enabled\" description: \"InnoDB Force Recovery is enabled. This mode should be used for data recovery purposes only. It prohibits writing to the data.\" - alert: InnoDB Log File size is too small expr: mysql_global_variables_innodb_log_file_size &lt; 16777216 for: 1m labels: severity: warning annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Log File size is too small\" description: \"The InnoDB Log File size is possibly too small. Choosing a small InnoDB Log File size can have significant performance impacts.\" - alert: InnoDB Flush Log at Transaction Commit expr: mysql_global_variables_innodb_flush_log_at_trx_commit != 1 for: 1m labels: severity: warning annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Flush Log at Transaction Commit\" description: \"InnoDB Flush Log at Transaction Commit is set to a values != 1. This can lead to a loss of commited transactions in case of a power failure.\" - alert: Table definition cache too small expr: mysql_global_status_open_table_definitions &gt; mysql_global_variables_table_definition_cache for: 1m labels: severity: page annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Table definition cache too small\" description: \"Your Table Definition Cache is possibly too small. If it is much too small this can have significant performance impacts!\" - alert: Table open cache too small expr: mysql_global_status_open_tables &gt;mysql_global_variables_table_open_cache * 99/100 for: 1m labels: severity: page annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Table open cache too small\" description: \"Your Table Open Cache is possibly too small (old name Table Cache). If it is much too small this can have significant performance impacts!\" - alert: Thread stack size is possibly too small expr: mysql_global_variables_thread_stack &lt; 262144 for: 1m labels: severity: page annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Thread stack size is possibly too small\" description: \"Thread stack size is possibly too small. This can cause problems when you use Stored Language constructs for example. A typical is 256k for thread_stack_size.\" - alert: InnoDB Plugin is enabled expr: mysql_global_variables_ignore_builtin_innodb == 1 for: 1m labels: severity: page annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; InnoDB Plugin is enabled\" description: \"InnoDB Plugin is enabled\" - alert: Binary Log is disabled expr: mysql_global_variables_log_bin != 1 for: 1m labels: severity: warning annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Binary Log is disabled\" description: \"Binary Log is disabled. This prohibits you to do Point in Time Recovery (PiTR).\" - alert: Binlog Cache size too small expr: mysql_global_variables_binlog_cache_size &lt; 1048576 for: 1m labels: severity: page annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Cache size too small\" description: \"Binlog Cache size is possibly to small. A value of 1 Mbyte or higher is OK.\" - alert: Binlog Transaction Cache size too small expr: mysql_global_variables_binlog_cache_size &lt; 1048576 for: 1m labels: severity: page annotations: summary: \"Instance &#123;&#123; $labels.instance &#125;&#125; Binlog Transaction Cache size too small\" description: \"Binlog Transaction Cache size is possibly to small. A value of 1 Mbyte or higher is typically OK.\"/etc/prometheus/rules.d/postgresql-status.rules123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441groups:- name: PostgreSQL-Status-Alert rules: ########## EXPORTER RULES ########## - alert: PGExporterScrapeError expr: pg_exporter_last_scrape_error &gt; 0 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: summary: 'Postgres Exporter running on &#123;&#123; $labels.job &#125;&#125; (instance: &#123;&#123; $labels.instance &#125;&#125;) is encountering scrape errors processing queries. Error count: ( &#123;&#123; $value &#125;&#125; )' - alert: NodeExporterScrapeError expr: node_textfile_scrape_error &gt; 0 for: 60s labels: service: system severity: critical severity_num: 300 annotations: summary: 'Node Exporter running on &#123;&#123; $labels.job &#125;&#125; (instance: &#123;&#123; $labels.instance &#125;&#125;) is encountering scrape errors processing custom metrics. Error count: ( &#123;&#123; $value &#125;&#125; )'########## POSTGRESQL RULES ########## - alert: PGIsUp expr: pg_up &lt; 1 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: summary: 'postgres_exporter running on &#123;&#123; $labels.job &#125;&#125; is unable to communicate with the configured database'# Whether a system switches from primary to replica or vice versa must be configured per named job.# No way to tell what value a system is supposed to be without a rule expression for that specific system# 2 to 1 means it changed from primary to replica. 1 to 2 means it changed from replica to primary# Set this alert for each system that you want to monitor a recovery status change# Below is an example for a target job called \"Replica\" and watches for the value to change above 1 which means it's no longer a replica## - alert: PGRecoveryStatusSwitch_Replica # expr: ccp_is_in_recovery_status&#123;job=\"Replica\"&#125; &gt; 1 # for: 60s# labels:# service: postgresql# severity: critical# severity_num: 300# annotations:# summary: '&#123;&#123; $labels.job &#125;&#125; has changed from replica to primary'# Absence alerts must be configured per named job, otherwise there's no way to know which job is down# Below is an example for a target job called \"Prod\"# - alert: PGConnectionAbsent# expr: absent(ccp_connection_stats_max_connections&#123;job=\"Prod\"&#125;)# for: 10s# labels:# service: postgresql# severity: critical# severity_num: 300# annotations:# description: 'Connection metric is absent from target (Prod). Check that postgres_exporter can connect to PostgreSQL.' - alert: PGIdleTxn expr: ccp_connection_stats_max_idle_in_txn_time &gt; 300 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: '&#123;&#123; $labels.job &#125;&#125; has at least one session idle in transaction for over 5 minutes.' summary: 'PGSQL Instance idle transactions' - alert: PGIdleTxn expr: ccp_connection_stats_max_idle_in_txn_time &gt; 900 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: '&#123;&#123; $labels.job &#125;&#125; has at least one session idle in transaction for over 15 minutes.' summary: 'PGSQL Instance idle transactions' - alert: PGQueryTime expr: ccp_connection_stats_max_query_time &gt; 43200 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: '&#123;&#123; $labels.job &#125;&#125; has at least one query running for over 12 hours.' summary: 'PGSQL Max Query Runtime' - alert: PGQueryTime expr: ccp_connection_stats_max_query_time &gt; 86400 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: '&#123;&#123; $labels.job &#125;&#125; has at least one query running for over 1 day.' summary: 'PGSQL Max Query Runtime' - alert: PGConnPerc expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) &gt; 75 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: '&#123;&#123; $labels.job &#125;&#125; is using 75% or more of available connections (&#123;&#123; $value &#125;&#125;%)' summary: 'PGSQL Instance connections' - alert: PGConnPerc expr: 100 * (ccp_connection_stats_total / ccp_connection_stats_max_connections) &gt; 90 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: '&#123;&#123; $labels.job &#125;&#125; is using 90% or more of available connections (&#123;&#123; $value &#125;&#125;%)' summary: 'PGSQL Instance connections' - alert: PGDBSize expr: ccp_database_size &gt; 1.073741824e+11 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; over 100GB in size: &#123;&#123; $value &#125;&#125; bytes' summary: 'PGSQL Instance size warning' - alert: PGDBSize expr: ccp_database_size &gt; 2.68435456e+11 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; over 250GB in size: &#123;&#123; $value &#125;&#125; bytes' summary: 'PGSQL Instance size critical' - alert: PGReplicationByteLag expr: ccp_replication_status_byte_lag &gt; 5.24288e+07 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; has at least one replica lagging over 50MB behind.' summary: 'PGSQL Instance replica lag warning' - alert: PGReplicationByteLag expr: ccp_replication_status_byte_lag &gt; 1.048576e+08 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; has at least one replica lagging over 100MB behind.' summary: 'PGSQL Instance replica lag warning' - alert: PGReplicationSlotsInactive expr: ccp_replication_slots_active == 0 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; has one or more inactive replication slots' summary: 'PGSQL Instance inactive replication slot' - alert: PGXIDWraparound expr: ccp_transaction_wraparound_percent_towards_wraparound &gt; 50 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; is over 50% towards transaction id wraparound.' summary: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; transaction id wraparound imminent' - alert: PGXIDWraparound expr: ccp_transaction_wraparound_percent_towards_wraparound &gt; 75 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; is over 75% towards transaction id wraparound.' summary: 'PGSQL Instance transaction id wraparound imminent' - alert: PGEmergencyVacuum expr: ccp_transaction_wraparound_percent_towards_emergency_autovac &gt; 75 for: 60s labels: service: postgresql severity: warning severity_num: 200 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; is over 75% towards emergency autovacuum processes beginning' summary: 'PGSQL Instance emergency vacuum imminent' - alert: PGEmergencyVacuum expr: ccp_transaction_wraparound_percent_towards_emergency_autovac &gt; 90 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; is over 90% towards emergency autovacuum processes beginning' summary: 'PGSQL Instance emergency vacuum imminent' - alert: PGArchiveCommandStatus expr: ccp_archive_command_status_seconds_since_last_fail &gt; 300 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: 'PGSQL Instance &#123;&#123; $labels.job &#125;&#125; has a recent failing archive command' summary: 'Seconds since the last recorded failure of the archive_command' - alert: PGSequenceExhaustion expr: ccp_sequence_exhaustion_count &gt; 0 for: 60s labels: service: postgresql severity: critical severity_num: 300 annotations: description: 'Count of sequences on instance &#123;&#123; $labels.job &#125;&#125; at over 75% usage: &#123;&#123; $value &#125;&#125;. Run following query to see full sequence status: SELECT * FROM monitor.sequence_status() WHERE percent &gt;= 75'########## SYSTEM RULES ########## - alert: ExporterDown expr: avg_over_time(up[5m]) &lt; 0.9 for: 10s labels: service: system severity: critical severity_num: 300 annotations: description: 'Metrics exporter service for &#123;&#123; $labels.job &#125;&#125; running on &#123;&#123; $labels.instance &#125;&#125; has been down at least 50% of the time for the last 5 minutes. Service may be flapping or down.' summary: 'Prometheus Exporter Service Down' - alert: DiskUsagePerc expr: (100 - 100 * sum(node_filesystem_avail_bytes&#123;device!~\"tmpfs|by-uuid\",fstype=~\"xfs|ext\"&#125; / node_filesystem_size_bytes&#123;device!~\"tmpfs|by-uuid\",fstype=~\"xfs|ext\"&#125;) BY (job,device)) &gt; 70 for: 2m labels: service: system severity: warning severity_num: 200 annotations: description: 'Disk usage on target &#123;&#123; $labels.job &#125;&#125; at &#123;&#123; $value &#125;&#125;%' - alert: DiskUsagePerc expr: (100 - 100 * sum(node_filesystem_avail_bytes&#123;device!~\"tmpfs|by-uuid\",fstype=~\"xfs|ext\"&#125; / node_filesystem_size_bytes&#123;device!~\"tmpfs|by-uuid\",fstype=~\"xfs|ext\"&#125;) BY (job,device)) &gt; 85 for: 2m labels: service: system severity: critical severity_num: 300 annotations: description: 'Disk usage on target &#123;&#123; $labels.job &#125;&#125; at &#123;&#123; $value &#125;&#125;%' - alert: DiskFillPredict expr: predict_linear(node_filesystem_free_bytes&#123;device!~\"tmpfs|by-uuid\",fstype=~\"xfs|ext\"&#125;[1h], 4 * 3600) &lt; 0 for: 5m labels: service: system severity: warning severity_num: 200 annotations: description: '(EXPERIMENTAL) Disk &#123;&#123; $labels.device &#125;&#125; on target &#123;&#123; $labels.job &#125;&#125; is predicted to fill in 4 hrs based on current usage' - alert: SystemLoad5m expr: node_load5 &gt; 5 for: 10m labels: service: system severity: warning severity_num: 200 annotations: description: 'System load for target &#123;&#123; $labels.job &#125;&#125; is high (&#123;&#123; $value &#125;&#125;)' - alert: SystemLoad5m expr: node_load5 &gt; 10 for: 10m labels: service: system severity: critical severity_num: 300 annotations: description: 'System load for target &#123;&#123; $labels.job &#125;&#125; is high (&#123;&#123; $value &#125;&#125;)' - alert: MemoryAvailable expr: (100 * (node_memory_Available_bytes) / node_memory_MemTotal_bytes) &lt; 25 for: 1m labels: service: system severity: warning severity_num: 200 annotations: description: 'Memory available for target &#123;&#123; $labels.job &#125;&#125; is at &#123;&#123; $value &#125;&#125;%' - alert: MemoryAvailable expr: (100 * (node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes) &lt; 10 for: 1m labels: service: system severity: critical severity_num: 300 annotations: description: 'Memory available for target &#123;&#123; $labels.job &#125;&#125; is at &#123;&#123; $value &#125;&#125;%' - alert: SwapUsage expr: (100 - (100 * (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes))) &gt; 60 for: 1m labels: service: system severity: warning severity_num: 200 annotations: description: 'Swap usage for target &#123;&#123; $labels.job &#125;&#125; is at &#123;&#123; $value &#125;&#125;%' - alert: SwapUsage expr: (100 - (100 * (node_memory_SwapFree_byte / node_memory_SwapTotal_bytes))) &gt; 80 for: 1m labels: service: system severity: critical severity_num: 300 annotations: description: 'Swap usage for target &#123;&#123; $labels.job &#125;&#125; is at &#123;&#123; $value &#125;&#125;%'########## PGBACKREST RULES ############ Uncomment and customize one or more of these rules to monitor your pgbackrest backups. # Full backups are considered the equivalent of both differentials and incrementals since both are based on the last full# And differentials are considered incrementals since incrementals will be based off the last diff if one exists# This avoid false alerts, for example when you don't run diff/incr backups on the days that you run a full# Stanza should also be set if different intervals are expected for each stanza. # Otherwise rule will be applied to all stanzas returned on target system if not set.# Otherwise, all backups returned by the pgbackrest info command run from where the database exists will be checked## Relevant metric names are: # ccp_backrest_last_full_time_since_completion_seconds# ccp_backrest_last_incr_time_since_completion_seconds# ccp_backrest_last_diff_time_since_completion_seconds## - alert: PGBackRestLastCompletedFull_main# expr: ccp_backrest_last_full_backup_time_since_completion_seconds&#123;stanza=\"main\"&#125; &gt; 604800# for: 60s# labels:# service: postgresql# severity: critical# severity_num: 300# annotations:# summary: 'Full backup for stanza [main] on system &#123;&#123; $labels.job &#125;&#125; has not completed in the last week.'## - alert: PGBackRestLastCompletedIncr_main# expr: ccp_backrest_last_incr_backup_time_since_completion_seconds&#123;stanza=\"main\"&#125; &gt; 86400# for: 60s# labels:# service: postgresql# severity: critical# severity_num: 300# annotations:# summary: 'Incremental backup for stanza [main] on system &#123;&#123; $labels.job &#125;&#125; has not completed in the last 24 hours.'### Runtime monitoring is handled with a single metric:## ccp_backrest_last_runtime_backup_runtime_seconds## Runtime monitoring should have the \"backup_type\" label set. # Otherwise the rule will apply to the last run of all backup types returned (full, diff, incr)# Stanza should also be set if runtimes per stanza have different expected times## - alert: PGBackRestLastRuntimeFull_main# expr: ccp_backrest_last_runtime_backup_runtime_seconds&#123;backup_type=\"full\", stanza=\"main\"&#125; &gt; 14400# for: 60s# labels:# service: postgresql# severity: critical# severity_num: 300# annotations:# summary: 'Expected runtime of full backup for stanza [main] has exceeded 4 hours'## - alert: PGBackRestLastRuntimeDiff_main# expr: ccp_backrest_last_runtime_backup_runtime_seconds&#123;backup_type=\"diff\", stanza=\"main\"&#125; &gt; 3600# for: 60s# labels:# service: postgresql# severity: critical# severity_num: 300# annotations:# summary: 'Expected runtime of diff backup for stanza [main] has exceeded 1 hour'##### If the pgbackrest command fails to run, the metric disappears from the exporter output and the alert never fires. ## An absence alert must be configured explicitly for each target (job) that backups are being monitored.## Checking for absence of just the full backup type should be sufficient (no need for diff/incr).## Note that while the backrest check command failing will likely also cause a scrape error alert, the addition of this ## check gives a clearer answer as to what is causing it and that something is wrong with the backups.## - alert: PGBackrestAbsentFull_Prod# expr: absent(ccp_backrest_last_full_backup_time_since_completion_seconds&#123;job=\"Prod\"&#125;)# for: 10s# labels:# service: postgresql# severity: critical# severity_num: 300# annotations:# description: 'Backup Full status missing for Prod. Check that pgbackrest info command is working on target system.'Alertmanager/etc/prometheus/alertmanager.yml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758global: resolve_timeout: 5m http_config: &#123;&#125; smtp_from: monitor@example.com smtp_hello: localhost smtp_smarthost: smtp.example.com:465 smtp_auth_username: monitor@example.com smtp_auth_password: '这里写密码' smtp_require_tls: false pagerduty_url: https://events.pagerduty.com/v2/enqueue hipchat_api_url: https://api.hipchat.com/ opsgenie_api_url: https://api.opsgenie.com/ wechat_api_url: https://qyapi.weixin.qq.com/cgi-bin/ victorops_api_url: https://alert.victorops.com/integrations/generic/20131114/alert/route: # 这里配置默认路由到'default-receiver' receiver: default-receiver group_by: - alertname - cluster group_wait: 10s group_interval: 10s repeat_interval: 1hinhibit_rules:- source_match: severity: critical target_match: severity: warning equal: - alertname - dev - instancereceivers:- name: default-receiver email_configs: - send_resolved: false to: monitor@example.com from: monitor@example.com hello: localhost smarthost: smtp.example.com:465 auth_username: monitor@example.com auth_password: '这里写密码' headers: From: monitor@example.com Subject: '&#123;&#123; template \"email.default.subject\" . &#125;&#125;' To: monitor@example.com html: '&#123;&#123; template \"email.default.html\" . &#125;&#125;' require_tls: false # 配置钉钉机器人 webhook_configs: - send_resolved: false url: http://localhost:8060/dingtalk/webhook001/send# 配置钉钉机器人 - name: dingtalk002 webhook_configs: - send_resolved: false url: http://localhost:8060/dingtalk/webhook002/sendtemplates: []配置dingtalk webhook程序123456789# 这里偷懒用docker跑钉钉的webhookdocker run -d \\ --restart=always \\ --name prometheus-webhook-dingtalk \\ -p 8060:8060 \\ -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime:ro \\ timonwong/prometheus-webhook-dingtalk \\ --ding.profile=\"webhook001=https://oapi.dingtalk.com/robot/send?access_token=xxxxxxxxxxxx\" \\ --ding.profile=\"webhook002=https://oapi.dingtalk.com/robot/send?access_token=yyyyyyyyyyy\"blackbox_exporter/etc/prometheus/backbox_exporter.yml1没空搞，占个位mysqld_exporter需要创建用于监控的数据库用户123CREATE USER 'prometheus'@'127.0.0.1' IDENTIFIED BY 'prometheus_password' WITH MAX_USER_CONNECTIONS 3;GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'prometheus'@'127.0.0.1';flush privileges;postgresql_exporter根据需求决定是否使用superuser作为postgresql_exporter的数据库用户如果要创建专用用户可以参照下面的方式创建用户12345678910111213141516171819202122232425262728293031# 创建postgresql_exporter专用用户CREATE USER postgres_exporter PASSWORD 'password';ALTER USER postgres_exporter SET SEARCH_PATH TO postgres_exporter,pg_catalog;# 创建schemaCREATE SCHEMA postgres_exporter;# 授权schemaGRANT USAGE ON SCHEMA postgres_exporter TO postgres_exporter;# 创建函数CREATE FUNCTION get_pg_stat_activity() RETURNS SETOF pg_stat_activity AS$$ SELECT * FROM pg_catalog.pg_stat_activity; $$LANGUAGE sqlVOLATILESECURITY DEFINER;# 创建视图CREATE VIEW postgres_exporter.pg_stat_activityAS SELECT * from get_pg_stat_activity();# 视图授权GRANT SELECT ON postgres_exporter.pg_stat_activity TO postgres_exporter;# 创建函数CREATE FUNCTION get_pg_stat_replication() RETURNS SETOF pg_stat_replication AS$$ SELECT * FROM pg_catalog.pg_stat_replication; $$LANGUAGE sqlVOLATILESECURITY DEFINER;# 创建视图CREATE VIEW postgres_exporter.pg_stat_replicationAS SELECT * FROM get_pg_stat_replication();# 视图授权GRANT SELECT ON postgres_exporter.pg_stat_replication TO postgres_exporter;grafana偷懒警告！这里定义管理员用户密码，还有secret_key，记得自己改，别瞎抄作业！生产环境不要用默认密码，不然有你哭的时候[security]admin_user = adminadmin_password = adminsecret_key = SW2YcwTIb9zpOOhoPsMm/etc/grafana/grafana.ini123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475app_mode = production[paths]data = /var/lib/grafanatemp_data_lifetime = 24hlogs = /var/log/grafanaplugins = /var/lib/grafana/plugins[server]protocol = httphttp_port = 3000# 这里的domain与下面的root_url匹配domain = grafana# root_url定义浏览器怎么访问grafana# http://IP_ADDRESS:3000或者http://domainname:3000root_url = http://grafana:3000enable_gzip = true[database]log_queries = true[remote_cache][session]provider = file[dataproxy][analytics]reporting_enabled = falsecheck_for_updates = false[security]admin_user = adminadmin_password = adminsecret_key = SW2YcwTIb9zpOOhoPsMm[snapshots][dashboards]versions_to_keep = 10[users]default_theme = dark[auth][auth.anonymous]enabled = trueorg_role = Viewer[auth.github][auth.google][auth.generic_oauth][auth.grafana_com][auth.proxy][auth.basic][auth.ldap][smtp][emails][log]mode = console filelevel = info[log.console][log.file]log_rotate = truedaily_rotate = truemax_days = 7[log.syslog][alerting]enabled = trueexecute_alerts = true[explore][metrics]enabled = trueinterval_seconds = 10[metrics.graphite][tracing.jaeger][grafana_com]url = https://grafana.com[external_image_storage][external_image_storage.s3][external_image_storage.webdav][external_image_storage.gcs][external_image_storage.azure_blob][external_image_storage.local][rendering][enterprise][panels]systemd服务prometheus.service/usr/lib/systemd/system/prometheus.service12345678910111213141516171819[Unit]Description=prometheusAfter=network.target[Service]Type=simpleUser=prometheusExecStart=/usr/local/bin/prometheus \\ --config.file=/etc/prometheus/prometheus.yml \\ --storage.tsdb.path=/var/lib/prometheus \\ --storage.tsdb.retention.time=15d \\ --storage.tsdb.retention.size=40GB \\ --web.console.templates=/etc/prometheus/consoles \\ --web.console.libraries=/etc/prometheus/console_librariesExecReload=/bin/kill -HUP $MAINPIDRestart=on-failureRestartSec=60s[Install]WantedBy=multi-user.targetalertmanager.servicce/usr/lib/systemd/system/alertmanager.service1234567891011121314[Unit]Description=alertmanagerAfter=network.target[Service]Type=simpleUser=prometheusExecStart=/usr/local/bin/alertmanager \\ --config.file=/etc/prometheus/alertmanager.yml \\ --storage.path=/var/lib/alertmanager \\ --data.retention=120hRestart=on-failureRestartSec=60s[Install]WantedBy=multi-user.targetnode_exporter.service/usr/lib/systemd/system/node_exporter.service1234567891011[Unit]Description=node_exporterAfter=network.target[Service]Type=simpleUser=prometheusExecStart=/usr/local/bin/node_exporterRestart=on-failureRestartSec=60s[Install]WantedBy=multi-user.targetblackbox_exporter.service/usr/lib/systemd/system/balckbox_exporter.service1234567891011121314[Unit]Description=blackbox_exporterAfter=network.target[Service]Type=simpleUser=prometheusExecStart=/usr/local/bin/blackbox_exporter \\ --config.file=/etc/prometheus/blackbox.yml \\ --web.listen-address=:9115 \\ --log.level=info Restart=on-failureRestartSec=60s[Install]WantedBy=multi-user.targetmysqld_exporter.service/usr/lib/systemd/system/mysqld_exporter.service12345678910111213141516171819202122[Unit]Description=mysqld_exporterAfter=network.target[Service]Type=simpleUser=prometheusEnvironment='DATA_SOURCE_NAME=prometheus:prometheus_password@tcp(127.0.0.1:3306)'ExecStart=/usr/local/bin/mysqld_exporter \\ --collect.engine_innodb_status \\ --collect.info_schema.innodb_metrics \\ --collect.info_schema.userstats \\ --collect.perf_schema.eventsstatements \\ --collect.perf_schema.indexiowaits \\ --collect.perf_schema.tableiowaits \\ --collect.slave_status \\ --log.level=info \\ --web.listen-address=:9104 \\ --web.telemetry-path=/metricsRestart=on-failureRestartSec=60s[Install]WantedBy=multi-user.targetpostgresql_exporter.service/usr/lib/systemd/system/postgresql_exporter.service12345678910111213141516[Unit]Description=postgresql_exporterAfter=network.target[Service]Type=simpleUser=prometheusEnvironment=DATA_SOURCE_NAME=postgresql://postgres_exporter:password@localhost:5432/postgres?sslmode=disableExecStart=/usr/local/bin/postgresql_exporter \\ --web.listen-address=:9187 \\ --web.telemetry-path=/metrics \\ --log.level=info \\ --log.format=logger:stderrRestart=on-failureRestartSec=60s[Install]WantedBy=multi-user.target启动服务修改了systemd脚本之后需要reload一下1systemctl daemon-reloadprometheus1systemctl enable --now prometheus.servicealertmanager1systemctl enable --now alertmanager.servicenode_exporter1systemctl enable --now node_exporter.servicegrafana1systemctl enable --now grafana.service其他其他服务同理，择需启动对应的服务即可验证服务prometheus浏览器访问http://prometheus_server_ip:9090alertmanager浏览器访问http://prometheus_server_ip:9093node_exporter浏览器访问http://prometheus_server_ip:9100grafana浏览器访问http://prometheus_server_ip:3000默认用户密码admin/admin，初次登录需要改密码配置grafana监控面板这里很多作业可以抄，这里简单列举几个我用到的面板抄作业之前，先看看人家的说明！node_exporter面板1 Node Exporter 0.16–0.18 for Prometheus 监控展示看板mysqld_exporter面板Percona出品的dashboard第三方人员提供的dashboardpostgresql_exporter面板Postgres Overviewkong面板kong官方提供的dashboard","categories":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://luanlengli.github.io/categories/Prometheus/"}],"tags":[{"name":"Prometheus","slug":"Prometheus","permalink":"https://luanlengli.github.io/tags/Prometheus/"}]},{"title":"Shell处理xml文件的简单方法","slug":"Shell处理xml文件的简单方法","date":"2019-04-09T01:51:59.000Z","updated":"2019-06-23T16:37:26.000Z","comments":true,"path":"2019/04/09/Shell处理xml文件的简单方法.html","link":"","permalink":"https://luanlengli.github.io/2019/04/09/Shell处理xml文件的简单方法.html","excerpt":"","text":"说明工作中遇到需要使用shell命令编辑xml文件，以shell自带的命令去处理非常的不方便，于是乎找了一下处理xml的命令行工具。在此记录一下处理过程。准备操作系统CentOS-7.x软件包libxml2、xml2、xmlstarlet操作样例准备一个xml文件，这里以Apache maven的settings.xml为例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!--Licensed to the Apache Software Foundation (ASF) under oneor more contributor license agreements. See the NOTICE filedistributed with this work for additional informationregarding copyright ownership. The ASF licenses this fileto you under the Apache License, Version 2.0 (the\"License\"); you may not use this file except in compliancewith the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing,software distributed under the License is distributed on an\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANYKIND, either express or implied. See the License for thespecific language governing permissions and limitationsunder the License.--&gt;&lt;!-- | This is the configuration file for Maven. It can be specified at two levels: | | 1. User Level. This settings.xml file provides configuration for a single user, | and is normally provided in $&#123;user.home&#125;/.m2/settings.xml. | | NOTE: This location can be overridden with the CLI option: | | -s /path/to/user/settings.xml | | 2. Global Level. This settings.xml file provides configuration for all Maven | users on a machine (assuming they're all using the same Maven | installation). It's normally provided in | $&#123;maven.conf&#125;/settings.xml. | | NOTE: This location can be overridden with the CLI option: | | -gs /path/to/global/settings.xml | | The sections in this sample file are intended to give you a running start at | getting the most out of your Maven installation. Where appropriate, the default | values (values used when the setting is not specified) are provided. | |--&gt;&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt; &lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: $&#123;user.home&#125;/.m2/repository &lt;localRepository&gt;/path/to/local/repo&lt;/localRepository&gt; --&gt; &lt;!-- interactiveMode | This will determine whether maven prompts you when it needs input. If set to false, | maven will use a sensible default value, perhaps based on some other setting, for | the parameter in question. | | Default: true &lt;interactiveMode&gt;true&lt;/interactiveMode&gt; --&gt; &lt;!-- offline | Determines whether maven should attempt to connect to the network when executing a build. | This will have an effect on artifact downloads, artifact deployment, and others. | | Default: false &lt;offline&gt;false&lt;/offline&gt; --&gt; &lt;!-- pluginGroups | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e. | when invoking a command line like \"mvn prefix:goal\". Maven will automatically add the group identifiers | \"org.apache.maven.plugins\" and \"org.codehaus.mojo\" if these are not already contained in the list. |--&gt; &lt;pluginGroups&gt; &lt;!-- pluginGroup | Specifies a further group identifier to use for plugin lookup. &lt;pluginGroup&gt;com.your.plugins&lt;/pluginGroup&gt; --&gt; &lt;/pluginGroups&gt; &lt;!-- proxies | This is a list of proxies which can be used on this machine to connect to the network. | Unless otherwise specified (by system property or command-line switch), the first proxy | specification in this list marked as active will be used. |--&gt; &lt;proxies&gt; &lt;!-- proxy | Specification for one proxy, to be used in connecting to the network. | &lt;proxy&gt; &lt;id&gt;optional&lt;/id&gt; &lt;active&gt;true&lt;/active&gt; &lt;protocol&gt;http&lt;/protocol&gt; &lt;username&gt;proxyuser&lt;/username&gt; &lt;password&gt;proxypass&lt;/password&gt; &lt;host&gt;proxy.host.net&lt;/host&gt; &lt;port&gt;80&lt;/port&gt; &lt;nonProxyHosts&gt;local.net|some.host.com&lt;/nonProxyHosts&gt; &lt;/proxy&gt; --&gt; &lt;/proxies&gt; &lt;!-- servers | This is a list of authentication profiles, keyed by the server-id used within the system. | Authentication profiles can be used whenever maven must make a connection to a remote server. |--&gt; &lt;servers&gt; &lt;!-- server | Specifies the authentication information to use when connecting to a particular server, identified by | a unique name within the system (referred to by the 'id' attribute below). | | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are | used together. | &lt;server&gt; &lt;id&gt;deploymentRepo&lt;/id&gt; &lt;username&gt;repouser&lt;/username&gt; &lt;password&gt;repopwd&lt;/password&gt; &lt;/server&gt; --&gt; &lt;!-- Another sample, using keys to authenticate. &lt;server&gt; &lt;id&gt;siteServer&lt;/id&gt; &lt;privateKey&gt;/path/to/private/key&lt;/privateKey&gt; &lt;passphrase&gt;optional; leave empty if not used.&lt;/passphrase&gt; &lt;/server&gt; --&gt; &lt;/servers&gt; &lt;!-- mirrors | This is a list of mirrors to be used in downloading artifacts from remote repositories. | | It works like this: a POM may declare a repository to use in resolving certain artifacts. | However, this repository may have problems with heavy traffic at times, so people have mirrored | it to several places. | | That repository definition will have a unique id, so we can create a mirror reference for that | repository, to be used as an alternate download site. The mirror site will be the preferred | server for that repository. |--&gt; &lt;mirrors&gt; &lt;!-- mirror | Specifies a repository mirror site to use instead of a given repository. The repository that | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used | for inheritance and direct lookup purposes, and must be unique across the set of mirrors. | &lt;mirror&gt; &lt;id&gt;mirrorId&lt;/id&gt; &lt;mirrorOf&gt;repositoryId&lt;/mirrorOf&gt; &lt;name&gt;Human Readable Name for this Mirror.&lt;/name&gt; &lt;url&gt;http://my.repository.com/repo/path&lt;/url&gt; &lt;/mirror&gt; --&gt; &lt;/mirrors&gt; &lt;!-- profiles | This is a list of profiles which can be activated in a variety of ways, and which can modify | the build process. Profiles provided in the settings.xml are intended to provide local machine- | specific paths and repository locations which allow the build to work in the local environment. | | For example, if you have an integration testing plugin - like cactus - that needs to know where | your Tomcat instance is installed, you can provide a variable here such that the variable is | dereferenced during the build process to configure the cactus plugin. | | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles | section of this document (settings.xml) - will be discussed later. Another way essentially | relies on the detection of a system property, either matching a particular value for the property, | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a | value of '1.4' might activate a profile when the build is executed on a JDK version of '1.4.2_07'. | Finally, the list of active profiles can be specified directly from the command line. | | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact | repositories, plugin repositories, and free-form properties to be used as configuration | variables for plugins in the POM. | |--&gt; &lt;profiles&gt; &lt;!-- profile | Specifies a set of introductions to the build process, to be activated using one or more of the | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles/&gt; | or the command line, profiles have to have an ID that is unique. | | An encouraged best practice for profile identification is to use a consistent naming convention | for profiles, such as 'env-dev', 'env-test', 'env-production', 'user-jdcasey', 'user-brett', etc. | This will make it more intuitive to understand what the set of introduced profiles is attempting | to accomplish, particularly when you only have a list of profile id's for debug. | | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo. &lt;profile&gt; &lt;id&gt;jdk-1.4&lt;/id&gt; &lt;activation&gt; &lt;jdk&gt;1.4&lt;/jdk&gt; &lt;/activation&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jdk14&lt;/id&gt; &lt;name&gt;Repository for JDK 1.4 builds&lt;/name&gt; &lt;url&gt;http://www.myhost.com/maven/jdk14&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshotPolicy&gt;always&lt;/snapshotPolicy&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; --&gt; &lt;!-- | Here is another profile, activated by the system property 'target-env' with a value of 'dev', | which provides a specific path to the Tomcat instance. To use this, your plugin configuration | might hypothetically look like: | | ... | &lt;plugin&gt; | &lt;groupId&gt;org.myco.myplugins&lt;/groupId&gt; | &lt;artifactId&gt;myplugin&lt;/artifactId&gt; | | &lt;configuration&gt; | &lt;tomcatLocation&gt;$&#123;tomcatPath&#125;&lt;/tomcatLocation&gt; | &lt;/configuration&gt; | &lt;/plugin&gt; | ... | | NOTE: If you just wanted to inject this configuration whenever someone set 'target-env' to | anything, you could just leave off the &lt;value/&gt; inside the activation-property. | &lt;profile&gt; &lt;id&gt;env-dev&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;target-env&lt;/name&gt; &lt;value&gt;dev&lt;/value&gt; &lt;/property&gt; &lt;/activation&gt; &lt;properties&gt; &lt;tomcatPath&gt;/path/to/tomcat/instance&lt;/tomcatPath&gt; &lt;/properties&gt; &lt;/profile&gt; --&gt; &lt;/profiles&gt; &lt;!-- activeProfiles | List of profiles that are active for all builds. | &lt;activeProfiles&gt; &lt;activeProfile&gt;alwaysActiveProfile&lt;/activeProfile&gt; &lt;activeProfile&gt;anotherAlwaysActiveProfile&lt;/activeProfile&gt; &lt;/activeProfiles&gt; --&gt;&lt;/settings&gt;在国内使用maven都需要在settings.xml里面配置国内的mirrors，以加速maven下载依赖包的过程。这里以阿里云的为例，需要在settings.xml文件里面添加如下内容12345678910&lt;mirrors&gt; ... &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; ...&lt;/mirrors&gt;操作过程处理settings.xml将settings.xml处理成容易处理的字符串清理注释123456789xmlstarlet ed -d '//comment()' settings.xml | grep -v mirrors | xml2# 输出实例/settings/@xmlns=http://maven.apache.org/SETTINGS/1.0.0/settings/@xmlns:xsi=http://www.w3.org/2001/XMLSchema-instance/settings/@xsi:schemaLocation=http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd/settings/pluginGroups/settings/proxies/settings/servers/settings/profiles处理mirror配置同样使用xml2处理xml12345678910111213141516171819cat &gt; aliyun_mirror.xml &lt;&lt;EOF&lt;settings&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt;&lt;/settings&gt;EOFxml2 &lt; aliyun_mirror.xml# 输出示例settings/mirrors/mirror/id=alimavensettings/mirrors/mirror/name=aliyun mavensettings/mirrors/mirror/url=http://maven.aliyun.com/nexus/content/groups/public/settings/mirrors/mirror/mirrorOf=central合并配置1234567891011121314Settings=$(xmlstarlet ed -d '//comment()' settings.xml | grep -v mirrors | xml2)AliyunMirror=$(xml2 &lt; aliyun_mirror.xml)printf \"$Settings\\n$AliyunMirror\"# 输出示例/settings/@xmlns=http://maven.apache.org/SETTINGS/1.0.0/settings/@xmlns:xsi=http://www.w3.org/2001/XMLSchema-instance/settings/@xsi:schemaLocation=http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd/settings/pluginGroups/settings/proxies/settings/servers/settings/profiles/settings/mirrors/mirror/id=alimaven/settings/mirrors/mirror/name=aliyun maven/settings/mirrors/mirror/url=http://maven.aliyun.com/nexus/content/groups/public//settings/mirrors/mirror/mirrorOf=central将字符串转换成xml格式文件这里将输出结果打印在终端，可以直接重定向到settings.xml文件1234567891011121314151617printf \"$Settings\\n$AliyunMirror\" | 2xml | xmllint --format -# 输出示例&lt;?xml version=\"1.0\"?&gt;&lt;settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\"&gt; &lt;pluginGroups/&gt; &lt;proxies/&gt; &lt;servers/&gt; &lt;profiles/&gt; &lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt;&lt;/settings&gt;","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/tags/Linux/"},{"name":"Shell","slug":"Shell","permalink":"https://luanlengli.github.io/tags/Shell/"}]},{"title":"git pull免密码配置","slug":"git-pull免密码配置","date":"2019-04-07T14:25:00.000Z","updated":"2019-08-20T04:43:40.000Z","comments":true,"path":"2019/04/07/git-pull免密码配置.html","link":"","permalink":"https://luanlengli.github.io/2019/04/07/git-pull免密码配置.html","excerpt":"","text":"说明git 拉取代码的时候，如果不使用密钥的方式时，会要求输入用户密码，非常繁琐为此需要配置免账号密码登录配置过程切换到用户家目录1cd ~运行git命令1git config --global credential.helper store此命令会在用户家目录生成一个.gitconfig，内容如下12[credential] helper = store再次运行git pull还是会提示输入用户密码，命令会在用户家目录生成.git-credentials文件，里面会保存刚才输入的用户密码。1http://&lt;USERNAME&gt;:&lt;PASSWORD&gt;@gitlab.com后续操作git的时候就不会要求输入用户密码了","categories":[],"tags":[{"name":"git","slug":"git","permalink":"https://luanlengli.github.io/tags/git/"}]},{"title":"阿里云RDS数据库MySQL5.7实例主从搭建过程","slug":"阿里云RDS数据库MySQL5-7实例主从搭建过程","date":"2019-04-06T03:16:33.000Z","updated":"2019-07-06T04:08:41.000Z","comments":true,"path":"2019/04/06/阿里云RDS数据库MySQL5-7实例主从搭建过程.html","link":"","permalink":"https://luanlengli.github.io/2019/04/06/阿里云RDS数据库MySQL5-7实例主从搭建过程.html","excerpt":"","text":"说明工作中遇到需要搭建阿里云RDS数据库MySQL5.7实例主从集群，在此记录搭建过程主库阿里云RDS数据库MySQL5.7实例从库本地服务器MySQL5.7，通过公网访问阿里云RDS实例阿里云RDS实例准备操作登录阿里云管理控制台选择【云数据库RDS版】切换到【云数据库RDS版】界面，左上角选择【数据中心】，在【实例列表】中找到主库，点击右侧的【管理】在【基本信息】可以看到主库的【外网地址】和端口外网地址示例：rm-xxxxxxxxxxxxxxx.mysql.rds.aliyuncs.com端口：3306切换到【账号管理】，点击【创建账号】用户名：repuser密码：repuserpassword账号类型：普通帐号授权数据库：选择需要同步的数据库，选择完毕后，权限统一设置为【只读】登录阿里云RDS实例，查看主库信息show variables like &#39;%binlog_format%&#39;;show variables like &#39;server_id&#39;;从库安装MySQL5.7，这里网上很多教程，自行解决修改从库my.cnf12345678910111213[mysqld]# 这里的server-id要跟RDS主库查询到的server-id不一样server-id = 3306 log_bin = /data/mysql-dataexpire_logs_days = 7max_binlog_size = 100Mreplicate-ignore-db = ccccc #不想同步的数据库replicate-ignore-db = ddddd #不想同步的数据库# 启动GTID gtid_mode = onenforce_gtid_consistency = onbinlog_format = row #设置 binlog 为 rowlog-slave-updates = 1在从库上对阿里云RDS实例做一次dump备份假设我们要同步的是aaa和bbb两个库123456789mysqldump -u'repuser' \\ -h'rm-xxxxxxxxxxxxxxx.mysql.rds.aliyuncs.com' \\ -P 3306 \\ -p'repuserpassword' \\ --set-gtid-purged=ON \\ --master-data=2 \\ --single-transaction \\ --default-character-set=utf8mb4 \\ --databases aaa bbb &gt; aliyun_rds_dump.sql从库导入dump备份1234mysql -u'root' \\ -p \\ -P3306 \\ &lt; aliyun_rds_dump.sql设置从库启用基于GTID的主从同步1234567mysql&gt; change master to master_host = &apos;xxxxxxxxxxxxxxx.mysql.rds.aliyuncs.com&apos;, master_port = 3306, master_user = &apos;repuser&apos;, master_password=&apos;repuserpassword&apos;, master_connect_retry=10, master_auto_position=1;mysql&gt; start slave;检查主从同步状态12mysql&gt; show slave status \\Gmysql&gt; show processlist \\G","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://luanlengli.github.io/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://luanlengli.github.io/tags/MySQL/"}]},{"title":"ZooKeeper安装部署记录","slug":"Zookeeper搭建部署记录","date":"2019-03-29T02:26:22.000Z","updated":"2019-06-04T09:00:51.000Z","comments":true,"path":"2019/03/29/Zookeeper搭建部署记录.html","link":"","permalink":"https://luanlengli.github.io/2019/03/29/Zookeeper搭建部署记录.html","excerpt":"","text":"说明此文仅记录Zookeeper单机和多节点搭建部署过程操作系统使用的CentOS-7.6.1810 x86_64JDK版本OpenJDK-1.8ZooKeeper版本3.4.14虚拟机配置1CPU 4G内存 20G系统盘 30G数据盘环境准备安装OpenJDK1yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel安装ZooKeeper创建Zookeeper用户12groupadd zookeeperuseradd -r -g zookeeper -s /bin/false zookeeper创建Zookeeper程序目录1mkdir -p /opt/zookeeper切换工作目录1cd /opt/software下载ZooKeeper国内有Apache基金会的镜像源，这里用的是清华大学的源1wget -O - https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz | tar xz --directory=/opt/software创建软连接1ln -sv zookeeper-3.4.14 zookeeper创建数据目录1mkdir -p /opt/zk-data /opt/zk-log创建配置文件1234567891011cat &gt; /opt/software/zookeeper/conf/zoo.cfg &lt;&lt;EOFtickTime=2000initLimit=10syncLimit=5dataDir=/opt/zk-datadataLogDir=/opt/zk-logclientPort=2181maxClientCnxns=200autopurge.snapRetainCount=3autopurge.purgeInterval=1EOF授权目录1chown -R zookeeper:zookeeper /opt/zookeeper /opt/zk-data /opt/zk-log启动Zookeeper1su -s /bin/sh -c \"/opt/software/zookeeper/bin/zkServer.sh start /opt/software/zookeeper/conf/zoo.cfg\" zookeeper测试Zookeeper使用客户端登陆123456789101112131415161718192021222324252627282930313233343536373839/opt/software/zookeeper/bin/zkCli.sh -server localhost:2181# 输出示例/bin/javaConnecting to localhost:2181YYYY-MM-DD 10:53:57,868 [myid:] - INFO [main:Environment@109] - Client environment:zookeeper.version=3.5.5-390fe37ea45dee01bf87dc1c042b5e3dcce88653, built on 05/03/2019 12:07 GMTYYYY-MM-DD 10:53:57,871 [myid:] - INFO [main:Environment@109] - Client environment:host.name=localhostYYYY-MM-DD 10:53:57,871 [myid:] - INFO [main:Environment@109] - Client environment:java.version=1.8.0_212YYYY-MM-DD 10:53:57,878 [myid:] - INFO [main:Environment@109] - Client environment:java.vendor=Oracle CorporationYYYY-MM-DD 10:53:57,878 [myid:] - INFO [main:Environment@109] - Client environment:java.home=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.212.b04-0.el7_6.x86_64/jreYYYY-MM-DD 10:53:57,878 [myid:] - INFO [main:Environment@109] - Client environment:java.class.path=/opt/zookeeper/zookeeper/bin/../zookeeper-server/target/classes:/opt/zookeeper/zookeeper/bin/../build/classes:/opt/zookeeper/zookeeper/bin/../zookeeper-server/target/lib/*.jar:/opt/zookeeper/zookeeper/bin/../build/lib/*.jar:/opt/zookeeper/zookeeper/bin/../lib/zookeeper-jute-3.5.5.jar:/opt/zookeeper/zookeeper/bin/../lib/zookeeper-3.5.5.jar:/opt/zookeeper/zookeeper/bin/../lib/slf4j-log4j12-1.7.25.jar:/opt/zookeeper/zookeeper/bin/../lib/slf4j-api-1.7.25.jar:/opt/zookeeper/zookeeper/bin/../lib/netty-all-4.1.29.Final.jar:/opt/zookeeper/zookeeper/bin/../lib/log4j-1.2.17.jar:/opt/zookeeper/zookeeper/bin/../lib/json-simple-1.1.1.jar:/opt/zookeeper/zookeeper/bin/../lib/jline-2.11.jar:/opt/zookeeper/zookeeper/bin/../lib/jetty-util-9.4.17.v20190418.jar:/opt/zookeeper/zookeeper/bin/../lib/jetty-servlet-9.4.17.v20190418.jar:/opt/zookeeper/zookeeper/bin/../lib/jetty-server-9.4.17.v20190418.jar:/opt/zookeeper/zookeeper/bin/../lib/jetty-security-9.4.17.v20190418.jar:/opt/zookeeper/zookeeper/bin/../lib/jetty-io-9.4.17.v20190418.jar:/opt/zookeeper/zookeeper/bin/../lib/jetty-http-9.4.17.v20190418.jar:/opt/zookeeper/zookeeper/bin/../lib/javax.servlet-api-3.1.0.jar:/opt/zookeeper/zookeeper/bin/../lib/jackson-databind-2.9.8.jar:/opt/zookeeper/zookeeper/bin/../lib/jackson-core-2.9.8.jar:/opt/zookeeper/zookeeper/bin/../lib/jackson-annotations-2.9.0.jar:/opt/zookeeper/zookeeper/bin/../lib/commons-cli-1.2.jar:/opt/zookeeper/zookeeper/bin/../lib/audience-annotations-0.5.0.jar:/opt/zookeeper/zookeeper/bin/../zookeeper-*.jar:/opt/zookeeper/zookeeper/bin/../zookeeper-server/src/main/resources/lib/*.jar:/opt/zookeeper/zookeeper/bin/../conf:YYYY-MM-DD 10:53:57,879 [myid:] - INFO [main:Environment@109] - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/libYYYY-MM-DD 10:53:57,879 [myid:] - INFO [main:Environment@109] - Client environment:java.io.tmpdir=/tmpYYYY-MM-DD 10:53:57,880 [myid:] - INFO [main:Environment@109] - Client environment:java.compiler=&lt;NA&gt;YYYY-MM-DD 10:53:57,880 [myid:] - INFO [main:Environment@109] - Client environment:os.name=LinuxYYYY-MM-DD 10:53:57,880 [myid:] - INFO [main:Environment@109] - Client environment:os.arch=amd64YYYY-MM-DD 10:53:57,880 [myid:] - INFO [main:Environment@109] - Client environment:os.version=3.10.0-957.12.2.el7.x86_64YYYY-MM-DD 10:53:57,880 [myid:] - INFO [main:Environment@109] - Client environment:user.name=zookeeperYYYY-MM-DD 10:53:57,881 [myid:] - INFO [main:Environment@109] - Client environment:user.home=/home/zookeeperYYYY-MM-DD 10:53:57,881 [myid:] - INFO [main:Environment@109] - Client environment:user.dir=/home/zookeeperYYYY-MM-DD 10:53:57,882 [myid:] - INFO [main:Environment@109] - Client environment:os.memory.free=55MBYYYY-MM-DD 10:53:57,887 [myid:] - INFO [main:Environment@109] - Client environment:os.memory.max=247MBYYYY-MM-DD 10:53:57,887 [myid:] - INFO [main:Environment@109] - Client environment:os.memory.total=59MBYYYY-MM-DD 10:53:57,890 [myid:] - INFO [main:ZooKeeper@868] - Initiating client connection, connectString=localhost:2181 sessionTimeout=30000 watcher=org.apache.zookeeper.ZooKeeperMain$MyWatcher@3b95a09cYYYY-MM-DD 10:53:57,906 [myid:] - INFO [main:X509Util@79] - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiationYYYY-MM-DD 10:53:57,930 [myid:] - INFO [main:ClientCnxnSocket@237] - jute.maxbuffer value is 4194304 BytesYYYY-MM-DD 10:53:57,943 [myid:] - INFO [main:ClientCnxn@1653] - zookeeper.request.timeout value is 0. feature enabled=YYYY-MM-DD 10:53:57,951 [myid:localhost:2181] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1112] - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)Welcome to ZooKeeper!JLine support is enabledYYYY-MM-DD 10:53:58,115 [myid:localhost:2181] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@959] - Socket connection established, initiating session, client: /0:0:0:0:0:0:0:1:56998, server: localhost/0:0:0:0:0:0:0:1:2181YYYY-MM-DD 10:53:58,149 [myid:localhost:2181] - INFO [main-SendThread(localhost:2181):ClientCnxn$SendThread@1394] - Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x1000012d1c20002, negotiated timeout = 30000WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper][zk: localhost:2181(CONNECTED) 1] ls /zookeeper[config, quota]托管为systemd服务1234567891011121314151617[Unit]Description=Zookeeper serviceAfter=network.target[Service]Type=simpleUser=zookeeperGroup=zookeeperEnvironment=ZOO_LOG_DIR=/opt/zk-logExecStart=/opt/software/zookeeper/bin/zkServer.sh start-foreground /opt/software/zookeeper/conf/zoo.cfgExecStop=/opt/software/zookeeper/bin/zkServer.sh stop /opt/software/zookeeper/conf/zoo.cfgWorkingDirectory=/opt/zk-dataRestartSec=60Restart=on-failure[Install]WantedBy=multi-user.target多节点部署ZookeeperZookeeper多节点部署比较简单一般情况下使用三个节点，奇数节点方便集群内部选主配置文件这里需要在zoo.cfg添加集群节点信息123server.1=zk1:2888:3888server.2=zk2:2888:3888server.3=zk3:2888:3888创建myid文件每个节点都需要在zookeeper数据目录下创建myid，用于标记节点序号zk11echo '1' &gt; /opt/zk-data/myidzk21echo '2' &gt; /opt/zk-data/myidzk31echo '3' &gt; /opt/zk-data/myid启动Zookeeper在initLimit配置初始化时间内，在zk1、zk2、zk3启动Zookeeper即可","categories":[],"tags":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"https://luanlengli.github.io/tags/ZooKeeper/"}]},{"title":"Elasticsearch-学习笔记","slug":"Elasticsearch-学习笔记","date":"2019-02-22T15:28:10.000Z","updated":"2019-06-23T16:36:12.000Z","comments":true,"path":"2019/02/22/Elasticsearch-学习笔记.html","link":"","permalink":"https://luanlengli.github.io/2019/02/22/Elasticsearch-学习笔记.html","excerpt":"","text":"_cat APIs通过访问elasticsearch集群的/_cat可以获取很多有用的信息。使用curl -X GET &quot;http://elasticsearch:9200/_cat&quot;可以单独列出所有可用的命令1234567891011121314151617181920212223242526272829curl -X GET \"http://elasticsearch-1:9200/_cat/\"=^.^=/_cat/allocation/_cat/shards/_cat/shards/&#123;index&#125;/_cat/master/_cat/nodes/_cat/tasks/_cat/indices/_cat/indices/&#123;index&#125;/_cat/segments/_cat/segments/&#123;index&#125;/_cat/count/_cat/count/&#123;index&#125;/_cat/recovery/_cat/recovery/&#123;index&#125;/_cat/health/_cat/pending_tasks/_cat/aliases/_cat/aliases/&#123;alias&#125;/_cat/thread_pool/_cat/thread_pool/&#123;thread_pools&#125;/_cat/plugins/_cat/fielddata/_cat/fielddata/&#123;fields&#125;/_cat/nodeattrs/_cat/repositories/_cat/snapshots/&#123;repository&#125;/_cat/templates获取_cat API帮助可以在/_cat/COMMAND后面加上参数help，获取对应的帮助信息12345678910curl -XGET \"http://elasticsearch-1:9200/_cat/allocation?help\"shards | s | number of shards on node disk.indices | di,diskIndices | disk used by ES indices disk.used | du,diskUsed | disk used (total, not just ES)disk.avail | da,diskAvail | disk available disk.total | dt,diskTotal | total capacity of all volumes disk.percent | dp,diskPercent | percent disk used host | h | host of node ip | | ip of node node | n | name of node获取当前master信息123curl -XGET \"http://elasticsearch-1:9200/_cat/master?v\"id host ip nodeJLyKyWH_QnSiXXuRNCj_3g 172.16.80.201 172.16.80.201 elasticsearch-1获取节点信息123456789101112131415curl -XGET \"http://elasticsearch-1:9200/_cat/nodes?format=json&amp;pretty\"[ &#123; \"ip\" : \"172.16.80.201\", \"heap.percent\" : \"7\", \"ram.percent\" : \"85\", \"cpu\" : \"1\", \"load_1m\" : \"0.00\", \"load_5m\" : \"0.08\", \"load_15m\" : \"0.20\", \"node.role\" : \"mdi\", \"master\" : \"*\", \"name\" : \"elasticsearch-1\" &#125;]获取索引信息以json格式返回结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475curl -XGET -H \"Accept: application/json\" \"http://elasticsearch-1:9200/_cat/indices?format=json&amp;pretty\"[ &#123; \"health\" : \"yellow\", \"status\" : \"open\", \"index\" : \".kibana\", \"uuid\" : \"V_t2TnJKQ9WV8E1C_uzIfA\", \"pri\" : \"1\", \"rep\" : \"1\", \"docs.count\" : \"6\", \"docs.deleted\" : \"0\", \"store.size\" : \"37.3kb\", \"pri.store.size\" : \"37.3kb\" &#125;, &#123; \"health\" : \"yellow\", \"status\" : \"open\", \"index\" : \"bank\", \"uuid\" : \"3OEfeSwsTyKyC2zKHbuzJQ\", \"pri\" : \"5\", \"rep\" : \"1\", \"docs.count\" : \"1000\", \"docs.deleted\" : \"0\", \"store.size\" : \"640.8kb\", \"pri.store.size\" : \"640.8kb\" &#125;, &#123; \"health\" : \"yellow\", \"status\" : \"open\", \"index\" : \"logstash-2015.05.18\", \"uuid\" : \"h8m1NlTIQS6Z49mHACYokA\", \"pri\" : \"5\", \"rep\" : \"1\", \"docs.count\" : \"4631\", \"docs.deleted\" : \"0\", \"store.size\" : \"27mb\", \"pri.store.size\" : \"27mb\" &#125;, &#123; \"health\" : \"yellow\", \"status\" : \"open\", \"index\" : \"shakespeare\", \"uuid\" : \"xwoP7Pc3Q3ySZ47MosHrFA\", \"pri\" : \"5\", \"rep\" : \"1\", \"docs.count\" : \"111396\", \"docs.deleted\" : \"0\", \"store.size\" : \"29mb\", \"pri.store.size\" : \"29mb\" &#125;, &#123; \"health\" : \"yellow\", \"status\" : \"open\", \"index\" : \"logstash-2015.05.20\", \"uuid\" : \"ewX3DcZHScOK4Mi6lF6Hqg\", \"pri\" : \"5\", \"rep\" : \"1\", \"docs.count\" : \"4750\", \"docs.deleted\" : \"0\", \"store.size\" : \"30.3mb\", \"pri.store.size\" : \"30.3mb\" &#125;, &#123; \"health\" : \"yellow\", \"status\" : \"open\", \"index\" : \"logstash-2015.05.19\", \"uuid\" : \"siQA71vbSYOgE3a8M7HDhg\", \"pri\" : \"5\", \"rep\" : \"1\", \"docs.count\" : \"4624\", \"docs.deleted\" : \"0\", \"store.size\" : \"29.3mb\", \"pri.store.size\" : \"29.3mb\" &#125;]以yaml格式返回结果1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162curl -XGET \"http://elasticsearch-1:9200/_cat/indices?format=yaml&amp;pretty\"---- health: \"yellow\" status: \"open\" index: \".kibana\" uuid: \"V_t2TnJKQ9WV8E1C_uzIfA\" pri: \"1\" rep: \"1\" docs.count: \"6\" docs.deleted: \"0\" store.size: \"37.3kb\" pri.store.size: \"37.3kb\"- health: \"yellow\" status: \"open\" index: \"bank\" uuid: \"3OEfeSwsTyKyC2zKHbuzJQ\" pri: \"5\" rep: \"1\" docs.count: \"1000\" docs.deleted: \"0\" store.size: \"640.8kb\" pri.store.size: \"640.8kb\"- health: \"yellow\" status: \"open\" index: \"logstash-2015.05.18\" uuid: \"h8m1NlTIQS6Z49mHACYokA\" pri: \"5\" rep: \"1\" docs.count: \"4631\" docs.deleted: \"0\" store.size: \"27mb\" pri.store.size: \"27mb\"- health: \"yellow\" status: \"open\" index: \"shakespeare\" uuid: \"xwoP7Pc3Q3ySZ47MosHrFA\" pri: \"5\" rep: \"1\" docs.count: \"111396\" docs.deleted: \"0\" store.size: \"29mb\" pri.store.size: \"29mb\"- health: \"yellow\" status: \"open\" index: \"logstash-2015.05.20\" uuid: \"ewX3DcZHScOK4Mi6lF6Hqg\" pri: \"5\" rep: \"1\" docs.count: \"4750\" docs.deleted: \"0\" store.size: \"30.3mb\" pri.store.size: \"30.3mb\"- health: \"yellow\" status: \"open\" index: \"logstash-2015.05.19\" uuid: \"siQA71vbSYOgE3a8M7HDhg\" pri: \"5\" rep: \"1\" docs.count: \"4624\" docs.deleted: \"0\" store.size: \"29.3mb\" pri.store.size: \"29.3mb\"_cluster APIs官方文档见这里Cluster Health官方文档见这里1234567891011121314151617curl -XGET \"http://elasticsearch-1:9200/_cluster/health?format=yaml&amp;pretty\"---cluster_name: \"elasticsearch\"status: \"yellow\"timed_out: falsenumber_of_nodes: 1number_of_data_nodes: 1active_primary_shards: 26active_shards: 26relocating_shards: 0initializing_shards: 0unassigned_shards: 26delayed_unassigned_shards: 0number_of_pending_tasks: 0number_of_in_flight_fetch: 0task_max_waiting_in_queue_millis: 0active_shards_percent_as_number: 50.0Cluster State显示集群状态，官方文档见这里查看版本信息123456curl -XGET \"http://elasticsearch-1:9200/_cluster/state/version?\" &#123; \"cluster_name\" : \"elasticsearch\", \"version\" : 16, \"state_uuid\" : \"w0Bp1nGfRdCGxhA5zD1BBw\"&#125;查看master节点12345curl -XGET \"http://elasticsearch-1:9200/_cluster/state/master_node?format=json&amp;pretty\"&#123; \"cluster_name\" : \"elasticsearch\", \"master_node\" : \"JLyKyWH_QnSiXXuRNCj_3g\"&#125;查看节点123456789101112curl -XGET \"http://elasticsearch-1:9200/_cluster/state/nodes?format=json&amp;pretty\"&#123; \"cluster_name\" : \"elasticsearch\", \"nodes\" : &#123; \"JLyKyWH_QnSiXXuRNCj_3g\" : &#123; \"name\" : \"elasticsearch-1\", \"ephemeral_id\" : \"qdnVkiizToCVPhmDieH3Mg\", \"transport_address\" : \"172.16.80.201:9300\", \"attributes\" : &#123; &#125; &#125; &#125;&#125;Cluster States显示集群统计信息，官方文档见这里1curl -XGET \"http://elasticsearch-1:9200/_cluster/stats?format=json&amp;pretty\"Cluster Update Settings用于更新集群设置，可以永久persistent和临时transient设置某些参数新启动的集群是没有配置的信息12345curl -XGET \"http://elasticsearch-1:9200/_cluster/settings?format=json&amp;pretty\"&#123; \"persistent\" : &#123; &#125;, \"transient\" : &#123; &#125;&#125;临时修改集群recovery最大速度123456789curl -XPUT \\ -H \"Content-Type: application/json\" \\ \"http://elasticsearch-1:9200/_cluster/settings?format=json&amp;pretty\" \\ -d '&#123; \"persistent\" : &#123; \"indices.recovery.max_bytes_per_sec\" : \"50mb\" &#125;&#125;'Node Stats1curl -XGET \"http://elasticsearch-1:9200/_nodes/stats?format=json&amp;pretty\"Node Info12345678910111213141516171819202122# 不带过滤参数，默认显示所有节点信息GET /_nodes# 获取所有节点信息GET /_nodes/_all# 获取当前节点信息GET /_nodes/_local# 获取master节点信息GET /_nodes/_master# 以主机名作为条件过滤节点，或者使用通配符匹配节点GET /_nodes/node_name_goes_hereGET /_nodes/node_name_goes_*# 可以使用IP地址加通配符过滤节点GET /_nodes/10.0.0.3,10.0.0.4GET /_nodes/10.0.0.*# 以role作为过滤条件GET /_nodes/_all,master:falseGET /_nodes/data:true,ingest:trueGET /_nodes/coordinating_only:true# 根据elasticsearch.yml文件定义的node.attr.rack属性过滤节点，例如`node.attr.rack: 2`GET /_nodes/rack:2GET /_nodes/ra*:2GET /_nodes/ra*:2*插件官方文档见这里二进制文件elasticsearch/bin/elasticsearch-plugin查看默认可用的插件12345678910111213141516171819202122232425262728elasticsearch-plugin install --helpInstall a pluginThe following official plugins may be installed by name: analysis-icu analysis-kuromoji analysis-phonetic analysis-smartcn analysis-stempel analysis-ukrainian discovery-azure-classic discovery-ec2 discovery-file discovery-gce ingest-attachment ingest-geoip ingest-user-agent lang-javascript lang-python mapper-attachments mapper-murmur3 mapper-size repository-azure repository-gcs repository-hdfs repository-s3 store-smb x-packCRUD新增索引12345678910curl -X PUT \"http://elasticsearch-1:9200/twitter\" -H 'Content-Type: application/json' -d'&#123; \"settings\" : &#123; \"index\" : &#123; \"number_of_shards\" : 3, \"number_of_replicas\" : 2 &#125; &#125;&#125;'删除索引1curl -X DELETE \"http://elasticsearch-1:9200/twitter\"获取索引1curl -X GET \"http://elasticsearch-1:9200/twitter\"更新索引123456789curl -X PUT \"http://elasticsearch-1:9200/twitter/_settings\" \\ -H 'Content-Type: application/json' \\ -d'&#123; \"index\" : &#123; \"number_of_replicas\" : 2 &#125;&#125;'新增文档1234567891011121314151617181920212223242526curl -XPUT \"http://elasticsearch-1:9200/students/class1/1?pretty\" -d '&#123; \"first_name\": \"Jing\", \"last_name\": \"Guo\", \"gender\": \"Male\", \"age\": 25, \"courses\": \"Xiang Long Shi Ba Zhang\"&#125;'curl -XPUT \"http://elasticsearch-1:9200/students/class1/2?pretty\" -d '&#123; \"first_name\": \"Rong\", \"last_name\": \"Huang\", \"gender\": \"Female\", \"age\": 23, \"courses\": \"Luo Ying Shen Jian Zhang\"&#125;'curl -XPUT \"http://elasticsearch-1:9200/students/class2/1?pretty\" -d '&#123; \"first_name\": \"Guo\", \"last_name\": \"Yang\", \"gender\": \"Male\", \"age\": 2, \"courses\": \"An Ran Xiao Hun Zhang\"&#125;'获取文档123456789101112131415161718192021222324252627282930curl -XGET \"http://elasticsearch-1:9200/students/class1/1?pretty\"&#123; \"_index\" : \"students\", \"_type\" : \"class1\", \"_id\" : \"1\", \"_version\" : 3, \"found\" : true, \"_source\" : &#123; \"first_name\" : \"Jing\", \"last_name\" : \"Guo\", \"gender\" : \"Male\", \"age\" : 25, \"courses\" : \"Xiang Long Shi Ba Zhang\" &#125;&#125;curl -XGET \"http://elasticsearch-1:9200/students/class1/2?pretty\"&#123; \"_index\" : \"students\", \"_type\" : \"class1\", \"_id\" : \"2\", \"_version\" : 1, \"found\" : true, \"_source\" : &#123; \"first_name\" : \"Rong\", \"last_name\" : \"Huang\", \"gender\" : \"Female\", \"age\" : 23, \"courses\" : \"Luo Ying Shen Jian Zhang\" &#125;&#125;更新文档直接PUT覆盖使用_updateAPI12345678curl -XPOST \"http://elasticsearch-1:9200/students/class1/2/_update?pretty\" \\ -H \"Accept: application/json\" \\ -d '&#123; \"doc\" : &#123; \"age\" : 22 &#125;&#125;'删除文档1curl -XDELETE \"http://elasticsearch-1:9200/students/class1/2?pretty\"查询数据Query DSL官方文档查询操作阶段分散阶段合并阶段查询方式通过Restful API查询，查询参数见官方文档1curl -XGET \"http://elasticsearch-1:9200/students/_search?pretty\"通过发送REST request body查询，查询参数见官方文档1234567891011121314151617181920212223242526# 匹配所有curl -XGET \"http://elasticsearch-1:9200/students/_search?pretty\" -d '&#123; \"query\": &#123; \"match_all\": &#123;&#125; &#125;&#125;'# 匹配年龄curl -XGET \"http://elasticsearch-1:9200/students/_search?pretty\" -d '&#123; \"query\":&#123; \"bool\":&#123; \"must\":[ &#123; \"range\":&#123; \"age\":&#123; \"gt\":\"21\", \"lt\":\"24\" &#125; &#125; &#125; ] &#125; &#125;&#125;'多索引、多类型查询_search：所有索引/INDEX_NAME/_search：单索引/INDEX1,INDEX2/_search：多索引/students/class1/_search：单类型搜索/students/class1,class2/_search：多类型搜索Mapping和AnalysisElasticsearch对每个文档，会取得所有域的所有值，生成一个名为“_all”的域执行查询的时候，如果未指定query_string，则在“_all”域上执行查询操作1234# 在_all域搜索curl /_search?q=\"Keyword\"# 在指定的FIELD_NAME这个域搜索curl /_search?q=FIELD_NAME:\"Keyword\"数据类型见官方文档arraybinaryrangebooleandategeo-pointgeo-shapeIPkeywordnestednumericobjecttexttoken countpercolatrojoin","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://luanlengli.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://luanlengli.github.io/tags/Elasticsearch/"}]},{"title":"Elasticsearch-5.6.x安装配置","slug":"Elasticsearch-5-6-x安装配置","date":"2019-02-21T09:57:55.000Z","updated":"2019-06-23T16:36:19.000Z","comments":true,"path":"2019/02/21/Elasticsearch-5-6-x安装配置.html","link":"","permalink":"https://luanlengli.github.io/2019/02/21/Elasticsearch-5-6-x安装配置.html","excerpt":"","text":"系统环境CentOS-7准备两台服务器，做elasticsearch集群1yum update -yhosts静态解析123127.0.0.1 localhost172.16.80.201 elasticsearch-1172.16.80.202 elasticsearch-2Oracle JDK-8u20112345678910111213141516# 切换工作目录cd /usr/local/# 下载JDK并解压wget --no-check-certificate \\ -O - \\ -c \\ --header \"Cookie: oraclelicense=accept-securebackup-cookie\" \"https://download.oracle.com/otn-pub/java/jdk/8u201-b09/42970487e3af4f5aa5bca3f542482c60/jdk-8u201-linux-x64.tar.gz\" \\ | tar xz # 创建软链接ln -sv /usr/local/jdk1.8.0_201 /usr/local/jdk# 设置环境变量cat &gt; /etc/profile.d/java.sh &lt;&lt;EOFexport JAVA_HOME=/usr/local/jdkexport PATH=$JAVA_HOME/bin:$PATH EOFsource /etc/profile.d/java.sh创建elasticsearch用户1useradd esuser修改limits1234cat &gt; /etc/security/limits.d/esuser.conf &lt;&lt;EOFesuser - nofile 262144esuser - memlock unlimitedEOF修改内核参数123456sysctl -w vm.max_map_count=262144cat &gt; /etc/sysctl.d/elasticsearch.conf &lt;&lt;EOFvm.max_map_count=262144# 内存耗尽才使用swap分区vm.swappiness = 0EOF安装elasticsearch下载elasticsearch12345cd /usr/local/wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.6.15.tar.gztar xzf elasticsearch-5.6.15.tar.gzln -sv /usr/local/elasticsearch-5.6.15 /usr/local/elasticsearchchown -R esuser:esuser /usr/local/elasticsearch /usr/local/elasticsearch-5.6.15配置elasticsearch修改config/elasticsearch.yml1234567891011121314151617181920212223# 定义节点名字node.name: elasticsearch-1# 定义数据目录path.data: /home/esuser/data# 定义日志目录path.logs: /home/esuser/logs# 定义节点为masternode.master: true# 定义节点为datanode.data: true# 启动时锁定内存，避免内存被系统swap# 看情况开启，默认是关闭的#bootstrap.memory_lock: true# 定义监听端口network.host: 0.0.0.0# 定义服务端口http.port: 9200# 定义单播主机discovery.zen.ping.unicast.hosts:- elasticsearch-1- elasticsearch-2http.cors.enabled: truehttp.cors.allow-origin: \"*\"修改config/jvm.options这里只需要修改-Xms和-Xmx，默认是2g，最大不超过32g，两个选项的值保持一致设置elasticsearch系统变量123cat &gt; /home/esuser/elasticsearch &lt;&lt;EOFJAVA_HOME=/usr/local/jdkEOF添加PATH123cat &gt; /etc/profile.d/elasticsearch.sh &lt;&lt;EOFexport ES_HOME=\"/usr/local/elasticsearch\"export PATH=$ES_HOME/bin:$PATH创建systemd服务脚本1234567891011121314[Unit]Description=ElasticSearch ServiceAfter=network.target[Service]Type=simpleEnvironmentFile=-/home/esuser/elasticsearchExecStart=/usr/local/elasticsearch/bin/elasticsearchPIDFile=/usr/local/elasticsearch/run/elasticsearch.pidUser=esuserLimitNOFILE=262144LimitMEMLOCK=infinityRestart=on-failure[Install]WantedBy=default.target启动elasticsearch通过systemd命令启动12systemctl daemon-reloadsystemctl enable --now elasticsearch命令行启动1su -s /bin/sh -c \"/usr/local/elasticsearch/bin/elasticsearch -d\" esuser访问elasticsearch1234567891011121314curl http://127.0.0.1:9200/&#123; \"name\" : \"elasticsearch-1\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"JN0WSrAZQo2qWCaZrbDGjg\", \"version\" : &#123; \"number\" : \"5.6.15\", \"build_hash\" : \"fe7575a\", \"build_date\" : \"2019-02-13T16:21:45.880Z\", \"build_snapshot\" : false, \"lucene_version\" : \"6.6.1\" &#125;, \"tagline\" : \"You Know, for Search\"&#125;安装elasticsearch-head直接用容器启动1docker run --net=host --restart=always -d --name elasticsearch-head mobz/elasticsearch-head:5-alpine访问elasticsearch-headcurl -I http://127.0.0.1:9100","categories":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://luanlengli.github.io/categories/Elasticsearch/"}],"tags":[{"name":"Elasticsearch","slug":"Elasticsearch","permalink":"https://luanlengli.github.io/tags/Elasticsearch/"}]},{"title":"MySQL5.7学习笔记","slug":"MySQL5-7学习笔记","date":"2019-02-15T10:33:01.000Z","updated":"2019-05-31T08:15:33.000Z","comments":true,"path":"2019/02/15/MySQL5-7学习笔记.html","link":"","permalink":"https://luanlengli.github.io/2019/02/15/MySQL5-7学习笔记.html","excerpt":"","text":"本文基于MySQL 5.7记录一、安装1、二进制安装1234567891011121314151617181920212223# 创建mysql相关的组和用户groupadd mysqluseradd -r -g mysql -s /bin/false mysql# 下载二进制包cd /usr/local/wget --no-check-certificate https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz# 解压tar xvzf mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz# 将目录移动到/usr/local/mysqlln -s mysql-5.7.24-linux-glibc2.12-x86_64 /usr/local/mysql# 创建数据库数据目录mkdir -p /path/to/mysql_data_dirchown -R mysql:mysql /path/to/mysql_data_dir# 使用叶金荣的配置生成工具创建my.cnf文件放在/etc/my.cnf# http://imysql.com/my-cnf-wizard.htmlchmod a+r /etc/my.cnfchown mysql:mysql /etc/my.cnf/usr/local/mysql/bin/mysqld --defaults-file=/etc/my.cnf --initialize --user=mysql --basedir=/usr/local/mysql/ --datadir=/mysql_data/usr/local/mysql/bin/mysql_ssl_rsa_setup --datadir=/mysql_data# 查看error.log文件里面的root密码grep password error.log | awk '&#123;print$NF&#125;'# 启动MySQLmysqld_safe --user=mysql &amp;2、rpm/deb软件源安装以CentOS7为例12345678910# 获取MySQL软件源yum install -y https://dev.mysql.com/get/mysql57-community-release-el7-1.noarch.rpm# 更新YUM缓存yum makecache# 安装MySQL5.7yum install mysql-community-server# 启动MySQLsystemctl start mysqld.service# 获取ROOT密码grep 'temporary password' /var/log/mysqld.log3、源码安装以CentOS7为例123456789101112131415161718192021222324252627282930313233343536373839404142# 安装编译环境yum -y install make gcc-c++ cmake bison-devel ncurses-devel# 下载MySQL源代码wget --no-check-certificate https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-boost-5.7.24.tar.gz# 创建MySQL相关的组和用户groupadd mysqluseradd -r -g mysql -s /bin/false mysql# 创建MySQL相关文件夹mkdir -p /app/mysqlmkdir -p /app/datamkdir -p /app/etcmkdir -p /app/logchown -R mysql:mysql /app# 解压源代码tar xvzf mysql-boost-5.7.24.tar.gz# 创建cmake编译目录mkdir buildcd build# 使用CMake生成makefilecmake -DCMAKE_INSTALL_PREFIX=/app/mysql/ \\-DMYSQL_USER=mysql \\-DMYSQL_TCP_PORT=3306 \\-DMYSQL_UNIX_ADDR=/app/data/mysql.sock \\-DDEFAULT_CHARSET=utf8mb4 \\-DDEFAULT_COLLATION=utf8_general_ci \\-DEXTRA_CHARSETS=all \\-DWITH_READLINE=1 \\-DWITH_SYSTEMD=1 \\-DWITH_BOOST=/root/mysql-5.7.24/boost/boost_1_59_0/ \\/root/mysql-5.7.24/# 编译安装make &amp;&amp; make install# 使用叶金荣MySQL配置生成工具生成my.cnf放在/app/etc/my.cnf# http://imysql.com/my-cnf-wizard.htmlchown mysql:mysql /app/etc/my.cnf# 初始化MySQL数据库mysqld --initialize --defaults-file=/app/etc/my.cnf --user=mysql --basedir=/app/mysql/ --datadir=/app/data/mysql_ssl_rsa_setup --datadir=/app/data/# 启动MySQL数据库mysqld_safe --defaults-file=/app/etc/my.cnf --user=mysql &amp; # 获取MySQL ROOT密码grep password error.log | awk '&#123;print$NF&#125;'4、安装后设定4.1、安全设置1mysql_secure_installation4.2、建议设置关闭主机名反向解析，在配置文件的[mysqld]里面添加skip_name_resolve = 11234vi /etc/my.cnf[mysqld]...skip_name_resolve = 15、数据库连接5.1、mysql命令行客户端1mysql -uroot -ppassword -h127.0.0.1 -P3306 -D db15.2、MySQL Benchmark图形客户端官方网站下载页面6、获取MySQL编译的参数在MySQL目录的docs/INFO_BIN12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# 例如安装目录为/usr/local/mysql# MySQL官方YUM源安装的路径为/usr/share/doc/packages/MySQL-servercat /usr/local/bin/mysql/docs/INFO_BIN===== Information about the build process: =====Build was run at 2018-10-04 08:26:03 on host 'vitro45'Build was done on Linux-3.8.13-16.2.1.el6uek.x86_64 using x86_64Build was done using cmake 2.8.12 ===== Compiler flags used (from the 'sql/' subdirectory): =====# compile C with /usr/bin/cc# compile CXX with /usr/bin/c++C_FLAGS = -fPIC -Wall -Wextra -Wformat-security -Wvla -Wwrite-strings -Wdeclaration-after-statement -O3 -g -fabi-version=2 -fno-omit-frame-pointer -fno-strict-aliasing -DDBUG_OFF -I/export/home/pb2/build/sb_0-30854123-1538633287.09/release/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/extra/rapidjson/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/release/libbinlogevents/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/libbinlogevents/export -isystem /export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/zlib -isystem /export/home/pb2/build/sb_0-30854123-1538633287.09/release/zlib -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/sql/conn_handler -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/libbinlogevents/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/sql -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/sql/auth -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/regex -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/extra/yassl/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/extra/yassl/taocrypt/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/release/sql -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/extra/lz4 -isystem /export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/include/boost_1_59_0/patches -isystem /usr/global/share/boost_1_59_0 -DHAVE_YASSL -DYASSL_PREFIX -DHAVE_OPENSSL -DMULTI_THREADEDC_DEFINES = -DHAVE_CONFIG_H -DHAVE_LIBEVENT1 -DHAVE_REPLICATION -DMYSQL_SERVER -D_FILE_OFFSET_BITS=64 -D_GNU_SOURCECXX_FLAGS = -fPIC -Wall -Wextra -Wformat-security -Wvla -Woverloaded-virtual -Wno-unused-parameter -O3 -g -fabi-version=2 -fno-omit-frame-pointer -fno-strict-aliasing -DDBUG_OFF -I/export/home/pb2/build/sb_0-30854123-1538633287.09/release/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/extra/rapidjson/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/release/libbinlogevents/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/libbinlogevents/export -isystem /export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/zlib -isystem /export/home/pb2/build/sb_0-30854123-1538633287.09/release/zlib -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/sql/conn_handler -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/libbinlogevents/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/sql -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/sql/auth -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/regex -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/extra/yassl/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/extra/yassl/taocrypt/include -I/export/home/pb2/build/sb_0-30854123-1538633287.09/release/sql -I/export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/extra/lz4 -isystem /export/home/pb2/build/sb_0-30854123-1538633287.09/mysql-5.7.24/include/boost_1_59_0/patches -isystem /usr/global/share/boost_1_59_0 -DHAVE_YASSL -DYASSL_PREFIX -DHAVE_OPENSSL -DMULTI_THREADEDCXX_DEFINES = -DHAVE_CONFIG_H -DHAVE_LIBEVENT1 -DHAVE_REPLICATION -DMYSQL_SERVER -D_FILE_OFFSET_BITS=64 -D_GNU_SOURCEPointer size: 8===== Feature flags used: =====-- Cache valuesBOOST_INCLUDE_DIR:PATH=/usr/global/share/boost_1_59_0BUILD_TESTING:BOOL=ONBUNDLE_MECAB:BOOL=ONCMAKE_BACKWARDS_COMPATIBILITY:STRING=2.4CMAKE_BUILD_TYPE:STRING=RelWithDebInfoCMAKE_INSTALL_PREFIX:PATH=/usr/local/mysqlCOMMUNITY_BUILD:BOOL=ONCTAGS_EXECUTABLE:FILEPATH=/usr/bin/ctagsDEB_CHANGELOG_TIMESTAMP:STRING=Thu, 04 Oct 2018 08:14:16 +0200DEB_CODENAME:STRING=n/aDOWNLOAD_BOOST:BOOL=OFFDOWNLOAD_BOOST_TIMEOUT:STRING=600ENABLED_PROFILING:BOOL=ONENABLE_DOWNLOADS:BOOL=OFFENABLE_GCOV:BOOL=OFFENABLE_GPROF:BOOL=OFFENABLE_MEMCACHED_SASL:BOOL=OFFENABLE_MEMCACHED_SASL_PWDB:BOOL=OFFEXECUTABLE_OUTPUT_PATH:PATH=FEATURE_SET:STRING=communityINSTALL_LAYOUT:STRING=STANDALONEINSTALL_PKGCONFIGDIR:PATH=LIBRARY_OUTPUT_PATH:PATH=LOCAL_BOOST_DIR:FILEPATH=/usr/global/share/boost_1_59_0LOCAL_BOOST_ZIP:FILEPATH=/usr/global/share/boost_1_59_0.tar.gzLOCAL_GMOCK_ZIP:FILEPATH=/usr/global/share/googletest-release-1.8.0.zipMECAB_INCLUDE_DIR:PATH=/export/home/pb2/build/sb_0-30854123-1538633287.09/mecab-0.996-el6-x86-64bit/includeMECAB_LIBRARY:FILEPATH=/export/home/pb2/build/sb_0-30854123-1538633287.09/mecab-0.996-el6-x86-64bit/lib/libmecab.aMERGE_UNITTESTS:BOOL=ONMUTEXTYPE:STRING=eventMYSQL_DATADIR:PATH=/usr/local/mysql/dataMYSQL_KEYRINGDIR:PATH=/usr/local/mysql/keyringMYSQL_MAINTAINER_MODE:BOOL=OFFOPTIMIZER_TRACE:BOOL=ONREPRODUCIBLE_BUILD:BOOL=OFFRPC_INCLUDE_DIR:PATH=/usr/includeSASL_SYSTEM_LIBRARY:FILEPATH=/usr/lib64/libsasl2.soTMPDIR:PATH=P_tmpdirWITH_ARCHIVE_STORAGE_ENGINE:BOOL=ONWITH_ASAN:BOOL=OFFWITH_ASAN_SCOPE:BOOL=OFFWITH_BLACKHOLE_STORAGE_ENGINE:BOOL=ONWITH_BOOST:PATH=/usr/global/shareWITH_CLIENT_PROTOCOL_TRACING:BOOL=ONWITH_DEBUG:BOOL=OFFWITH_DEFAULT_COMPILER_OPTIONS:BOOL=ONWITH_DEFAULT_FEATURE_SET:BOOL=ONWITH_EDITLINE:STRING=bundledWITH_EMBEDDED_SERVER:BOOL=ONWITH_EMBEDDED_SHARED_LIBRARY:BOOL=OFFWITH_EXTRA_CHARSETS:STRING=allWITH_FEDERATED_STORAGE_ENGINE:BOOL=ONWITH_INNODB_EXTRA_DEBUG:BOOL=OFFWITH_INNODB_MEMCACHED:BOOL=1WITH_LIBEVENT:STRING=bundledWITH_LIBWRAP:BOOL=OFFWITH_LZ4:STRING=bundledWITH_MECAB:STRING=/export/home/pb2/build/sb_0-30854123-1538633287.09/mecab-0.996-el6-x86-64bitWITH_MECAB_PATH:PATH=/export/home/pb2/build/sb_0-30854123-1538633287.09/mecab-0.996-el6-x86-64bitWITH_MSAN:BOOL=OFFWITH_NGRAM_PARSER:BOOL=ONWITH_NUMA:BOOL=ONWITH_PARTITION_STORAGE_ENGINE:BOOL=ONWITH_PIC:BOOL=ONWITH_RAPID:BOOL=ONWITH_SASL:STRING=systemWITH_SSL:STRING=bundledWITH_SYSTEMD:BOOL=OFFWITH_TEST_TRACE_PLUGIN:BOOL=OFFWITH_UBSAN:BOOL=OFFWITH_UNIT_TESTS:BOOL=ONWITH_VALGRIND:BOOL=OFFWITH_ZLIB:STRING=bundledXPLUGIN_LOG_PROTOBUF:STRING=1===== EOF =====二、配置数据库1、配置文件路径文件优先级，同一个参数以最后一个为准，启动时可指定–default-file=/path/to/my.cnf/etc/my.cnf/etc/mysql/my.cnf$MYSQL_HOME/my.cnf–default-extra-file=/path/to/*.cnf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139[client]port = 3306socket = /data/mysql/mysql.sock[mysql]prompt=&quot;\\u@mysqldb \\R:\\m:\\s [\\d]&gt; &quot;no-auto-rehash[mysqld]user = mysqlport = 3306basedir = /usr/local/mysqldatadir = /data/mysql/socket = /data/mysql/mysql.sockpid-file = mysqldb.pidcharacter-set-server = utf8mb4skip_name_resolve = 1open_files_limit = 65535back_log = 1024max_connections = 512max_connect_errors = 1000000table_open_cache = 1024table_definition_cache = 1024table_open_cache_instances = 64thread_stack = 512Kexternal-locking = FALSEmax_allowed_packet = 32Msort_buffer_size = 16Mjoin_buffer_size = 16Mthread_cache_size = 768interactive_timeout = 600wait_timeout = 600tmp_table_size = 96Mmax_heap_table_size = 96Mslow_query_log = 1slow_query_log_file = /data/mysql/slow.loglog-error = /data/mysql/error.loglong_query_time = 0.1log_queries_not_using_indexes =1log_throttle_queries_not_using_indexes = 60min_examined_row_limit = 100log_slow_admin_statements = 1log_slow_slave_statements = 1server-id = 3306log-bin = /data/mysql/mybinlogsync_binlog = 1binlog_cache_size = 4Mmax_binlog_cache_size = 2Gmax_binlog_size = 1Gexpire_logs_days = 7master_info_repository = TABLErelay_log_info_repository = TABLEgtid_mode = onenforce_gtid_consistency = 1log_slave_updatesslave-rows-search-algorithms = &apos;INDEX_SCAN,HASH_SCAN&apos;binlog_format = rowbinlog_checksum = 1relay_log_recovery = 1relay-log-purge = 1key_buffer_size = 32Mread_buffer_size = 8Mread_rnd_buffer_size = 16Mbulk_insert_buffer_size = 64Mmyisam_sort_buffer_size = 128Mmyisam_max_sort_file_size = 10Gmyisam_repair_threads = 1lock_wait_timeout = 3600explicit_defaults_for_timestamp = 1innodb_thread_concurrency = 0innodb_sync_spin_loops = 100innodb_spin_wait_delay = 30transaction_isolation = REPEATABLE-READ#innodb_additional_mem_pool_size = 16Minnodb_buffer_pool_size = 45875Minnodb_buffer_pool_instances = 4innodb_buffer_pool_load_at_startup = 1innodb_buffer_pool_dump_at_shutdown = 1innodb_data_file_path = ibdata1:1G:autoextendinnodb_flush_log_at_trx_commit = 1innodb_log_buffer_size = 32Minnodb_log_file_size = 2Ginnodb_log_files_in_group = 2innodb_max_undo_log_size = 2Ginnodb_undo_directory = undologinnodb_undo_tablespaces = 8# 根据您的服务器IOPS能力适当调整# 一般配普通SSD盘的话，可以调整到 10000 - 20000# 配置高端PCIe SSD卡的话，则可以调整的更高，比如 50000 - 80000innodb_io_capacity = 4000innodb_io_capacity_max = 8000innodb_flush_neighbors = 0innodb_write_io_threads = 8innodb_read_io_threads = 8innodb_purge_threads = 4innodb_page_cleaners = 4innodb_open_files = 65535innodb_max_dirty_pages_pct = 50innodb_flush_method = O_DIRECTinnodb_lru_scan_depth = 4000innodb_checksum_algorithm = crc32innodb_lock_wait_timeout = 10innodb_rollback_on_timeout = 1innodb_print_all_deadlocks = 1innodb_file_per_table = 1innodb_online_alter_log_max_size = 2Ginternal_tmp_disk_storage_engine = InnoDBinnodb_stats_on_metadata = 0# some var for MySQL 8log_error_verbosity = 3innodb_print_ddl_logs = 1binlog_expire_logs_seconds = 604800#innodb_dedicated_server = 0innodb_status_file = 1# 注意: 开启 innodb_status_output &amp; innodb_status_output_locks 后, 可能会导致log-error文件增长较快innodb_status_output = 0innodb_status_output_locks = 0#performance_schemaperformance_schema = 1performance_schema_instrument = &apos;%=on&apos;#innodb monitorinnodb_monitor_enable=&quot;module_innodb&quot;innodb_monitor_enable=&quot;module_server&quot;innodb_monitor_enable=&quot;module_dml&quot;innodb_monitor_enable=&quot;module_ddl&quot;innodb_monitor_enable=&quot;module_trx&quot;innodb_monitor_enable=&quot;module_os&quot;innodb_monitor_enable=&quot;module_purge&quot;innodb_monitor_enable=&quot;module_log&quot;innodb_monitor_enable=&quot;module_lock&quot;innodb_monitor_enable=&quot;module_buffer&quot;innodb_monitor_enable=&quot;module_index&quot;innodb_monitor_enable=&quot;module_ibuf_system&quot;innodb_monitor_enable=&quot;module_buffer_page&quot;innodb_monitor_enable=&quot;module_adaptive_hash&quot;[mysqld_safe][mysqld_multi][mysqldump]quickmax_allowed_packet = 32M[server]2、数据库变量2.1、查看数据库全局变量1mysql&gt; show global variables;2.2、查看数据库会话变量1mysql&gt; show session variables;3、开启MySQL SSL功能1234567891011121314151617# 使用MySQL自带工具生成证书mysql_ssl_rsa_setup --datadir=MYSQL_DATA_DIR# 证书默认存放data-dirll /mysql_data/*pem-rw------- 1 root root 1675 Oct 30 13:43 /mysql_data/ca-key.pem-rw-r--r-- 1 root root 1107 Oct 30 13:43 /mysql_data/ca.pem-rw-r--r-- 1 root root 1107 Oct 30 13:43 /mysql_data/client-cert.pem-rw------- 1 root root 1679 Oct 30 13:43 /mysql_data/client-key.pem-rw------- 1 root root 1675 Oct 30 13:43 /mysql_data/private_key.pem-rw-r--r-- 1 root root 451 Oct 30 13:43 /mysql_data/public_key.pem-rw-r--r-- 1 root root 1107 Oct 30 13:43 /mysql_data/server-cert.pem-rw------- 1 root root 1675 Oct 30 13:43 /mysql_data/server-key.pem# 配置文件指明SSL证书路径[mysqld]ssl-ca=/mysql_data/ca.pemssl-cert=/mysql_data/server-cert.pemssl-key=/mysql_data/server-key.pem123456789101112131415161718192021# 查看数据库SSL变量mysql&gt; show global variables like &apos;%ssl%&apos;;+---------------+-----------------------------+| Variable_name | Value |+---------------+-----------------------------+| have_openssl | DISABLED || have_ssl | DISABLED || ssl_ca | /mysql_data/ca.pem || ssl_capath | || ssl_cert | /mysql_data/server-cert.pem || ssl_cipher | || ssl_crl | || ssl_crlpath | || ssl_key | /mysql_data/server-key.pem |+---------------+-----------------------------+# 强制用户使用SSL登录mysql&gt; alter user &apos;username&apos;@&apos;host&apos; require ssl;# 创建用户时要求使用SSL登录mysql&gt; grant all privileges on dbname.tablename to &apos;username&apos;@&apos;host&apos; identified by &apos;password&apos; require ssl;# 取消强制用户使用SSL登录mysql&gt; alter user &apos;username&apos;@&apos;host&apos; require none;12345# 使用SSL证书登录数据库mysql -uUSERNAME -pPASSWORD -hHOSTNAME \\--ssl-ca=ca.pem \\--ssl-cert=client-cert.pem \\--ssl-key=client-key.pem三、事务隔离一组原子性SQL操作，或者是独立的工作单元。1、ACID1.1、Atomicity原子性整个事务中的所有操作要么全部执行成功，要么失败后完全回滚。1.2、Consistency一致性数据库总是从一个一致性状态转换到另一个一致性状态。1.3、Isolation隔离性一个事务的操作在提交之前，不能被其他会话所见。可分为多个隔离级别1.4、Durability永久性事务一旦提交，操作结果将永久保存到数据库中。2、锁机制2.1、写锁其他事务不能读取，也不能写。2.2、读锁其他事务可以读，但不能写。3、脏读脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。4、不可重复读是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。5、幻读指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。6、事务隔离级别默认是REPEATEABLE READ 可重复读6.1、READ UNCOMMITED读未提交事务间完全不隔离，会产生脏读，可以读取未提交的记录。6.2、READ COMMITED读已提交仅能读取到已提交的记录，不会脏读，会不可重复读，会幻读。6.3、REPEATABLE READ可重复读每次读取的结果集都相同，而不管其他事务有没有提交，但是无法阻止其他事务插入数据，因此可能导致幻读。6.4、SERIALIZABILE可串行化最严格的锁，在事务完成前，其他事务无法对数据对象进行写操作。并行性能最差。四、数据库日志1、查询日志 query log记录查询操作，产生额外IO消耗，一般不打开。12345678910111213mysql&gt; show global variables like &apos;%general%&apos;;+------------------+---------------------------+| Variable_name | Value |+------------------+---------------------------+| general_log | OFF || general_log_file | /var/log/hostname.log |+------------------+---------------------------+mysql&gt; show global variables like &apos;log_output&apos;;+---------------+-------+| Variable_name | Value |+---------------+-------+| log_output | FILE |+---------------+-------+2、慢查询日志 slow query log执行时长超过指定时长的查询操作12345678910111213141516mysql&gt; show global variables like &apos;long_query_time&apos;;+-----------------+----------+| Variable_name | Value |+-----------------+----------+| long_query_time | 0.100000 |+-----------------+----------+&gt; show global variables like &apos;%slow%&apos;;+---------------------------+-------------------------+| Variable_name | Value |+---------------------------+-------------------------+| log_slow_admin_statements | ON || log_slow_slave_statements | ON || slow_launch_time | 2 || slow_query_log | ON || slow_query_log_file | /var/log/mysql/slow.log |+---------------------------+-------------------------+3、错误日志 error log记录MySQL启动关闭过程输出的日志记录MySQL运行过程中产生的错误日志记录event scheduler运行event时产生的日志记录主从复制架构中从服务器上启动从服务器线程时产生的日志1234567891011&gt; show global variables like &apos;%error%&apos;;+---------------------+--------------------------+| Variable_name | Value |+---------------------+--------------------------+| binlog_error_action | ABORT_SERVER || log_error | /var/log/mysql/error.log || log_error_verbosity | 3 || max_connect_errors | 1000000 || max_error_count | 64 || slave_skip_errors | OFF |+---------------------+--------------------------+4、二进制日志 binary log4.1、介绍记录对mysql数据更新或潜在发生更新的SQL语句，并以”事务”的形式保存在磁盘中。用于通过“重放”日志生成数据副本。4.2、日志记录内容基于“语句”记录：statement基于“行”记录：row混合模式：mixed，由数据库自动判定4.3、日志文件构成日志文件：mysql-bin，二进制格式索引文件：mysql-bin.index，文本格式1234567891011121314151617181920212223242526272829303132mysql &gt; show global variables like &apos;%bin%&apos;;+--------------------------------------------+----------------------------+| Variable_name | Value |+--------------------------------------------+----------------------------+| binlog_cache_size | 4194304 || binlog_checksum | CRC32 || binlog_direct_non_transactional_updates | OFF || binlog_error_action | ABORT_SERVER || binlog_format | ROW || binlog_group_commit_sync_delay | 0 || binlog_group_commit_sync_no_delay_count | 0 || binlog_gtid_simple_recovery | ON || binlog_max_flush_queue_time | 0 || binlog_order_commits | ON || binlog_row_image | FULL || binlog_rows_query_log_events | OFF || binlog_stmt_cache_size | 32768 || binlog_transaction_dependency_history_size | 25000 || binlog_transaction_dependency_tracking | COMMIT_ORDER || innodb_api_enable_binlog | OFF || innodb_locks_unsafe_for_binlog | OFF || log_bin | ON || log_bin_basename | /mysql_data/mybinlog || log_bin_index | /mysql_data/mybinlog.index || log_bin_trust_function_creators | OFF || log_bin_use_v1_row_events | OFF || log_statements_unsafe_for_binlog | ON || max_binlog_cache_size | 2147483648 || max_binlog_size | 1073741824 || max_binlog_stmt_cache_size | 18446744073709547520 || sync_binlog | 1 |+--------------------------------------------+----------------------------+12345678910111213141516171819mysql &gt; show binary logs;+-----------------+-----------+| Log_name | File_size |+-----------------+-----------+| mybinlog.000001 | 177 || mybinlog.000002 | 11841 |+-----------------+-----------+mysql &gt; show master status;+-----------------+----------+--------------+------------------+-------------------------------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+-----------------+----------+--------------+------------------+-------------------------------------------+| mybinlog.000002 | 11841 | | | 2c719e09-d6a4-11e8-b4f9-560000590d55:1-31 |+-----------------+----------+--------------+------------------+-------------------------------------------+mysql &gt; show binlog events in &apos;mybinlog.000002&apos; from 4 limit 1;+-----------------+-----+-------------+-----------+-------------+---------------------------------------+| Log_name | Pos | Event_type | Server_id | End_log_pos | Info |+-----------------+-----+-------------+-----------+-------------+---------------------------------------+| mybinlog.000002 | 4 | Format_desc | 3306 | 123 | Server ver: 5.7.24-log, Binlog ver: 4 |+-----------------+-----+-------------+-----------+-------------+---------------------------------------+4.4、mysqlbinlog工具读取二进制日志基于时间点1mysqlbinlog -uroot -pxvnCv3Yz -hlocalhost -P3306 --start-datetime='2018-10-23 00:00:00' --stop-datetime='2018-10-24 13:00:00' -d myblog /mysql_data/mybinlog.000002基于position1mysqlbinlog -uroot -pxvnCv3Yz -hlocalhost -P3306 --start-position=10613 --stop-position=11684 -d myblog /mysql_data/mybinlog.0000024.5、二进制日志事件的格式1234567891011121314# at 11305#181024 11:38:30 server id 3306 end_log_pos 11684 CRC32 0xf0b28827 Query thread_id=32 exec_time=0 error_code=0SET TIMESTAMP=1540352310/*!*/;CREATE TABLE `vote_record_memory` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_id` varchar(20) NOT NULL, `vote_id` int(11) NOT NULL, `group_id` int(11) NOT NULL, `create_time` datetime NOT NULL, PRIMARY KEY (`id`), KEY `index_user_id` (`user_id`)) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4/*!*/;事件发生的日期和时间： 181024 11:38:30事件发生的服务器标识： server id 3306事件结束位置： end_log_pos 11684事件类型： Query事件发生时所在服务器执行此事件的线程ID： thread_id=32语句的时间戳与写入二进制文件的时间差： exec_time=0错误代码： error_code=0事件内容1234567891011SET TIMESTAMP=1540352310/*!*/;CREATE TABLE `vote_record_memory` ( `id` int(11) NOT NULL AUTO_INCREMENT, `user_id` varchar(20) NOT NULL, `vote_id` int(11) NOT NULL, `group_id` int(11) NOT NULL, `create_time` datetime NOT NULL, PRIMARY KEY (`id`), KEY `index_user_id` (`user_id`)) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4/*!*/;GTID：Global Transaction ID；全局事务ID5、中继日志 relay log主从复制架构中，从服务器将主服务器的二进制日志事件拷贝到自己的中继日志。6、事务日志 transaction log由数据库引擎自行管理。当有更新操作时，存储引擎只将数据在内存中的copy修改成更新后的值，但并不将数据的更新刷新的硬盘，只是将更新操作的行为记录到硬盘上的事务日志中。因为事务日志的记录是采用文件追加的方式，因此写日志的操作是磁盘上一小块区域内的顺序I/O，而不像随机I/O需要在磁盘的多个地方移动磁头。因此事务日志的记录是非常快的。事务日志由多个日志文件组组成，循环使用日志文件。事务日志的大小需要根据实际情况设定。6.1、重做日志redo log用于记录修改后的数据，顺序记录，可以根据这个文件的记录内容重新恢复数据。6.2、回滚日志undo log用于存放被修改前的数据，回滚操作，实现事务一致性。五、备份还原1、为什么要备份灾难备份硬件故障软件故障自然灾害黑客攻击人为操作业务测试2、注意事项能容忍丢失多少数据备份操作持锁时长备份负载恢复数据的时长需要恢复哪些数据备份的有效性备份恢复测试和演练生产数据和备份数据分开存放3、备份类型热备：备份过程数据库可读可写温备：备份过程数据库可读不可写冷备：备份过程数据库不可读不可写全量备份：完整数据集增量备份：仅备份最近一次全备或者增备以来变化的数据差异备份：仅备份最近一次全备以来变化的数据物理备份：块级别备份，备份数据文件逻辑备份：从数据库导出数据，与存储引擎无关，可用于数据迁移4、备份内容数据二进制日志、InnoDB事务日志配置文件代码（存储过程、存储函数、触发器、事件调度器）5、逻辑备份工具Schema和数据存储在一个巨大的单个SQL语句文件5.1、mysqldumpMyISAM：备份库添加只读锁，直至备份完成默认导出所有数据库的所有表锁定方法（对InnoDB有效实现温备）：–lock-all-tables 锁定所有库的所有表–log-tables 对于每个单独的数据库，在启动备份前锁定其所有表InnoDB：支持热备、温备–single-transaction 设置本次会话隔离级别为Repeatable Read，确保本次会话不会看到其他会话提交的数据，保证数据一致性命令说明123456# 备份指定数据库或者数据库中的表mysqldump [options] db_name [table_name]# 备份多个数据库mysqldump [options] --database db1 [db2, db3]# 备份所有数据库mysqldump [options] --all-databases命令示例123456# 备份数据库db1，导出为db1.sqlmysqldump -uroot -ppassword -h127.0.0.1 -P3306 db1 &gt; db1.sql# 备份数据库db1的table1表，导出为table1.sqlmysqldump -uroot -ppassword -h127.0.0.1 -P3306 db1 table1 &gt; table1.sql# 备份数据库db1的table1和table2表，导出为table1_table2.sqlmysqldump -uroot -ppassword -h127.0.0.1 -P3306 db1 table1 table2 &gt; table1_table2.sql6、物理备份6.1、xtrabackup适用于InnoDB和XtraDB，一般使用innobackupex（xtrabackup的命令简化版）官方网站下载地址安装123456# 下载xtrabackup二进制包wget --no-check-certificate https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.12/binary/tarball/percona-xtrabackup-2.4.12-Linux-x86_64.libgcrypt11.tar.gz# 解压二进制包tar xvzf percona-xtrabackup-2.4.12-Linux-x86_64.libgcrypt11.tar.gz.tar.gzmv percona-xtrabackup-2.4.12-Linux-x86_64 /usr/local/ln -sv /usr/local/percona-xtrabackup-2.4.12-Linux-x86_64 /usr/local/xtrabackup命令说明1234567891011121314151617181920212223# 全量备份整个数据库（包括配置文件、二进制日志、重做日志、回滚日志、数据文件）/usr/local/xtrabackup/bin/innobackupex --defaults-file=/etc/my.cnf --user=root --password=password --port=3306 BACKUP_DIR# 增量备份整个数据库/usr/local/xtrabackup/bin/innobackupex --defaults-file=/etc/my.cnf --user=root --password=password --host=localhost --port=3306 --incremental --incremental-basedir=/backup_dir/YYYY-MM-DD_HH-mm-ss/ /mysql_backup/# 将增量备份的redo log合并到全量备份/usr/local/xtrabackup/bin/innobackupex --apply-log --redo-only FULL_BACKUP_DIR/usr/local/xtrabackup/bin/innobackupex --apply-log --redo-only FULL_BACKUP_DIR --incremental-dir=/INCREMENTAL_BACKUP_DIR/# 恢复全量备份/usr/local/xtrabackup/bin/innobackupex --defaults-file=/etc/my.cnf --copy-back FULL_BACKUP_DIR# 从全量备份中“导出”表# 需要在配置中启用innodb_file_per_table=1# 此命令会为每一个InnoDB表创建一个.exp结尾的文件仅包含数据，不包含表结构/usr/local/xtrabackup/bin/innobackupex --apply-log --export FULL_BACKUP_DIR# 从全量备份中“导入”表mysql&gt; create table table_name (...) engine=InnoDB;mysql&gt; alter table db_name.table_name discard tablespaces;# 将“导出”表的table_name.ibd和table_name文件拷贝到服务器数据目录，使用以下命令“导入”mysql&gt; alter table db_name.table_name import tablespaces;6.2、其他备份工具LVM快照1234567891011# 锁定所有表mysql &gt; flush tables with read lock;# 记录二进制日志文件和事件位置mysql &gt; flush logs;mysql -e &apos;show master status&apos; &gt; /path/to/file# 创建快照lvcreate -L # -s -p r -n NAME /dev/vg_name/lv_name# 释放锁mysql &gt; unlock tables;# 挂载快照卷，执行数据备份# 备份完成删除快照卷六、主从复制1、主节点dump thread：为每个Slave的IO Thread 启动一个dump线程，向其发送binary log events2、从节点IO Thread：从Master请求binary log events，并保存到中继日志SQL Thread：从中继日志读取日志事件，在本地完成重放3、特点Mater和Slave之间是异步复制主从数据不一致的情况比较常见4、复制架构4.1、主从复制4.1.1、主节点配置过程启动二进制日志为当前节点设置全局唯一的ID号12345678# 在my.cnf里面添加以下内容[mysqld]log-bin = master-binloginnodb_file_per_table = 1server-id = 1sync_binlog = 1innodb_flush_logs_at_trx_commit = 1sync_master_info = 1创建有复制权限的用户账号12mysql&gt; grant replication slave,replication client on *.* to 'repuser'@'%' identified by 'password';mysql&gt; flush privileges;4.1.2、从节点配置过程启动中继日志为当前节点设置全局唯一的ID号设置为只读状态（不影响主从复制）1234567891011# 在my.cnf里面添加以下内容[mysqld]skip_slave_start = 1relay-log = relay-logrelay-log = relay-log.indexserver-id = 2innodb_file_per_table = 1skip_name_resolve = 1read_only = 1sync_relay_log = 1sync_relay_log_info = 1使用有复制权限的用户账号连接到主服务器，并启动复制线程12mysql&gt; change master to master_host='master-node',master_user='repuser',master_password='password',master_port=3306,master_log_file='mater-log.003',master_log_pos=1111,master_connect_retry=10;mysql&gt; start slave [io_thread|sql_thread];查看从状态信息1mysql&gt; show slave status\\G;4.1.3、复制架构中应该注意的问题限制从服务器只读123456# 确保my.cnf配置以下选项[mysqld]read_only = 1# read_only对拥有super权限用户无效# 阻止所有用户，不影响主从复制mysql&gt; flush tables with read lock;保证主从复制事务安全主节点配置12345678[mysqld]# 每次事务提交都写入binlogsync_binlog = 1# 如果启用了InnoDB，需要启用以下配置# 每次事务提交时，log buffer 会被写入到日志文件并刷写到磁盘。innodb_flush_logs_at_trx_commit = 1# 每次事务提交都写入master.info，这样在崩溃的时候，最多丢失一个事务，但是会造成大量的磁盘IOsync_master_info = 1从节点1234567[mysqld]# 手动启动slaveskip_slave_start = 1# slave的IO线程每次接受到master发送过来的binlog日志都要写入到系统缓冲去，然后刷入relay log中继日志里面，这样在崩溃的时候，最多丢失一个事务，但是会造成大量的磁盘IOsync_relay_log = 1# slave的IO线程每次接受到master发送过来的binlog日志都要写入到系统缓冲去，然后刷入relay-log.info中继日志里面，这样在崩溃的时候，最多丢失一个事务，但是会造成大量的磁盘IOsync_relay_log_info = 14.2、主主复制4.2.1、注意事项互为主从容易数据不一致，慎用自动增长IDA节点使用奇数ID123[mysqld]auto_increment_offset = 1auto_increment_increment = 2B节点使用偶数ID123[mysqld]auto_increment_offset = 2auto_increment_increment = 24.2.2、配置步骤使用唯一的server_id启动bin_log和relay_log创建拥有复制权限的用户定义自动增长ID字段数值范围为奇偶均把对方指定为主节点，启动复制线程4.3、半同步复制跟主从复制类似主节点会等待一个从节点写入完成再返回客户端写入成功通过semisync插件提供半同步复制功能123456# 二进制安装包/usr/local/mysql/lib/plugin/semisync_master.so/usr/local/mysql/lib/plugin/semisync_slave.so# rpm安装包/usr/lib64/mysql/plugin/semisync_master.so/usr/lib64/mysql/plugin/semisync_slave.so4.3.1、主节点启动二进制日志为当前节点设置全局唯一的ID号12345678# 在my.cnf里面添加以下内容[mysqld]log-bin = master-binloginnodb_file_per_table = 1server-id = 1sync_binlog = 1innodb_flush_logs_at_trx_commit = 1sync_master_info = 1创建有复制权限的用户账号12mysql&gt; grant replication slave,replication client on *.* to 'repuser'@'%' identified by 'password';mysql&gt; flush privileges;4.3.2、从节点配置过程启动中继日志为当前节点设置全局唯一的ID号设置为只读状态（不影响主从复制）1234567891011# 在my.cnf里面添加以下内容[mysqld]skip_slave_start = 1relay-log = relay-logrelay-log = relay-log.indexserver-id = 2innodb_file_per_table = 1skip_name_resolve = 1read_only = 1sync_relay_log = 1sync_relay_log_info = 1使用有复制权限的用户账号连接到主服务器，并启动复制线程12mysql&gt; change master to master_host='master-node',master_user='repuser',master_password='password',master_port=3306,master_log_file='mater-log.003',master_log_pos=1111,master_connect_retry=10;mysql&gt; start slave ;查看从状态信息1mysql&gt; show slave status\\G;4.3.3、启动semicsync插件主节点1234567891011121314151617181920212223242526272829303132333435363738394041# 安装半同步复制master插件mysql&gt; install plugin rpl_semi_sync_master soname &apos;semisync_master.so&apos;;mysql&gt; show plugins;+----------------------------+----------+--------------------+--------------------+---------+| Name | Status | Type | Library | License |+----------------------------+----------+--------------------+--------------------+---------+| rpl_semi_sync_master | ACTIVE | REPLICATION | semisync_master.so | GPL |+----------------------------+----------+--------------------+--------------------+---------+mysql&gt; &gt; show global variables like &apos;%semi%&apos;;+-------------------------------------------+------------+| Variable_name | Value |+-------------------------------------------+------------+| rpl_semi_sync_master_enabled | OFF || rpl_semi_sync_master_timeout | 10000 || rpl_semi_sync_master_trace_level | 32 || rpl_semi_sync_master_wait_for_slave_count | 1 || rpl_semi_sync_master_wait_no_slave | ON || rpl_semi_sync_master_wait_point | AFTER_SYNC |+-------------------------------------------+------------+# 启动半同步复制master节点mysql&gt; set global variables rpl_semi_sync_master_enable = 1mysql&gt; show status like &apos;%semi_sync_%&apos;;+--------------------------------------------+-------+| Variable_name | Value |+--------------------------------------------+-------+| Rpl_semi_sync_master_clients | 0 || Rpl_semi_sync_master_net_avg_wait_time | 0 || Rpl_semi_sync_master_net_wait_time | 0 || Rpl_semi_sync_master_net_waits | 0 || Rpl_semi_sync_master_no_times | 0 || Rpl_semi_sync_master_no_tx | 0 || Rpl_semi_sync_master_status | ON || Rpl_semi_sync_master_timefunc_failures | 0 || Rpl_semi_sync_master_tx_avg_wait_time | 0 || Rpl_semi_sync_master_tx_wait_time | 0 || Rpl_semi_sync_master_tx_waits | 0 || Rpl_semi_sync_master_wait_pos_backtraverse | 0 || Rpl_semi_sync_master_wait_sessions | 0 || Rpl_semi_sync_master_yes_tx | 0 || Rpl_semi_sync_slave_status | OFF |+--------------------------------------------+-------+从节点123456789101112131415161718192021222324252627# 停止从节点复制线程mysql&gt; stop slave;# 安装半同步复制Slave插件mysql&gt; install plugin rpl_semi_sync_master soname &apos;semisync_slave.so&apos;;mysql&gt; &gt; show plugins;+----------------------------+----------+--------------------+--------------------+---------+| Name | Status | Type | Library | License |+----------------------------+----------+--------------------+--------------------+---------+| rpl_semi_sync_slave | ACTIVE | REPLICATION | semisync_slave.so | GPL |+----------------------------+----------+--------------------+--------------------+---------+mysql&gt; &gt; show global variables like &apos;%semi%&apos;;+-------------------------------------------+------------+| Variable_name | Value |+-------------------------------------------+------------+| rpl_semi_sync_slave_enabled | OFF || rpl_semi_sync_slave_trace_level | 32 |+-------------------------------------------+------------+# 启动半同步复制Slave插件mysql&gt; set global variables rpl_semi_sync_slave_enabled=1;mysql&gt; show status like &apos;%semi_sync_%&apos;;+--------------------------------------------+-------+| Variable_name | Value |+--------------------------------------------+-------+| Rpl_semi_sync_slave_status | ON |+--------------------------------------------+-------+# 开启从节点复制线程mysql&gt; start slave;5、复制过滤5.1、主服务器123456[mysqld]# 数据库记录到binlogbinlog_do_db = db1binlog_do_db = db2# 忽略数据库binlog_ignore_db = db1,db25.2、从服务器123456789[mysqld]# 复制数据库replicate_do_db = db1,db2# 忽略复制数据库replicate_ignore_db = db1,db2# 复制表replicate_do_table = db.table1,db.table2# 忽略复制表replicate_ingore_table = db.table1,db.table26、监控维护6.1、相关文件6.1.1、master.info保存Slave连接至master时的相关信息，例如账号密码、服务器地址、Position等6.1.2、relay-log.info保存当前Slave节点已经复制的当前二进制日志与本地relay-log日志的对应项6.2、清理日志6.3、复制监控12345mysql&gt; show master status;mysql&gt; show binlog events;mysql&gt; show binary logs;mysql&gt; show slave status;mysql&gt; show processlist;6.4、从服务器是否落后于主服务器12mysql&gt; show slave status \\G;Seconds_Behind_Master: 06.5、确定主从数据是否一致12345678910# 安装依赖包yum install perl-Time-HiRes -y# 下载二进制包wget https://www.percona.com/downloads/percona-toolkit/3.0.12/binary/tarball/percona-toolkit-3.0.12_x86_64.tar.gz# 解压tar xzf percona-toolkit-3.0.12_x86_64.tar.gzmv percona-toolkit-3.0.12 /usr/local/ln -sv /usr/local/percona-toolkit-3.0.12 /usr/local/percona-toolkit# 运行工具检查pt-table-checksum6.6、数据不一致如何处理重新同步七、MHA（Master High Availability）1、介绍MHA(Master High Availability)目前在MySQL高可用方面是一个相对成熟的解决方案，是一套优秀的作为MySQL高可用性环境下故障切换和主从提升的高可用软件。MySQL故障切换过程中，MHA能做到在0～30秒之内自动完成数据库的故障切换操作，MHA能在最大程度上保证数据的一致性，以达到真正意义上的高可用。该软件由两部分组成：MHA Manager(管理节点)和MHA Node(数据节点)。MHA Mananger可以单独部署在一台独立的机器上管理多个master-slave集群，也可以部署在一台slave节点上。MHA Node运行在每台MySQL服务器上，MHA Manager会定时探测集群中的master节点中，当master出现故障时，它可以自动将最新数据的slave提升为新的master，然后将所有其他slave重新指向新的master。整个故障转移过程对应用程序完全透明。在MHA自动故障切换过程中，MHA试图从宕机的主服务器上保存二进制日志，最大程度的保证数据不丢失，但这这并不总是可行的。例如，如果主服务器硬件故障或无法通过ssh访问，MHA没法保存二进制日志，只进行故障转移而丢失了最新的数据。使用MySQL 5.5的半同步复制，可以大大降低数据丢失的风险。MHA可以与半同步复制结合起来。如果只有一个slave已经收到了最新的二进制日志，MHA可以将最新的二进制日志应用于其他所有的slave服务器上，因此可以保证所有节点的数据一致性。2、MHA组件Manager节点masterha_check_ssh：MHA依赖的SSH环境检测工具masterha_check_repl：MySQL复制环境检测工具masterha_manager：MHA服务主程序masterha_check_status：MHA运行状态探测工具masterha_master_monitor：MySQL master节点监测工具masterha_master_switch：master节点切换工具masterha_conf_host：添加删除配置的节点masterha_stop：关闭MHA服务Node节点save_binary_logs：保存复制master的binlogapply_diff_relay_logs：识别差异的中继日志事件并应用到其他slavefilter_mysqlbinlog：去除不必要的rollback事件（已弃用）purge_relay_logs：清除中继日志（不阻塞SQL线程）自定义扩展secondary_check_script：通过多条网络路由检查master可用性master_ip_failover_script：更新Application使用的master IP地址shutdown_script：关闭master节点report_script：发送报告init_conf_load_script：加载初始化配置参数master_ip_online_chage_script：更新master节点IP地址3、准备MHA环境3.1、MHA对MySQL复制环境有特殊要求各节点开启二进制日志和中继日志从节点需要配置read-only，关闭relay_log_purge3.2、服务器角色分配manager：MHA Managerdb1：master节点db2：slave节点db3：slave节点3.3、环境准备各节点启用epel源各节点/etc/hosts文件12345vi /etc/hosts10.0.0.10 manager10.0.0.11 db110.0.0.12 db210.0.0.13 db3初始master节点配置12345[mysqld]# server_id必须唯一server_id=1relay-log=relay-loglog-bin=master-bin初始Slave节点配置1234567[mysqld]# server_id必须唯一server_id=2relay-log=relay-loglog-bin=master-binrelay_log_purge=0read_only=1配置主从复制db1配置为master节点db2、db3以db1为master节点，开启slave线程所有数据库节点创建MHA所需的管理用户1mysql&gt; grant all privileges on *.* to &apos;mhauser&apos;@&apos;%&apos; identified by &apos;password&apos;;配置manager、db1、db2、db3免密码SSH连接12345678910111213# 在manager节点上配置证书，分发到其他节点# 下面以root用户为例ssh-keygen -t rsacat /root/.ssh/id_rsa &gt; /root/.ssh/authorized_keyschmod 0600 /root/.ssh/authorized_keysfor i in db1 db2 db3;doscp -p .ssh/id_rcs .ssh/authorized_keys $i:/root/.ssh/donefor i in manager db1 db2 db3;dossh $i datedone3.4、安装MHA3.4.1、node节点安装1234# 安装依赖包yum install perl perl-DBD-MySQL -ywget https://github.com/yoshinorim/mha4mysql-node/releases/download/v0.58/mha4mysql-node-0.58-0.el7.centos.noarch.rpmyum localinstall mha4mysql-node-0.58-0.el7.centos.noarch.rpm3.4.2、manager节点安装配置123456789101112131415161718192021222324252627282930313233# 安装perl相关软件包yum install perl perl-DBD-MySQL -y# 下载安装mha4mysql-manager软件包wget https://github.com/yoshinorim/mha4mysql-manager/releases/download/v0.58/mha4mysql-manager-0.58-0.el7.centos.noarch.rpmyum localinstall mha4mysql-manager-0.58-0.el7.centos.noarch.rpm -y# 编辑MHA配置文件/etc/mha_app1.confcat &gt; /etc/mha_app1.conf &lt;&lt;EOF[server default]# mysql创建给MHA使用的账号密码user=rootpassword=supersecure# manager工作目录manager_workdir=/data/masterha/app1# manager日志路径manager_log=/data/masterha/app1/manager.log# MySQL工作目录remote_workdir=/data/masterha/app1# ssh使用的用户ssh_user=rootrepl_user=repluserrepl_password=passwordping_interval=1[server1]hostname=db1# ssh_port=22222candidate_master=1[server2]hostname=db2candidate_master=1[server3]hostname=db3# 禁止节点切换成master节点可以配置no_master=1EOF3.4.3、manager节点检测配置和启动12345678910# 检查各节点ssh互信通信配置masterha_check_ssh --conf=/etc/mha_app1.conf# 检查受管MySQL复制集群配置masterha_check_repl --conf=/etc/mha_app1.conf# 启动mha4mysql-managermasterha_manager --conf=/etc/mha_app1.conf# 检查mha4mysql-manager状态masterha_check_status --conf=/etc/mha_app1.conf# 关闭mha4mysql-manager服务masterha_stop --conf=/etc/mha_app1.conf3.4.4、MHA注意点keepalived提供VIP和master切换的对应脚本masterha_manager启动参数–remove_dead_master_conf，当发生主从切换后，老的主库的IP将会从配置文件中移除–ignore_last_failover，MHA切换时会产生app1.failover.complete文件，两次切换时间少于8小时会切换失败，使用该参数能忽略八、复制的问题和解决1、数据损坏和丢失MasterMHA + 半同步复制Slave重新复制2、混合使用存储引擎MyISAM不支持事务InnoDB支持事务3、server_id不唯一重新复制4、复制延迟需要额外的监控工具辅助九、Percona XtraDB Cluster1、介绍项目地址： http://www.percona.com/doc/percona-xtradb-cluster/intro.htmlPercona XtraDB Cluster是MySQL高可用性和可扩展性的解决方案.Percona XtraDB Cluster提供的特性有：同步复制，事务要么在所有节点提交或不提交。多主复制，可以在任意节点进行写操作。在从服务器上并行应用事件，真正意义上的并行复制。节点自动配置。数据一致性，不再是异步复制。Percona XtraDB Cluster完全兼容MySQL和Percona Server，表现在：数据的兼容性应用程序的兼容性：无需更改应用程序2、特点当执行一个查询时，在本地节点上执行。因为所有数据都在本地，无需远程访问。无需集中管理。可以在任何时间点失去任何节点，但是集群将照常工作。良好的读负载扩展，任意节点都可以查询。加入新节点，开销大。需要复制完整的数据。不能有效的解决写缩放问题，所有的写操作都将发生在所有节点上。有多少个节点就有多少重复的数据。3、局限性目前的复制仅仅支持InnoDB存储引擎。任何写入其他引擎的表，包括mysql.*表将不会复制。但是DDL语句会被复制的，因此创建用户将会被复制，但是insert into mysql.user…将不会被复制的。DELETE操作不支持没有主键的表。没有主键的表在不同的节点顺序将不同，如果执行SELECT…LIMIT… 将出现不同的结果集。在多主环境下LOCK/UNLOCK TABLES不支持。以及锁函数GET_LOCK(), RELEASE_LOCK()..查询日志不能保存在表中。如果开启查询日志，只能保存到文件中。允许最大的事务大小由wsrep_max_ws_rows和wsrep_max_ws_size定义。任何大型操作将被拒绝。如大型的LOAD DATA操作。由于集群是乐观的并发控制，事务commit可能在该阶段中止。如果有两个事务向在集群中不同的节点向同一行写入并提交，失败的节点将中止。对于集群级别的中止，集群返回死锁错误代码(Error: 1213 SQLSTATE: 40001 (ER_LOCK_DEADLOCK)).XA事务不支持，由于在提交上可能回滚。整个集群的写入吞吐量是由最弱的节点限制，如果有一个节点变得缓慢，那么整个集群将是缓慢的。为了稳定的高性能要求，所有的节点应使用统一的硬件。集群节点建议最少3个。2个也可以运行，但是官方不推荐这么做，因为3个节点是为了预防脑裂。如果DDL语句有问题将破坏集群。建议使用pt-online-schema-change操作DDL。4、安装galera-cluster4.1、软件包官网下载地址官方文档地址4.2、准备工作1234# cat /etc/hosts10.0.0.11 db110.0.0.12 db210.0.0.13 db3节点一123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170[root@db1 ~]# mkdir -p /usr/local/pxc[root@db1 ~]# mkdir -p /data/pxc/mysql3306/&#123;data,tmp,logs&#125; [root@db1 ~]# vi /etc/my.cnf [client]port = 3306socket = /data/pxc/mysql3306/tmp/mysql.sock# The MySQL server[mysqld]#########Basic##################explicit_defaults_for_timestamp=trueport = 3306 user = mysql basedir = /usr/local/pxcdatadir = /data/pxc/mysql3306/data tmpdir = /data/pxc/mysql3306/tmp pid-file = /data/pxc/mysql3306/tmp/mysql.pid socket = /data/pxc/mysql3306/tmp/mysql.sock #skip-grant-tables #character setcharacter_set_server = utf8open_files_limit = 65535back_log = 500#event_scheduler = ON#lower_case_table_names=1skip-external-lockingskip_name_resolve = 1default-storage-engine = InnoDB#timeoutwait_timeout=1000interactive_timeout=1000connect_timeout = 20server-id = 11 #ip最后一位#pluginplugin-load=\"semisync_master.so;semisync_slave.so\"#########error log#############log-error = /data/pxc/mysql3306/logs/error.log log-warnings = 2 #########general log##############general_log=1#general_log_file=/data/pxc/mysql3306/logs/mysql.log #########slow log#############slow_query_log = 1long_query_time=1slow_query_log_file = /data/pxc/mysql3306/logs/mysql.slow ############# for replication###################log-bin = /data/pxc/mysql3306/logs/mysql-bin binlog_format = rowmax_binlog_size = 50Mbinlog_cache_size = 2Mmax_binlog_cache_size = 2Mexpire-logs-days = 7slave-net-timeout=30log_bin_trust_function_creators = 1log-slave-updates = 1 skip-slave-start = 1#super_read_only =1 #GTIDgtid-mode = onbinlog_gtid_simple_recovery=1enforce_gtid_consistency=1#relay logrelay-log = /data/pxc/mysql3306/logs/mysql-relay relay-log-index=/data/pxc/mysql3306/logs/relay-bin.indexmax-relay-log-size = 500M#replication crash safesync_master_info = 1sync_relay_log_info = 1sync_relay_log = 1relay_log_recovery = 1master_info_repository = TABLErelay_log_info_repository = TABLE#######per_thread_buffers#####################max_connections=1100max_user_connections=1000max_connect_errors=10000#myisam_recoverkey_buffer_size = 64Mmax_allowed_packet = 16M#table_cache = 3096table_open_cache = 6144table_definition_cache = 4096read_buffer_size = 1Mjoin_buffer_size = 128Kread_rnd_buffer_size = 1M#myisamsort_buffer_size = 128Kmyisam_max_sort_file_size = 1Gmyisam_repair_threads = 1myisam_sort_buffer_size = 32Mtmp_table_size = 32Mmax_heap_table_size = 64Mquery_cache_type=0query_cache_size = 0bulk_insert_buffer_size = 32Mthread_cache_size = 64#thread_concurrency = 32thread_stack = 192K###############InnoDB###########################innodb_data_home_dir = /data/pxc/mysql3306/data innodb_log_group_home_dir = /data/pxc/mysql3306/logs innodb_data_file_path = ibdata1:10M:autoextendinnodb_buffer_pool_size = 512M #根据内存大小设置innodb_buffer_pool_instances = 8#innodb_additional_mem_pool_size = 16Minnodb_log_file_size = 50Minnodb_log_buffer_size = 16Minnodb_log_files_in_group = 3innodb_flush_log_at_trx_commit = 1sync_binlog = 1innodb_lock_wait_timeout = 10innodb_sync_spin_loops = 40innodb_max_dirty_pages_pct = 80innodb_support_xa = 1innodb_thread_concurrency = 0innodb_thread_sleep_delay = 500innodb_concurrency_tickets = 1000innodb_flush_method = O_DIRECTinnodb_file_per_table = 1innodb_read_io_threads = 16innodb_write_io_threads = 16innodb_io_capacity = 800 innodb_flush_neighbors = 1innodb_file_format = Barracudainnodb_purge_threads=4 innodb_purge_batch_size = 32innodb_old_blocks_pct=75innodb_change_buffering=allinnodb_stats_on_metadata=OFFinnodb_print_all_deadlocks = 1performance_schema=0 transaction_isolation = READ-COMMITTED############# PXC ##################### innodb_autoinc_lock_mode=2 wsrep_cluster_name=pxc-dongzheng wsrep_provider=/usr/local/pxc/lib/libgalera_smm.so wsrep_cluster_address=gcomm://10.0.0.11,10.0.0.12,10.0.13 wsrep_node_address=10.0.0.11 # 本机IP地址 wsrep_slave_threads=2 wsrep_sst_auth=sst:sky wsrep_sst_method=xtrabackup-v2 wsrep_provider_options=\"debug=1\"[mysqldump]quickmax_allowed_packet = 128M[mysql]no-auto-rehashmax_allowed_packet = 128Mprompt = '(product)\\u@\\h:\\p [\\d]&gt; 'default_character_set = utf8[myisamchk]key_buffer_size = 64Msort_buffer_size = 512kread_buffer = 2Mwrite_buffer = 2M[mysqlhotcopy]interactive-timeout[mysqld_safe]#malloc-lib= /usr/local/mysql/lib/mysql/libjemalloc.so节点二、节点三修改my.cnf里面的对应值就行wsrep_node_address =server-id =4.3、启动集群先初始化节点一，将节点一加入集群123# 启动集群mysqld --defaults-file=/etc/my.cnf --initialize-insecuremysqld --defaults-file=/etc/my.cnf --wsrep-new-cluster &amp;12# 创建账号mysql&gt; GRANT PROCESS, RELOAD, LOCK TABLES, REPLICATION CLIENT ON *.* TO &apos;sst&apos;@&apos;localhost&apos;IDENTIFIED BY &apos;sky&apos;;FLUSH PRIVILEGES;节点二、节点三加入集群1mysqld --defaults-file=/etc/my.cnf &amp;","categories":[],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://luanlengli.github.io/tags/MySQL/"}]},{"title":"Linux运维常见面试题","slug":"Linux运维常见面试题","date":"2019-02-14T02:07:19.000Z","updated":"2019-02-15T02:36:18.000Z","comments":true,"path":"2019/02/14/Linux运维常见面试题.html","link":"","permalink":"https://luanlengli.github.io/2019/02/14/Linux运维常见面试题.html","excerpt":"","text":"LINUX系统软件安装和卸载的常见方法RHEL/CentOS系列rpm -e &quot;软件包名字&quot;yum remove &quot;软件包名字&quot;，此操作会卸载相关的依赖Debian/Ubuntu系列apt-get remove &quot;软件包名字&quot;，不删除依赖，保留配置文件apt-get autoremove &quot;软件包名字&quot;，删除依赖，保留配置文件apt-get purge &quot;软件包名字&quot;，不删除依赖，删除配置文件源代码安装的，可以使用make uninstall，或者直接删掉软件目录修改LINUX的主机名RHEL/CentOS系列hostnamectl set-hostname &quot;主机名&quot;修改/etc/sysconfig/network里面的HOSTNAMEDebian/Ubuntu系列修改/etc/hosts和/etc/hostname的主机名，然后运行/etc/init.d/hostname.sh start临时有效hostname &quot;主机名&quot;自动切割压缩备份日志脚本每天凌晨5点执行，带日期后缀以Nginx日志为例切割打包压缩前一天的日志之后，将压缩包上传到FTP服务器192.168.1.2账号aaa密码bbb123456789101112131415161718192021222324252627#!/bin/bashLOG_PATH='/path/to/nginx_log/'DATE=`date +%F`# 让Nginx切割日志cd $&#123;LOG_PATH&#125;for file in `ls | grep *log`;do mv $&#123;file&#125; $&#123;file&#125;.$&#123;DATE&#125;donenginx -s reopen# 打包文件tar nginx_log_$&#123;DATE&#125;.tar.gz `ls *$&#123;DATE&#125;`# 清理日志文件rm -rf *$&#123;DATE&#125;# 上传文件ftp -n &lt;&lt;- EOFopen 192.168.1.2user aaa bbbput nginx_log_$&#123;DATE&#125;.tar.gzbyeEOF# 删除过期文件find $&#123;LOG_PATH&#125; -type f -mtime +60 -exec rm -rf &#123;&#125; \\;10 5 * * * /bin/bash /path/to/logrotate.shiptables允许来自127.0.0.1端口1234访问114.114.114.114端口801iptables -A INPUT -s 127.0.0.1 --sport 1234 -d 114.114.114.114 --dport 80 -j ACCPET禁用从lo进入的流量1iptables -A INPUT -i lo -j REJECT允许外部访问22端口1iptables -A INPUT -p tcp --dport 22 -j ACCPET在postrouting链上，把源地址192.168.2.0/24的数据包源地址修改为192.168.1.21iptables -t nat -A POSTROUTING -s 192.168.2.0/24 -j SNAT --to-source 192.168.1.2删除INPUT链上第八条规则12iptables -Ln --line-numbersiptables -D INPUT 8eth0的数据包转发到eth11iptables -A FORWARD -i eth0 -o eth1 -j ACCEPTiptables匹配参数-A：在chain里追加规则-D：删除chain里的规则-I：将规则插入到chain-t：指定表-s：匹配源地址-d：匹配目的地址-p：协议匹配-i：入口匹配-o：出口匹配--sport：源端口匹配--dport：出口端口匹配-j：数据包处理动作!：取反iptables基本概念tableraw 用于配置数据包，raw 中的数据包不会被系统跟踪filter 是用于存放所有与防火墙相关操作的默认表nat 用于 网络地址转换（例如：端口转发）mangle 用于对特定数据包的修改security 用于 强制访问控制网络规则（例如： SELinux）处理流程12345678910111213141516171819202122232425262728293031323334 XXXXXXXXXXXXXXXXXX XXX Network XXX XXXXXXXXXXXXXXXXXX + | v +-------------+ +------------------+ |table: filter| &lt;---+ | table: nat | |chain: INPUT | | | chain: PREROUTING| +-----+-------+ | +--------+---------+ | | | v | v [local process] | **************** +--------------+ | +---------+ Routing decision +------&gt; |table: filter | v **************** |chain: FORWARD|**************** +------+-------+Routing decision |**************** | | | v **************** |+-------------+ +------&gt; Routing decision &lt;---------------+|table: nat | | ****************|chain: OUTPUT| | ++-----+-------+ | | | | v v | +-------------------++--------------+ | | table: nat ||table: filter | +----+ | chain: POSTROUTING||chain: OUTPUT | +--------+----------++--------------+ | v XXXXXXXXXXXXXXXXXX XXX Network XXX XXXXXXXXXXXXXXXXXX统计TCP连接状态1netstat -an|awk '/tcp/ &#123;print $6&#125;'|sort|uniq -c列出每个IP的TCP连接数1netstat -n | awk '/^tcp/ &#123;print $5&#125;' | awk -F: '&#123;print $1&#125;' | sort | uniq -c | sort -rn生成随机字符串1openssl rand -hex 201dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d \"=+/[:space:]\" | dd bs=32 count=1 2&gt;/dev/null描述tcp三次握手的过程第一次握手：建立连接时，客户端发送syn包(syn=j)到服务器，并进入SYN_SEND状态，等待服务器确认；第二次握手：服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器 进入SYN_RECV状态；第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户端和服务器进入 ESTABLISHED状态，完成三次握手。内核参数的含义net.ipv4.tcp_tw_recycle=1启用此功能可以加快TIME-WAIT状态的TCP连接回收内核会记录最后一个报文的时间戳，如果新的TCP报文时间戳小于最后一个报文的时间戳，则会drop包，导致无法建立TCP连接net.ipv4.tcp_tw_reuse=1允许将处于TIME-WAIT状态的socket用于新的TCP连接net.ipv4.tcp_timestamps=1发送TCP时间戳vm.swappiness=0最大限度使用物理内存，物理内存耗尽才启用swapLinux开机启动顺序以BIOS-based和RHEL/CentOS-7为例硬件加电加载BIOS硬件自检BIOS按照启动顺序读取存储设备的MBR根据MBR找到并加载bootloaderbootloader从/boot目录加载vmlinuz内核镜像，提取initramfs的内容并存放在tmpfs中操作系统内核从initramfs中加载必要的驱动和挂载根文件系统，启动systemd作为第一个系统进程systemd读取配置文件，读取default-target文件，引导至default-target定义的状态初始化网络设置主机名基于内核参数初始化硬件设备加载文件系统启动systemd服务启动完成ps aux中VSZ和RSS的含义VSZ：虚拟内存集，代表进程占用的虚拟内存空间RSS：物理内存集，代表进程实际占用的物理内存空间top命令中字段的含义PID：进程IDUSER：进程所有者的用户名PR：进程优先级NI：nice值，数值越小，优先级越高VIRT：虚拟内存集，代表进程占用的虚拟内存空间RES：物理内存集，代表进程实际占用的物理内存空间SHR：共享内存大小S：进程状态%CPU：CPU时间占比%MEM：物理内存占比TIME+：进程使用的CPU时间总量COMMAND：进程命令load average含义1load average: 0.00, 0.00, 0.00分别代表1分钟内、5分钟内、15分钟内系统的平均负载系统负载指的是运行队列的长度，即等待CPU的平均进程数因此load average的值=CPU核数是最理想的状态","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/tags/Linux/"}]},{"title":"kubectl命令的用法","slug":"kubectl命令的用法","date":"2019-02-12T07:10:40.000Z","updated":"2020-01-14T09:09:40.609Z","comments":true,"path":"2019/02/12/kubectl命令的用法.html","link":"","permalink":"https://luanlengli.github.io/2019/02/12/kubectl命令的用法.html","excerpt":"","text":"说明Kubernetes版本为v1.14.8某些功能在更高的版本才会有，会做特殊说明低版本不管！QuickStart获取帮助1kubectl --help查看集群信息1kubectl cluster-info指定kubeconfig默认是当前用户家目录下的~/.kube/config1kubectl --config /path/to/kubeconfigkubeconfig上下文配置12345678# 查看kubeconfig信息kubectl config view# 合并多个kubeconfig文件并查看合并后的kubeconfig信息KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 kubectl config view# 显示当前上下文kubectl config current-context# 设置默认上下文为cluster-namekubectl config use-context cluster-name指定命名空间默认命名空间是default12kubectl --namespace kube-systemkubectl -n kube-system指定所有命名空间1kubectl --all-namespaces指定输出格式默认不带任何输出参数时，是以较短结果输出12345kubectl get podkubectl get pod --output=widekubectl get pod -o widekubectl get pod -o jsonkubectl get pod -o yaml输出结果排序默认是以Pod名字排序可以通过指定--sort-by来做排序例如让Pod以宿主机名称排序1kubectl get pod -n kube-system -o wide --sort-by=\"&#123;.spec.nodeName&#125;\"测试yaml文件，实际不生效1kubectl apply -f demo.yaml --dry-run生成模板文件yaml文件内容字段比较多，很难一下子想到所需的字段，可以通过kubectl命令来生成对应的模板下面以deployment为例1kubectl -n kube-system get deployment coredns -o yaml --export输出示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113apiVersion: extensions/v1beta1kind: Deploymentmetadata: annotations: deployment.kubernetes.io/revision: \"1\" creationTimestamp: null generation: 1 labels: k8s-app: kube-dns name: coredns selfLink: /apis/extensions/v1beta1/namespaces/kube-system/deployments/corednsspec: progressDeadlineSeconds: 600 replicas: 2 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kube-dns strategy: rollingUpdate: maxSurge: 25% maxUnavailable: 1 type: RollingUpdate template: metadata: creationTimestamp: null labels: k8s-app: kube-dns spec: containers: - args: - -conf - /etc/coredns/Corefile image: gcr.azk8s.cn/google_containers/coredns:1.3.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: coredns ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /health port: 8080 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/coredns name: config-volume readOnly: true - mountPath: /tmp name: tmp dnsPolicy: Default nodeSelector: beta.kubernetes.io/os: linux priorityClassName: system-cluster-critical restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; serviceAccount: coredns serviceAccountName: coredns terminationGracePeriodSeconds: 30 tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/master volumes: - emptyDir: &#123;&#125; name: tmp - configMap: defaultMode: 420 items: - key: Corefile path: Corefile name: coredns name: config-volumestatus: &#123;&#125;创建资源12345678# 声明yaml定义的资源kubectl apply -f /path/to/yaml# 删除podkubectl delete pod busybox# 删除带k8s-app: busybox标签的podkubectl delete pod -l k8s-app=busybox# 删除所有podkubectl delete pod --all编辑资源1234# 编辑名为CoreDNS的configmapskubectl edit -n kube-system configmaps coredns# 设置kube的编辑器KUBE_EDITOR=\"nano\" kubectl edit configmaps corednskubectl cheatsheet自动补全bash12source &lt;(kubectl completion bash)echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrczsh12source &lt;(kubectl completion zsh)echo \"if [ $commands[kubectl] ]; then source &lt;(kubectl completion zsh); fi\" &gt;&gt; ~/.zshrc上下文配置查看kubeconfig配置1kubectl config view配置多kubeconfig12KUBECONFIG=~/.kube/config:~/.kube/kubconfig2kubectl config view查看上下文清单1kubectl config get-contexts查看当前上下文1kubectl config current-context切换默认上下文1kubectl config use-context my-cluster-name创建k8s对象创建资源根据文件创建12345kubectl apply -n NAMESPACE -f /path/to/yamlfilekubectl apply -n NAMESPACE -f /path/to/yamlfile1 -f /path/to/yamlfile2kubectl apply -n NAMESPACE -f /path/to/yamlfile_dirkubectl apply -n NAMESPACE -f http://website/yamlfilekubectl create deployment nginx --images=nginx通过stdin创建12345678910111213141516171819202122232425262728293031323334353637cat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: busybox-sleepspec: containers: - name: busybox image: busybox args: - sleep - \"1000000\"---apiVersion: v1kind: Podmetadata: name: busybox-sleep-lessspec: containers: - name: busybox image: busybox args: - sleep - \"1000\"EOFcat &lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: password: $(echo -n \"s33msi4\" | base64 -w0) username: $(echo -n \"jane\" | base64 -w0)EOF查看资源信息查看多个kind资源1kubectl get pod,svc查看所有命名空间1kubectl get pod --all-namespaces设置命令输出格式123kubectl get pod -o widekubectl get pod -o yamlkubectl get pod -o yaml --export描述资源信息12kubectl describe node k8s-node1kubectl describe pod kube-apiserver-k8s-master根据名字排序1kubectl get services --sort-by=.metadata.name只获取资源名字1kubectl get deployments -o jsonpath='&#123;.items[*].metadata.name&#125;'根据Pod重启次数排序1kubectl get pods --sort-by='.status.containerStatuses[0].restartCount'根据PV大小排序1kubectl get pv -n test --sort-by=.spec.capacity.storage根据时间戳排序1kubectl get events --sort-by=.metadata.creationTimestamp根据label筛选1kubectl get pods --selector=app=cassandra获取版本label1234kubectl get pods \\--selector=app=cassandra \\-o \\jsonpath='&#123;.items[*].metadata.labels.version&#125;'获取非master的节点1kubectl get node --selector='!node-role.kubernetes.io/master'获取运行状态为Running的Pod1kubectl get pods --field-selector=status.phase=Running获取节点的ExternalIP1kubectl get nodes -o jsonpath='&#123;.items[*].status.addresses[?(@.type==\"ExternalIP\")].address&#125;'列出Pod的label1kubectl get pods --show-labels列出ready状态的node12JSONPATH=\"&#123;range .items[*]&#125;&#123;@.metadata.name&#125;:&#123;range @.status.conditions[*]&#125;&#123;@.type&#125;=&#123;@.status&#125;;&#123;end&#125;&#123;end&#125;\"kubectl get nodes -o jsonpath=\"$JSONPATH\" | grep \"Ready=True\"列出被Pod使用的Secret1kubectl get pods -o json | jq '.items[].spec.containers[].env[]?.valueFrom.secretKeyRef.name' | grep -v null | sort | uniq查看diff1kubectl diff -f /path/to/yamlfile更新资源更新镜像1kubectl set image deployment/frontend www=image:v2查看更新历史1kubectl rollout history deployment/frontend回滚更新操作1kubectl rollout undo deployment/frontend回滚到指定revision1kubectl rollout undo deployment/frontend --to-revision=2滚动更新状态1kubectl rollout status -w deployment/frontend滚动重启PodKubernetes v1.15版本加入重启Pod的功能1kubectl rollout restart deployment/frontendPatch资源设置node不可调度1kubectl patch node k8s-node-1 -p '&#123;\"spec\":&#123;\"unschedulable\":true&#125;&#125;'修改deployment副本数量1kubectl patch deployments my-deploy -p '&#123;\"spec\": &#123;\"replicase\": 3&#125;&#125;'更新Pod容器的镜像12kubectl patch pod valid-pod -p '&#123;\"spec\":&#123;\"containers\":[&#123;\"name\":\"kubernetes-serve-hostname\",\"image\":\"new image\"&#125;]&#125;&#125;'kubectl patch pod valid-pod --type='json' -p='[&#123;\"op\": \"replace\", \"path\": \"/spec/containers/0/image\", \"value\":\"new imag去除数组对象12kubectl patch deployment valid-deployment --type json \\-p='[&#123;\"op\": \"remove\", \"path\": \"/spec/template/spec/containers/0/livenessProbe\"&#125;]'添加数组对象12kubectl patch sa default --type='json' \\-p='[&#123;\"op\": \"add\", \"path\": \"/secrets/1\", \"value\": &#123;\"name\": \"whatever\" &#125; &#125;]'资源删改编辑资源12kubectl edit svc/docker-registryKUBE_EDITOR=\"nano\" kubectl edit svc/docker-registry伸缩资源1234kubectl scale --replicas=3 rs/fookubectl scale --replicas=3 -f foo.yamlkubectl scale --current-replicas=2 --replicas=3 deployment/mysqlkubectl scale --replicas=5 rc/foo rc/bar rc/baz删除资源12345kubectl delete -f ./pod.jsonkubectl delete pod,service baz fookubectl delete pods,services -l name=myLabelkubectl -n my-ns delete pod,svc --allkubectl get pods -n mynamespace --no-headers=true | awk '/pattern1|pattern2/&#123;print $1&#125;' | xargs kubectl delete -n mynamespace pod与Pod交互查看Pod日志1kubectl logs my-pod根据label看Pod日志1kubectl logs -l name=myLabel查看上一个Pod实例日志1kubectl logs my-pod --previous查看Pod容器日志1kubectl logs my-pod -c my-container查看Pod所有容器日志1kubectl logs my-pod --all-containers在Pod上运行命令1kubectl exec my-pod -- ls /在Pod指定容器上运行命令1kubectl exec my-pod -c my-container -- ls /查看Pod metrics数据12kubectl top pod POD_NAME --containerskubectl top pod --all-namespaces --containers=true与Node交互设置node不可调度1kubectl cordon my-node恢复node可调度1kubectl uncordon my-node驱逐node的Pod1kubectl drain my-node查看node metrics数据1kubectl top node my-node修改taint1kubectl taint nodes foo dedicated=special-user:NoSchedule资源类型查看资源类型123456789kubectl api-resourceskubectl api-resources --namespaced=truekubectl api-resources --namespaced=falsekubectl api-resources -o namekubectl api-resources -o widekubectl api-resources --verbs=list,getkubectl api-resources --api-group=extensionskubectl api-versionskubectl get crd命令输出格式Output formatDescription-o=custom-columns=Print a table using a comma separated list of custom columns-o=custom-columns-file=Print a table using the custom columns template in the file-o=jsonOutput a JSON formatted API object`-o=jsonpath=Print the fields defined in a jsonpath expression-o=jsonpath-file=Print the fields defined by the jsonpath expression in the file-o=namePrint only the resource name and nothing else-o=wideOutput in the plain-text format with any additional information, and for pods, the node name is included-o=yamlOutput a YAML formatted API object命令输出日志级别VerbosityDescription--v=0Generally useful for this to always be visible to a cluster operator.--v=1A reasonable default log level if you don’t want verbosity.--v=2Useful steady state information about the service and important log messages that may correlate to significant changes in the system. This is the recommended default log level for most systems.--v=3Extended information about changes.--v=4Debug level verbosity.--v=6Display requested resources.--v=7Display HTTP request headers.--v=8Display HTTP request contents.--v=9Display HTTP request contents without truncation of contents.更多cheatsheetscheatsheet-kubernetes-a4","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"如何生成kubeconfig文件","slug":"如何生成kubeconfig文件","date":"2019-02-12T06:56:58.000Z","updated":"2019-06-18T09:40:55.000Z","comments":true,"path":"2019/02/12/如何生成kubeconfig文件.html","link":"","permalink":"https://luanlengli.github.io/2019/02/12/如何生成kubeconfig文件.html","excerpt":"","text":"修改cluster12345kubectl config set-cluster CLUSTER_NAME \\--certificate-authority=/path/to/ca \\--embed-certs=true \\--server=KUBE_APISERVER \\--kubeconfig=/path/to/kubeconfig修改user12345kubectl config set-credentials USERNAME \\--client-certificate=/path/to/cert \\--client-key=/path/to/key \\--embed-certs=true \\--kubeconfig=/path/to/kubeconfig修改context1234kubectl config set-context CONTEXT \\--cluster=CLUSTER_NAME \\--user=USERNAME \\--kubeconfig=/path/to/kubeconfig设置默认context1kubectl config use-context CONTEXT --kubeconfig=/path/to/kubeconfig","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"kubernetes TLS Bootstrap与节点证书签发","slug":"kubernetes-TLS-Bootstrap与节点证书签发","date":"2019-02-04T05:03:18.000Z","updated":"2019-06-18T09:40:30.000Z","comments":true,"path":"2019/02/04/kubernetes-TLS-Bootstrap与节点证书签发.html","link":"","permalink":"https://luanlengli.github.io/2019/02/04/kubernetes-TLS-Bootstrap与节点证书签发.html","excerpt":"","text":"Bootstrap初始化流程参考官方文档kubelet启动kubelet发现没有kubeconfig文件kubelet查找bootstrap-kubeconfig文件kubelet读取bootstrap-kubeconfig文件，获取kube-apiserver的URL和最低权限的tokenkubelet具备了创建和检索CSR请求的受限tokenkubelet使用此token作为凭证与kube-apiserver通信kubelet使用bootstrap-kubeconfig的CA证书为自己创建CSR（nodeclient）CSR可以通过两种方式获得批准，批准之后下发证书（kubelet-client.crt）配置kube-controller-manager自动批准CSR配置外部流程去通过kubernetes API或者kubectl去批准CSRkubelet使用密钥和证书创建kubeconfig文件kubelet开始正常启动流程可选：当证书接近到期时，kubelet会自动请求续订证书证书的续订自动签发，可以参考CSR批准证书的过程","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"以DaemonSet的方式运行kube-proxy","slug":"以DaemonSet的方式运行kube-proxy","date":"2019-01-08T02:23:31.000Z","updated":"2019-06-18T09:41:13.000Z","comments":true,"path":"2019/01/08/以DaemonSet的方式运行kube-proxy.html","link":"","permalink":"https://luanlengli.github.io/2019/01/08/以DaemonSet的方式运行kube-proxy.html","excerpt":"","text":"说明通常来说，二进制部署kubernetes集群的时候，一般都是把kube-proxy作为系统守护进程启动，例如注册成为systemd服务。用过kubeadm的同学都知道，kubeadm是以DaemonSet的方式部署kube-proxy，将kube-proxy托管到kubernetes集群管理。这样可以很方便的通过修改kube-proxy的configmap来统一管控集群的kube-proxy配置。操作过程创建yaml文件kube-proxy-daemonset.yaml根据情况替换KUBE_APISERVER和KUBE_VERSION123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183apiVersion: v1kind: ServiceAccountmetadata: name: kube-proxy namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:kube-proxyroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-proxiersubjects:- kind: ServiceAccount name: kube-proxy namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata: name: kube-proxy namespace: kube-system labels: app: kube-proxydata: config.conf: |- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration bindAddress: 0.0.0.0 clientConnection: acceptContentTypes: \"\" burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /var/lib/kube-proxy/kubeconfig.conf qps: 5 # 集群中pod的CIDR范围，从这个范围以外发送到服务集群IP的流量将被伪装，从POD发送到外部LoadBalanceIP的流量将被定向到各自的集群IP clusterCIDR: 10.244.0.0/16 configSyncPeriod: 15m0s conntrack: # 每个核心最大能跟踪的NAT连接数，默认32768 maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0s enableProfiling: false healthzBindAddress: 0.0.0.0:10256 hostnameOverride: \"\" iptables: # SNAT所有通过服务集群ip发送的通信 masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30s ipvs: minSyncPeriod: 0s # ipvs调度类型，默认是rr scheduler: rr syncPeriod: 30s metricsBindAddress: 127.0.0.1:10249 # 使用ipvs模式 mode: ipvs featureGates: SupportIPVSProxyMode: true oomScoreAdj: -999 portRange: \"\" resourceContainer: /kube-proxy udpIdleTimeout: 250ms kubeconfig.conf: |- apiVersion: v1 kind: Config clusters: - cluster: certificate-authority: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt server: &#123;&#123;KUBE_APISERVER&#125;&#125; name: default contexts: - context: cluster: default namespace: default user: default name: default current-context: default users: - name: default user: tokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token---apiVersion: extensions/v1beta1kind: DaemonSetmetadata: labels: k8s-app: kube-proxy name: kube-proxy namespace: kube-systemspec: revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kube-proxy template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" creationTimestamp: null labels: k8s-app: kube-proxy spec: containers: - command: - /usr/local/bin/kube-proxy - --config=/var/lib/kube-proxy/config.conf image: k8s.gcr.io/kube-proxy-amd64:&#123;&#123;KUBE_VERSION&#125;&#125; imagePullPolicy: IfNotPresent securityContext: privileged: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/localtime name: host-time readOnly: true - mountPath: /var/lib/kube-proxy name: kube-proxy - mountPath: /run/xtables.lock name: xtables-lock - mountPath: /lib/modules name: lib-modules readOnly: true dnsPolicy: ClusterFirst hostNetwork: true nodeSelector: beta.kubernetes.io/arch: amd64 priorityClassName: system-node-critical restartPolicy: Always schedulerName: default-scheduler serviceAccount: kube-proxy serviceAccountName: kube-proxy terminationGracePeriodSeconds: 30 tolerations: - key: CriticalAddonsOnly operator: Exists - operator: Exists - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists - effect: NoSchedule key: node.kubernetes.io/disk-pressure operator: Exists - effect: NoSchedule key: node.kubernetes.io/memory-pressure operator: Exists - effect: NoSchedule key: node.kubernetes.io/unschedulable operator: Exists - effect: NoSchedule key: node.kubernetes.io/network-unavailable operator: Exists volumes: - configMap: defaultMode: 420 name: kube-proxy name: kube-proxy - hostPath: path: /run/xtables.lock type: FileOrCreate name: xtables-lock - hostPath: path: /lib/modules type: \"\" name: lib-modules - hostPath: path: /etc/localtime name: host-time templateGeneration: 1 updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate部署1kubectl apply -f kube-proxy-daemonset.yaml验证1kubectl -n kube-system get pod","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"}]},{"title":"RHEL/CentOS升级OpenSSL和OpenSSH","slug":"RHEL-CentOS升级OpenSSL和OpenSSH","date":"2019-01-07T15:01:46.000Z","updated":"2019-02-11T14:39:34.000Z","comments":true,"path":"2019/01/07/RHEL-CentOS升级OpenSSL和OpenSSH.html","link":"","permalink":"https://luanlengli.github.io/2019/01/07/RHEL-CentOS升级OpenSSL和OpenSSH.html","excerpt":"","text":"说明本文以openssl-1.0.2p和openssh-7.9p1为例先编译安装新版本的OpenSSL，然后基于新版本的OpenSSL编译安装OpenSSHRHEL/CentOS-6.x和7.x的操作类似注意事项请不要盲目复制粘贴考虑好每一步的后果，做好备份先拿测试服务器做白老鼠测试，通过之后再通过自动化部署的方式升级下载源代码OpenSSL官网地址OpenSSH官网地址检查当前版本123456# 检查openssl版本openssl version# 检查ssh版本ssh -V# 检查openssh-server版本sshd --version安装编译环境1yum install -y gcc make zlib-devel pam-devel libedit-devel krb5-devel安装OpenSSL解压源代码1tar xzf openssl-1.0.2p.tar.gz编译安装编译选项参照CentOS-7.x软件包的选项123456789101112131415161718cd openssl-1.0.2p.tar.gz./config --prefix=/usr/local/openssl-1.0.2p \\ no-asm 386 \\ zlib \\ enable-camellia \\ enable-seed \\ enable-tlsext \\ enable-rfc3779 \\ enable-cms \\ enable-md2 \\ no-mdc2 \\ no-rc5 \\ no-ec2m \\ no-gost \\ no-srp \\ --with-krb5-flavor=MIT \\ sharedmake depend &amp;&amp; make &amp;&amp; make install备份旧版本的openssl123mv /usr/bin/openssl /usr/bin/openssl.`date +%Y%m%d`bakmv /usr/include/openssl /usr/include/openssl.`date +%Y%m%d`bakmv /usr/lib64/openssl/engines /usr/lib64/openssl/engines.`date +%Y%m%d`bak创建软链接到新版本openssl123ln -sv /usr/local/openssl-1.0.2p/bin/openssl /usr/bin/opensslln -sv /usr/local/openssl-1.0.2p/include/openssl /usr/include/opensslln -sv /usr/local/openssl-1.0.2p/lib/engines/ /usr/lib64/openssl/engines创建新版本的链接库12345cat &gt; /etc/ld.so.conf.d/openssl-1.0.2p.conf &lt;&lt;EOF# OpenSSL 1.0.2p/usr/local/openssl-1.0.2p/libEOFldconfig检查openssl版本1openssl version安装OpenSSH解压源代码1tar xzf openssh-7.9p1.tar.gz使用新版OpenSSL编译安装OpenSSH12345678910cd openssh-7.9p1./configure --prefix=/usr/local/openssh-7.9p1 \\ --with-ssl-dir=/usr/local/openssl-1.0.2p \\ --with-md5-passwords \\ --with-mantype=man \\ --disable-strip \\ --with-smartcard \\ --with-pam \\ --with-kerberos5make &amp;&amp; make install备份替换OpenSSH1234567891011121314for binary in `ls /usr/local/openssh-7.9p1/bin/`;do mv /usr/bin/$&#123;binary&#125; /usr/bin/$&#123;binary&#125;.`date +%Y%m%d`bak ln -sv /usr/local/openssh-7.9p1/bin/$&#123;binary&#125; /usr/bin/$&#123;binary&#125;donefor exec in `ls /usr/local/openssh-7.9p1/libexec/`;do mv /usr/libexec/openssh/$&#123;exec&#125; /usr/libexec/openssh/$&#123;exec&#125;.`date +%Y%m%d`bak ln -sv /usr/local/openssh-7.9p1/libexec/$&#123;exec&#125; /usr/libexec/openssh/$&#123;exec&#125;donefor sbinary in `ls /usr/local/openssh-7.9p1/sbin/`;do mv /usr/sbin/$&#123;sbinary&#125; /usr/sbin/$&#123;sbinary&#125;.`date +%Y%m%d`bak ln -sv /usr/local/openssh-7.9p1/sbin/$&#123;sbinary&#125; /usr/sbin/$&#123;sbinary&#125;done链接配置目录到新版本的OpenSSH123mv /usr/local/openssh-7.9p1/etc /usr/local/openssh-7.9p1/etc.bakln -sv /etc/ssh /usr/local/openssh-7.9p1/etcfind /etc/ssh/ -name '*key' -exec chmod 0400 &#123;&#125; \\;检查SSH配置1sshd -t -f /etc/ssh/sshd_config重启SSH服务RHEL/CentOS-6.x1service sshd restartRHEL/CentOS-7.x修改/usr/lib/systemd/system/sshd.service1234vim /usr/lib/systemd/system/sshd.service...Type=simple...重启sshd服务12systemctl daemon-reloadsystemctl restart sshd检查OpenSSH版本12ssh -Vsshd --version","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/tags/Linux/"}]},{"title":"etcd学习笔记","slug":"etcd学习笔记","date":"2018-12-22T08:49:18.000Z","updated":"2019-11-27T09:07:03.000Z","comments":true,"path":"2018/12/22/etcd学习笔记.html","link":"","permalink":"https://luanlengli.github.io/2018/12/22/etcd学习笔记.html","excerpt":"","text":"kubernetes底层数据存放在etcd中，这里记录一下学习的笔记。说明Etcd 是 CoreOS 推出的分布式一致性键值存储，用于共享配置和服务发现Etcd 支持集群模式部署，从而实现自身高可用本文以CentOS-7.6和etcd-v3.3.10为例etcd安装二进制文件安装下载1234567# 下载并解压wget -q -O - https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz | tar xz# 查看解压后的文件ls etcd-v3.3.10-linux-amd64Documentation etcd etcdctl README-etcdctl.md README.md READMEv2-etcdctl.md# 将二进制执行文件移动到/usr/local/bin/mv etcd-v3.3.10-linux-amd64/etcd etcd-v3.3.10-linux-amd64/etcdctl /usr/local/bin/配置创建用户12groupadd -r etcduseradd -r -g etcd -s /bin/false etcd创建目录1mkdir -p /var/lib/etcd /etc/etcd/配置文件创建配置文件etcd.config.yaml，内容如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# This is the configuration file for the etcd server.# Human-readable name for this member.name: 'default'# Path to the data directory.data-dir: /var/lib/etcd/default.etcd# Path to the dedicated wal directory.wal-dir: /var/lib/etcd/wal# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 10000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: http://localhost:2380# List of comma separated URLs to listen on for client traffic.listen-client-urls: http://localhost:2379# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 5# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: http://localhost:2380# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: http://localhost:2379# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster:# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: # Path to the client server TLS key file. key-file: # Enable client cert authentication. client-cert-auth: false # Path to the client server TLS trusted CA cert file. trusted-ca-file: # Client TLS using generated certificates auto-tls: falsepeer-transport-security: # Path to the peer server TLS cert file. cert-file: # Path to the peer server TLS key file. key-file: # Enable peer client cert authentication. client-cert-auth: false # Path to the peer server TLS trusted CA cert file. trusted-ca-file: # Peer TLS using generated certificates. auto-tls: false# Enable debug-level logging for etcd.debug: falselogger: zap# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [stderr]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: periodicauto-compaction-retention: \"1\"# Set level of detail for exported metrics, specify 'extensive' to include histogram metrics.# default is 'basic'metrics: 'basic'创建服务文件使用systemd托管etcd的服务1234567891011121314151617cat &gt; /usr/lib/systemd/system/etcd.service &lt;&lt;EOF[Unit]Description=etcd key-value storeDocumentation=https://github.com/etcd-io/etcdAfter=network.target[Service]User=etcdType=notifyExecStart=/usr/local/bin/etcd --config-file /etc/etcd/etcd.config.yamlRestart=alwaysRestartSec=10sLimitNOFILE=65535[Install]WantedBy=multi-user.targetEOF运行etcd123chown -R etcd:etcd /var/lib/etcd /etc/etcdsystemctl daemon-reloadsystemctl start etcd.service验证etcd服务123etcdctl cluster-healthmember 8e9e05c52164694d is healthy: got healthy result from http://localhost:2379cluster is healthyetcd集群部署构建集群的方式静态发现预先已知 Etcd 集群中有哪些节点，在启动时直接指定好 Etcd 的各个 node 节点地址动态发现通过已有的 Etcd 集群作为数据交互点，然后在扩展新的集群时实现通过已有集群进行服务发现的机制DNS动态发现通过 DNS 查询方式获取其他节点地址信息节点信息这里只提供静态发现部署etcd集群的流程IP地址主机名CPU内存172.16.80.201etcd148G172.16.80.202etcd248G172.16.80.203etcd348G静态发现部署etcd集群创建配置文件etcd11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# This is the configuration file for the etcd server.# Human-readable name for this member.name: 'etcd1'# Path to the data directory.data-dir: /var/lib/etcd/etcd1.etcd# Path to the dedicated wal directory.wal-dir: /var/lib/etcd/wal# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 10000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: 'http://172.16.80.201:2380'# List of comma separated URLs to listen on for client traffic.listen-client-urls: 'http://172.16.80.201:2379,http://127.0.0.1:2379'# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 5# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: 'http://172.16.80.201:2380'# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: 'http://172.16.80.201:2379'# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: 'etcd1=http://172.16.80.201:2380,etcd2=http://172.16.80.202:2380,etcd3=http://172.16.80.203:2380'# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: # Path to the client server TLS key file. key-file: # Enable client cert authentication. client-cert-auth: false # Path to the client server TLS trusted CA cert file. trusted-ca-file: # Client TLS using generated certificates auto-tls: falsepeer-transport-security: # Path to the peer server TLS cert file. cert-file: # Path to the peer server TLS key file. key-file: # Enable peer client cert authentication. client-cert-auth: false # Path to the peer server TLS trusted CA cert file. trusted-ca-file: # Peer TLS using generated certificates. auto-tls: false# Enable debug-level logging for etcd.debug: falselogger: zap# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: ['stderr']# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: periodicauto-compaction-retention: \"1\"# Set level of detail for exported metrics, specify 'extensive' to include histogram metrics.# default is 'basic'metrics: 'basic'etcd21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# This is the configuration file for the etcd server.# Human-readable name for this member.name: 'etcd2'# Path to the data directory.data-dir: /var/lib/etcd/etcd2.etcd# Path to the dedicated wal directory.wal-dir: /var/lib/etcd/wal# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 10000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: 'http://172.16.80.202:2380'# List of comma separated URLs to listen on for client traffic.listen-client-urls: 'http://172.16.80.202:2379,http://127.0.0.1:2379'# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 5# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: 'http://172.16.80.202:2380'# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: 'http://172.16.80.202:2379'# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: 'etcd1=http://172.16.80.201:2380,etcd2=http://172.16.80.202:2380,etcd3=http://172.16.80.203:2380'# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: # Path to the client server TLS key file. key-file: # Enable client cert authentication. client-cert-auth: false # Path to the client server TLS trusted CA cert file. trusted-ca-file: # Client TLS using generated certificates auto-tls: falsepeer-transport-security: # Path to the peer server TLS cert file. cert-file: # Path to the peer server TLS key file. key-file: # Enable peer client cert authentication. client-cert-auth: false # Path to the peer server TLS trusted CA cert file. trusted-ca-file: # Peer TLS using generated certificates. auto-tls: false# Enable debug-level logging for etcd.debug: falselogger: zap# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [stderr]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: periodicauto-compaction-retention: \"1\"# Set level of detail for exported metrics, specify 'extensive' to include histogram metrics.# default is 'basic'metrics: 'basic'etcd31234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# This is the configuration file for the etcd server.# Human-readable name for this member.name: 'etcd3'# Path to the data directory.data-dir: /var/lib/etcd/etcd3.etcd# Path to the dedicated wal directory.wal-dir: /var/lib/etcd/wal# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 10000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: 'http://172.16.80.203:2380'# List of comma separated URLs to listen on for client traffic.listen-client-urls: 'http://172.16.80.203:2379,http://127.0.0.1:2379'# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 5# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: 'http://172.16.80.203:2380'# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: 'http://172.16.80.203:2379'# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: 'etcd1=http://172.16.80.201:2380,etcd2=http://172.16.80.202:2380,etcd3=http://172.16.80.203:2380'# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: # Path to the client server TLS key file. key-file: # Enable client cert authentication. client-cert-auth: false # Path to the client server TLS trusted CA cert file. trusted-ca-file: # Client TLS using generated certificates auto-tls: falsepeer-transport-security: # Path to the peer server TLS cert file. cert-file: # Path to the peer server TLS key file. key-file: # Enable peer client cert authentication. client-cert-auth: false # Path to the peer server TLS trusted CA cert file. trusted-ca-file: # Peer TLS using generated certificates. auto-tls: false# Enable debug-level logging for etcd.debug: falselogger: zap# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [stderr]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: periodicauto-compaction-retention: \"1\"# Set level of detail for exported metrics, specify 'extensive' to include histogram metrics.# default is 'basic'metrics: 'basic'启动etcd集群1234for NODE in 172.16.80.201 172.16.80.202 172.16.80.203;do ssh $NODE systemctl enable etcd ssh $NODE systemctl start etcd &amp;done检查etcd集群123456789101112export ETCDCTL_API=2etcdctl --endpoints 'http://172.16.80.201:2379,http://172.16.80.202:2379,http://172.16.80.202:2379' cluster-healthmember 222fd3b0bb4a5931 is healthy: got healthy result from http://172.16.80.203:2379member 8349ef180b115a83 is healthy: got healthy result from http://172.16.80.201:2379member f525d2d797a7c465 is healthy: got healthy result from http://172.16.80.202:2379cluster is healthyexport ETCDCTL_API=3etcdctl --endpoints='http://172.16.80.201:2379,http://172.16.80.202:2379,http://172.16.80.202:2379' endpoint healthhttp://172.16.80.201:2379 is healthy: successfully committed proposal: took = 2.879402mshttp://172.16.80.203:2379 is healthy: successfully committed proposal: took = 6.708566mshttp://172.16.80.202:2379 is healthy: successfully committed proposal: took = 7.187607msSSL/TLS加密此段翻译自官方文档etcd支持自动TLS、客户端证书身份认证、客户端到服务器端以及对等集群的加密通信生成证书为方便起见，这里使用CFSSL工具生成证书下载CFSSL12345mkdir ~/bincurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x ~/bin/&#123;cfssl,cfssljson&#125;export PATH=$PATH:~/bin创建工作目录12mkdir ~/cfsslcd ~/cfssl创建默认配置文件12cfssl print-defaults config &gt; ca-config.jsoncfssl print-defaults csr &gt; ca-csr.json证书类型介绍客户端证书用于服务器验证客户端身份服务器端证书用于客户端验证服务器端身份对等证书由etcd集群成员使用，同时使用客户端认证和服务器端认证配置CA修改ca-config.json说明expiry定义过期时间，这里的43800h为5年usages字段定义用途signing代表可以用于签发其他证书key encipherment代表将密钥加密server authclient auth12345678910111213141516171819202122232425262728293031323334&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"43800h\" &#125;, \"profiles\": &#123; \"server\": &#123; \"expiry\": \"43800h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\" ] &#125;, \"client\": &#123; \"expiry\": \"43800h\", \"usages\": [ \"signing\", \"key encipherment\", \"client auth\" ] &#125;, \"peer\": &#123; \"expiry\": \"43800h\", \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ] &#125; &#125; &#125;&#125;配置证书请求修改ca-csr.json，可以根据自己的需求修改对应字段1234567891011121314151617&#123; \"CN\": \"My own CA\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"US\", \"L\": \"CA\", \"O\": \"My Company Name\", \"ST\": \"San Francisco\", \"OU\": \"Org Unit 1\", \"OU\": \"Org Unit 2\" &#125; ]&#125;生成CA证书运行以下命令生成CA证书1cfssl gencert -initca ca-csr.json | cfssljson -bare ca -生成以下文件123ca-key.pemca.csrca.pemca-key.pem为CA的私钥，请妥善保管csr文件为证书请求文件，可以删除生成服务器端证书1cfssl print-defaults csr &gt; server.json修改server.json的CN和hosts字段，names字段按需修改说明hosts字段为列表，服务器端需要将自己作为客户端访问集群，可以使用hostname或者IP地址的形式定义hosts123456789101112131415161718192021&#123; \"CN\": \"example.net\", \"hosts\": [ \"127.0.0.1\", \"192.168.1.1\", \"ext.example.com\", \"coreos1.local\", \"coreos1\" ], \"key\": &#123; \"algo\": \"ecdsa\", \"size\": 256 &#125;, \"names\": [ &#123; \"C\": \"US\", \"L\": \"CA\", \"ST\": \"San Francisco\" &#125; ]&#125;创建服务器端证书和私钥1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server生成以下文件123server-key.pemserver.csrserver.pem生成客户端证书1cfssl print-defaults csr &gt; client.json修改client.json，客户端证书不需要hosts字段，只需要CN字段设置为client1234... \"CN\": \"client\", \"hosts\": [\"\"],...创建客户端证书和私钥1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client生成以下文件123client-key.pemclient.csrclient.pem生成对等证书1cfssl print-defaults csr &gt; member1.json修改member1.json的CN字段和hosts字段123456789... \"CN\": \"member1\", \"hosts\": [ \"192.168.122.101\", \"ext.example.com\", \"member1.local\", \"member1\" ],...创建member1的证书和密钥1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer member1.json | cfssljson -bare member1生成以下文件123member1-key.pemmember1.csrmember1.pem对于多个member需要重复此操作，用于生成相对应的对等证书验证证书1234openssl x509 -in ca.pem -text -nooutopenssl x509 -in server.pem -text -nooutopenssl x509 -in client.pem -text -nooutopenssl x509 -in member1.pem -text -noout示例1、客户端使用HTTPS传输数据给服务器端准备CA证书ca.pem，密钥对server.pem server-key.pem启动服务器端启动参数如下123456etcd --name infra0 \\--data-dir /var/lib/etcd/infra0 \\--cert-file=/path/to/server.pem \\--key-file=/path/to/server-key.pem \\--advertise-client-urls=https://127.0.0.1:2379 \\--listen-client-urls=https://127.0.0.1:2379客户端使用HTTPS访问服务器端使用curl加载CA证书测试HTTPS连接12345curl --cacert /path/to/ca.pem \\https://127.0.0.1:2379/v2/keys/foo \\-X PUT \\-d value=bar \\-v示例2、客户端使用客户端证书作为身份验证访问服务器端在示例1的基础上，需要客户端证书client.pem和client-key.pem启动服务器端启动参数如下，这里比示例1多了client-cert-auth和truested-ca-file12345678etcd --name infra0 \\--data-dir /var/lib/etcd/infra0 \\--cert-file=/path/to/server.pem \\--key-file=/path/to/server-key.pem \\--advertise-client-urls=https://127.0.0.1:2379 \\--listen-client-urls=https://127.0.0.1:2379 \\--client-cert-auth \\--trusted-ca-file=/path/to/ca.crt重复示例1的访问1curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v此命令结果会提示被服务器端拒绝123...routines:SSL3_READ_BYTES:sslv3 alert bad certificate...使用客户端证书访问服务器端1234567curl --cacert /path/to/ca.pem \\--cert /path/to/client.pem \\--key /path/to/client-key.pem \\-L https://127.0.0.1:2379/v2/keys/foo \\-X PUT \\-d value=bar \\-v命令结果包含以下信息身份认证成功1234...SSLv3, TLS handshake, CERT verify (15):...TLS handshake, Finished (20)示例3、在集群中传输安全和客户端证书这里需要为每个member配备对应的member证书，操作步骤见生成证书部分假设有2个member，这两个member都已生成对应的证书(member1.pem、member1-key.pem、member2.pem、member2-key.pem)etcd 成员将组成一个集群，集群中成员之间的所有通信将使用客户端证书进行加密和验证。etcd的输出将显示其连接的地址使用HTTPS。启动服务器端从https://discovery.etcd.io/new获取discovery_url作为启动集群的发现服务发现服务可以在内网环境搭建，详见github地址1DISCOVERY_URL=$(curl https://discovery.etcd.io/new)member1123456789etcd --name infra1 \\--data-dir /var/lib/etcd/infra1 \\--peer-client-cert-auth \\--peer-trusted-ca-file=/path/to/ca.pem \\--peer-cert-file=/path/to/member1.pem \\--peer-key-file=/path/to/member1-key.pem \\--initial-advertise-peer-urls=https://10.0.1.11:2380 \\--listen-peer-urls=https://10.0.1.11:2380 \\--discovery $&#123;DISCOVERY_URL&#125;member2123456789etcd --name infra2 \\--data-dir /var/lib/etcd/infra2 \\--peer-client-cert-auth \\--peer-trusted-ca-file=/path/to/ca.pem \\--peer-cert-file=/path/to/member2.pem \\--peer-key-file=/path/to/member2-key.pem \\--initial-advertise-peer-urls=https://10.0.1.12:2380 \\--listen-peer-urls=https://10.0.1.12:2380 \\--discovery $&#123;DISCOVERY_URL&#125;示例4、自动自签名对于只需要加密传输数据而不需要身份验证的场景，etcd支持使用自动生成的自签名证书加密传输数据启动服务器端1DISCOVERY_URL=$(curl https://discovery.etcd.io/new)member11234567etcd --name infra1 \\--data-dir /var/lib/etcd/infra1 \\--auto-tls \\--peer-auto-tls \\--initial-advertise-peer-urls=https://10.0.1.11:2380 \\--listen-peer-urls=https://10.0.1.11:2380 \\--discovery $&#123;DISCOVERY_URL&#125;member21234567etcd --name infra2 \\--data-dir /var/lib/etcd/infra2 \\--auto-tls \\--peer-auto-tls \\--initial-advertise-peer-urls=https://10.0.1.12:2380 \\--listen-peer-urls=https://10.0.1.12 :2380 \\--discovery $&#123;DISCOVERY_URL&#125;注意由于自签名证书不会进行身份认证，因此curl会返回错误，因此需要添加-k参数禁用证书链检查etcd维护操作查看所有的key12export ETCDCTL_API=3etcdctl get / --prefix --keys-only请求最大字节数官网文档说明max-request-bytes限制请求的大小，默认值是1572864，即1.5M。在某些场景可能会出现请求过大导致无法写入的情况，可以调大到10485760即10M。快照条目数量调整--snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘。从v3.2开始，--snapshot-count的默认值已从10000更改为100000。注意此参数具体数值可以通过根据实际情况调整过低会带来频繁的IO压力，影响集群可用性和写入吞吐量。过高则导致内存占用过高以及会让etcd的GC变慢历史数据压缩（针对v3的API）由于etcd保存了key的历史记录，因此能通过MVCC机制获取多版本的数据，需要定期压缩历史记录避免性能下降和空间耗尽。到达上限阈值时，集群将处于只读和只能删除key的状态，无法写操作。历史数据压缩只是针对数据的历史版本进行清理，清理之后只能读取到清理点之后的历史版本手动压缩清理revision为3之前的历史数据12export ETCDCTL_API=3etcdctl compact 3清理之后，访问revision3之前的数据会提示不存在123export ETCDCTL_API=3etcdctl get KEY_NAME --rev=2Error: etcdserver: mvcc: required revision has been compacted自动压缩启动参数中添加--auto-compaction-retention=1即为每小时压缩一次碎片整理(针对v3的API)在数据压缩操作之后，旧的revision被压缩，会产生内部碎片，这些内部碎片可以被etcd使用，但是仍消耗磁盘空间。碎片整理就是将这部分空间释放出来。12export ETCDCTL_API=3etcdctl defrag空间配额etcd通过--quota-backend-bytes参数来限制etcd数据库的大小，以字节为单位。默认是2147483648即2GB，最大值为8589934592即8GB。容量限制见官方文档数据备份(针对v3的API)快照备份通过快照etcd集群可以作备份数据的用途。可以通过快照备份的数据，将etcd集群恢复到快照的时间点。12export ETCDCTL_API=3etcdctl snapshot save /path/to/snapshot.db检查快照状态123456etcd --write-out=table snapshot status /path/to/snapshot.db+----------+----------+------------+------------+| HASH | REVISION | TOTAL KEYS | TOTAL SIZE |+----------+----------+------------+------------+| dd97719a | 24276 | 1113 | 3.0 MB |+----------+----------+------------+------------+基于快照的定期备份脚本12345678910111213#!/bin/shTIME=$(date +%Y%m%d)HOUR=$(date +%H)BACKUP_DIR=\"/data/etcd_backup/$&#123;TIME&#125;\"mkdir -p $BACKUP_DIRexport ETCDCTL_API=3/usr/local/bin/etcdctl --cacert=/etc/etcd/ssl/etcd-ca.pem \\ --cert=/etc/etcd/ssl/etcd-client.pem \\ --key=/etc/etcd/ssl/etcd-client-key.pem \\ --endpoints=https://member1:2379,https://member2:2379,https://member3:2379 \\ snapshot save $BACKUP_DIR/snapshot-$&#123;HOUR&#125;.db# 清理2天前的etcd备份find /data/etcd_backup -type d -mtime +2 -exec rm -rf &#123;&#125; \\;etcd镜像集群(针对v3的API)通过mirror-maker实时做镜像的方式同步数据，如果出现主机房服务挂了可以通过切换域名的形式切换到灾备机房；这个过程中数据是可以保持一致的。提前部署好两套etcd集群之后，可以在主集群上面运行以下命令1234567891011121314151617export ETCDCTL_API=3etcdctl make-mirror --no-dest-prefix=true http://mirror1:2379,http://mirror2:2379,http://mirror3:2379# 输出示例48854660466272077883689495010091067112511831241make-mirror的输出为30s一次，程序为前台运行，可以通过nohup &gt;/path/to/log 2&gt;&amp;1 &amp;的方式扔到后台运行etcd监控debug endpoint启动参数中添加--debug即可打开debug模式，etcd会在http://x.x.x.x:2379/debug路径下输出debug信息。由于debug信息很多，会导致性能下降。/debug/pprof为go语言runtime的endpoint，可以用于分析CPU、heap、mutex和goroutine利用率。这里示例为使用go命令获取etcd最耗时的操作12345678910111213141516171819202122232425$ go tool pprof http://127.0.0.1:2379/debug/pprof/profileFetching profile over HTTP from http://127.0.0.1:2379/debug/pprof/profileSaved profile in /root/pprof/pprof.etcd-3.2.24.samples.cpu.001.pb.gzFile: etcd-3.2.24Type: cpuTime: Feb 10, 2019 at 9:57pm (CST)Duration: 30s, Total samples = 60ms ( 0.2%)Entering interactive mode (type \"help\" for commands, \"o\" for options)(pprof) (pprof) (pprof) top10Showing nodes accounting for 60ms, 100% of 60ms totalShowing top 10 nodes out of 25 flat flat% sum% cum cum% 60ms 100% 100% 60ms 100% runtime.futex 0 0% 100% 10ms 16.67% github.com/coreos/etcd/cmd/vendor/github.com/coreos/etcd/etcdserver.(*raftNode).start.func1 0 0% 100% 10ms 16.67% github.com/coreos/etcd/cmd/vendor/github.com/coreos/etcd/etcdserver.(*raftNode).tick 0 0% 100% 10ms 16.67% github.com/coreos/etcd/cmd/vendor/github.com/coreos/etcd/raft.(*node).Tick 0 0% 100% 20ms 33.33% runtime.chansend 0 0% 100% 30ms 50.00% runtime.exitsyscall 0 0% 100% 30ms 50.00% runtime.exitsyscallfast 0 0% 100% 30ms 50.00% runtime.exitsyscallfast.func1 0 0% 100% 30ms 50.00% runtime.exitsyscallfast_pidle 0 0% 100% 60ms 100% runtime.futexwakeup(pprof) exitmetrics endpoint每个etcd节点都会在/metrics路径下输出监控信息，监控软件可以通过此路径获取指标信息具体的metrics信息可以参看官方文档--listen-metrics-urls定义metrics的location。--metrics可以定义basic和extensive这里通过curl命令来获取metrics信息1curl http://127.0.0.1:2379/metricshealth check这里通过curl命令来获取health信息，返回结果为json1curl http://127.0.0.1:2379/health返回结果如下123&#123; \"health\": \"true\"&#125;对接Prometheus配置文件HTTP123456global: scrape_interval: 10sscrape_configs: - job_name: etcd-cluster-monitoring static_configs: - targets: ['10.240.0.32:2379','10.240.0.33:2379','10.240.0.34:2379']HTTPS12345678910111213global: scrape_interval: 10sscrape_configs: - job_name: etcd-cluster-monitoring static_configs: - targets: ['10.240.0.32:2379','10.240.0.33:2379','10.240.0.34:2379'] scheme: https tls_config: # CA certificate to validate API server certificate with. ca_file: /path/to/etcd-ca.pem cert_file: /path/to/etcd-cert.pem key_file: /path/to/etcd-key.pem # insecure_skip_verify: true | false监控告警使用Alertmanager进行监控告警Prometheus 1.x 范例Prometheus 2.x 范例监控指标展示使用Grafana读取Prometheus的数据展示监控数据，Dashboard模板etcd故障处理leader节点故障leader节点故障，etcd集群会自动选举出新的leader。故障检测模型是基于超时的，因此选举新的leader节点不会在旧的leader节点故障之后立刻发生。选举leader期间，集群不会处理写入操作。选举期间的写入请求会进入队列等待处理，直至选出新的leader节点。已经发送给故障leader但尚未提交的数据可能会丢失。这是因为新的leader节点有权对旧leader节点的数据进行修改客户端会发现一些写入请求可能会超时，没有提交的数据会丢失。follower节点故障follower故障节点数量少于集群节点的一半时，etcd集群是可以正常工作的。例如3个节点故障了1个，5个节点故障了2个follower节点故障后，客户端的etcd库应该自动连接到etcd集群的其他成员。超过半数节点故障由于Raft算法的原理所限，超过半数的集群节点故障会导致etcd集群进入不可写入的状态。只要正常工作的节点超过集群节点的一半，那么etcd集群会自动选举leader节点并且自动恢复到健康状态如果无法修复多数节点，那么就需要走灾难恢复的操作流程网络分区由于网络故障，导致etcd集群被切分成两个或者更多的部分。那么占有多数节点的一方会成为可用集群，少数节点的一方不可写入。如果对等切分了集群，那么每个部分都不可用。这是因为Raft一致性算法保证了etcd是不存在脑裂现象。只要网络分区的故障解除，少数节点的一方会自动从多数节点一方识别出leader节点，然后恢复状态。集群启动失败只有超过半数成员启动完成之后，集群的bootstrap才会成功。Raft一致性算法保证了集群节点的数据一致性和稳定性，因此对于节点的恢复，更多的是恢复etcd节点服务，然后恢复数据新的集群可以删除所有成员的数据目录，然后重新走创建集群的步骤已有集群这个就要看无法启动的节点是数据文件损坏，还是其他原因导致的。这里以数据文件损坏为例。寻找正常的节点，使用etcdctl snapshot save命令保存出快照文件将故障节点的数据目录清空，使用etcdctl snapshot restore命令将数据恢复到数据目录使用etcdctl member list确认故障节点的信息使用etcdctl member remove删除故障节点使用etcdctl member add MEMBER_NAME --peer-urls=http://member:2379重新添加成员修改etcd启动参数--initial-cluster-state=existing启动故障节点的etcd服务etcd灾难恢复这里的灾难恢复，只能恢复v2或者v3的数据，不能同时恢复v2和v3。两套API是相互隔离的。针对v3的APIetcd v3的API提供了快照和恢复功能，可以在不损失快照点数据的情况下重建集群快照备份数据1ETCDCTL_API=3 etcdctl --endpoints http://member1:2379,http://member2:2379,http://member3:2379 snapshot save /path/to/snapshot.db恢复集群恢复etcd集群，只需要快照文件db即可。使用etcdctl snapshot restore命令还原数据时会自动创建新的etcd数据目录。恢复过程会覆盖快照文件里面的一些metadata（特别是member id和cluster id），该member会失去之前的id。覆盖metadata可以防止新成员无意中加入现有集群。从快照中恢复集群，必须以新集群启动。恢复时可以选择验证快照完整性hash。使用etcdctl snapshot save生成的快照，则具有完整性hash如果是直接从数据目录拷贝数据快照，则没有完整性hash，需要使用--skip-hash-check跳过检查恢复节点数据这里假定原有的集群节点为member1、member2、member3在member1、member2、member3上分别恢复快照数据123456789101112131415$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name member1 \\ --initial-cluster member1=http://member1:2380,member2=http://member2:2380,member3=http://member3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://member1:2380$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name member2 \\ --initial-cluster member1=http://member1:2380,member2=http://member2:2380,member3=http://member3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://member2:2380$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\ --name member3 \\ --initial-cluster member1=http://member1:2380,member2=http://member2:2380,member3=http://member3:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-advertise-peer-urls http://member3:2380启动etcd集群在member1、member2、member3上分别启动集群123456789101112131415$ etcd \\ --name member1 \\ --listen-client-urls http://member1:2379 \\ --advertise-client-urls http://member1:2379 \\ --listen-peer-urls http://member1:2380 &amp;$ etcd \\ --name member2 \\ --listen-client-urls http://member2:2379 \\ --advertise-client-urls http://member2:2379 \\ --listen-peer-urls http://member2:2380 &amp;$ etcd \\ --name member3 \\ --listen-client-urls http://member3:2379 \\ --advertise-client-urls http://member3:2379 \\ --listen-peer-urls http://member3:2380 &amp;针对v2的API备份数据123456ETCDCTL_API=3etcdctl backup \\ --data-dir /path/to/data-dir \\ --wal-dir /path/to/wal_dir \\ --backup-dir /path/to/backup_data_dir \\ --backup-wal-dir /path/to/backup_wal_dir清理数据目录12rm -rf /path/to/data-dirrm -rf /path/to/wal-dir恢复数据12mv /path/to/backup_data_dir /path/to/data-dirmv /path/to/backup_wal_dir /path/to/wal_dir启动etcd集群启动参数需要添加--force-new-cluster123etcd --data-dir /path/to/data-dir \\ --wal-dir /path/to/wal_dir \\ --force-new-clusteretcd版本升级这里可以参考etcd的升级文档etcd的FAQ摘自Frequently Asked Questions (FAQ)客户端是否需要向etcd集群的leader节点发送请求leader节点负责处理所有需要集群共识的请求（例如写请求）。客户端不需要知道哪个节点是leader，follower节点会将所有需要集群共识的请求转发给leader节点。所有节点都可以处理不需要集群共识的请求（例如序列化读取）。listen-client-urls、listen-peer-urls、advertise-client-urls、initial-advertise-peer-urls的区别listen-client-urls和listen-peer-urls指定etcd服务端用于接收传入连接的本地地址，要监听所有地址，请指定0.0.0.0作为监听地址。advertise-client-urls 和initial-advertise-peer-urls指定etcd的客户端及集群其他成员访问etcd服务的地址，此地址必须要被外部访问，因此不能设置127.0.0.1或者0.0.0.0等地址。为什么不能通过更改listen-peer-urls或者initial-advertise-peer-urls来更新etcdctl member list中列出的advertise peer urls每个member的advertise-peer-urls来自初始化集群时的initial-advertise-peer-urls参数在member启动完成后修改listen-peer-urls或者initial-advertise-peer-urls参数不会影响现有的advertise-peer-urls，因为修改此参数需要通过集群仲裁以避免出现脑裂修改advertise-peer-url请使用etcd member update命令操作系统要求etcd会将数据写入磁盘，因此高性能的磁盘会更好，推荐使用SSD默认存储配额为2GB，最大值为8GB为了避免使用swap或者内存不足，服务器内存至少要超过存储配额为什么etcd需要奇数个集群成员etcd集群需要通过大多数节点仲裁才能将集群状态更新到一致仲裁为(n/2)+1双数个集群成员并不比奇数个节点容错性强集群容错性列表Cluster SizeMajorityFailure Tolerance110220321431532642743853954集群最大节点数量理论上没有硬性限制，一般不超过7个节点建议5个节点，5个节点可以容忍2个节点故障下线，在大多数情况下已经足够更多的节点可以提供更好的可用性，但是写入性能会有影响部署跨数据中心的etcd集群是否合适跨数据中心的etcd集群可以提高可用性数据中心之间的网络延迟可能会影响节点的election默认的etcd配置可能会因为网络延迟频繁选举或者心跳超时，需要调整对应的参数为什么etcd会因为磁盘IO延迟而重新选举这是故意设计的磁盘IO延迟是leader节点存活指标的一部分磁盘IO延迟很高导致选举超时，即使leader节点在选举间隔内能处理网络信息（例如发送心跳），但它实际上是不可用的，因为它无法及时提交新的提议如果经常出现因磁盘IO延迟而重新选举，请关注一下磁盘或者修改etcd时间参数etcd性能压测这里参考官方文档性能指标延迟完成操作所需的时间吞吐量一段时间内完成的总操作数量通常情况下，平均延迟会随着吞吐量的增加而增加。etcd使用Raft一致性算法完成成员之间的数据同步并达成集群共识。集群的共识性能，尤其是提交延迟，主要受到两个方面限制。网络IO延迟磁盘IO延迟提交延迟的构成成员之间的网络往返时间RTT同一个数据中心内部的RTT是ms级别跨数据中心的RTT就需要考虑物理限制和网络质量fdatasync数据落盘时间机械硬盘fdatasync延迟通常在10ms左右固态硬盘则低于1ms其他延迟构成序列化etcd请求需要通过etcd后端boltdb的MVVC机制来完成，通常会在10ms完成。etcd定期将最近提交的请求快照，然后跟磁盘上的快照合并，这个操作过程会导致延迟出现峰值。正在进行的数据压缩也会影响到延迟，所以要跟业务错开benchmark跑分etcd自带的benchmark命令行工具可以用来测试etcd性能写入请求123456789101112131415161718192021222324# 假定 HOST_1 是 leader, 写入请求发到 leaderbenchmark --endpoints=$&#123;HOST_1&#125; \\ --conns=1 \\ --clients=1 \\ put --key-size=8 \\ --sequential-keys \\ --total=10000 \\ --val-size=256benchmark --endpoints=$&#123;HOST_1&#125; \\ --conns=100 \\ --clients=1000 \\ put --key-size=8 \\ --sequential-keys \\ --total=100000 \\ --val-size=256 # 写入发到所有成员benchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \\ --conns=100 \\ --clients=1000 \\ put --key-size=8 \\ --sequential-keys \\ --total=100000 \\ --val-size=256序列化读取123456789101112131415161718192021222324252627# Single connection read requestsbenchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \\ --conns=1 \\ --clients=1 \\ range YOUR_KEY \\ --consistency=l \\ --total=10000benchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \\ --conns=1 \\ --clients=1 \\ range YOUR_KEY \\ --consistency=s \\ --total=10000# Many concurrent read requestsbenchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \\ --conns=100 \\ --clients=1000 \\ range YOUR_KEY \\ --consistency=l \\ --total=100000benchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \\ --conns=100 \\ --clients=1000 \\ range YOUR_KEY \\ --consistency=s \\ --total=100000etcd性能调优参考官方文档etcd默认配置是基于同一个数据中心，网络延迟较低的情况。对于网络延迟较高，那么就需要优化心跳间隔和选举超时时间时间参数（time parameter）延迟不止有网络延迟，还可能受到节点磁盘IO影响。每一次超时设置应该包括请求发出到响应成功的时间。心跳间隔（heartbeat interval）leader节点通知各follower节点自己的存活信息。最佳实践是通过ping命令获取RTT最大值，然后设置为RTT的0.5~1.5倍。默认是100ms。选举超时（election timeout）follower节点在多久之后没收到leader节点的心跳信息，就开始选举新leader节点。默认是1000ms。选举超时应该设置为至少是RTT的10倍，以避免网络出现波动导致重新选举。快照（snapshot）etcd会将所有变更的key追加写入到wal日志文件中。一行记录一个key的变更，因此日志会不断增长。为避免日志过大，etcd会定期做快照。快照操作会保存当前系统状态并移除旧的日志。snapshot-count参数控制快照的频率，默认是10000，即每10000次变更会触发一次快照操作。如果内存使用率高并且磁盘使用率高，可以尝试调低这个参数。磁盘etcd集群对磁盘IO延迟非常的敏感。etcd需要存储变更日志、快照等操作，可能会导致磁盘IO出现很高的fsync延迟。磁盘IO延迟高会导致leader节点心跳信息超时、请求超时、重新选举等。etcd所使用的磁盘与系统盘分开data目录和wal目录分别挂载不同的磁盘有条件推荐使用SSD固态硬盘使用ionice调高etcd进程的IO优先级（这个针对etcd数据目录在系统盘的情况）1ionice -c2 -n0 -p `pgrep etcd`网络如果leader节点接收来自客户端的大量请求，无法及时处理follower的请求，那么follower节点处理的请求也会因此出现延迟。具体表现为follower会提示sending buffer is full。可以通过调高leader的网络优先级或者通过流量管控机制来提高对follower的请求响应。","categories":[],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"},{"name":"etcd","slug":"etcd","permalink":"https://luanlengli.github.io/tags/etcd/"}]},{"title":"Adobe Flash Player下载地址","slug":"Adobe-Flash-Player下载地址","date":"2018-12-18T01:37:30.000Z","updated":"2020-01-09T01:22:58.610Z","comments":true,"path":"2018/12/18/Adobe-Flash-Player下载地址.html","link":"","permalink":"https://luanlengli.github.io/2018/12/18/Adobe-Flash-Player下载地址.html","excerpt":"","text":"Flash Player for Internet Explorer - ActiveXhttps://fpdownload.macromedia.com/pub/flashplayer/latest/help/install_flash_player_ax.exeFlash Player for Firefox - NPAPIhttps://fpdownload.macromedia.com/pub/flashplayer/latest/help/install_flash_player.exeFlash Player for Opera and Chromium-based browsers - PPAPIhttps://fpdownload.macromedia.com/pub/flashplayer/latest/help/install_flash_player_ppapi.exe","categories":[{"name":"Windows","slug":"Windows","permalink":"https://luanlengli.github.io/categories/Windows/"}],"tags":[{"name":"Windows","slug":"Windows","permalink":"https://luanlengli.github.io/tags/Windows/"}]},{"title":"CentOS7使用社区YUM源安装Mariadb Galera集群","slug":"CentOS7使用社区YUM源安装Mariadb-Galera集群","date":"2018-12-10T16:07:07.000Z","updated":"2019-05-28T06:36:23.000Z","comments":true,"path":"2018/12/11/CentOS7使用社区YUM源安装Mariadb-Galera集群.html","link":"","permalink":"https://luanlengli.github.io/2018/12/11/CentOS7使用社区YUM源安装Mariadb-Galera集群.html","excerpt":"","text":"简介本文以MariaDB官方文档为基础，记录操作步骤安装Mariadb数据库前的准备工作准备虚拟机三台IP地址主机名CPU内存172.16.10.101db123G172.16.10.102db223G172.16.10.103db323G添加Mariadb官方YUM源，下面以Mariadb 10.1为例官方YUM源编辑器使用以下命令快速添加YUM源1234567tee /etc/yum.repos.d/mariadb.repo &lt;&lt;-'EOF'[mariadb]name = MariaDBbaseurl = https://mirrors.ustc.edu.cn/mariadb/yum/10.1/centos7-amd64gpgkey=https://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDBgpgcheck=1 EOF刷新YUM缓存1yum makecache查看Mariadb相关的安装包，注意软件包版本和对应的YUM源名字1yum list MariaDB* galera关闭firewalld防火墙1systemctl disable firewalld --now设置主机名（设置三台虚拟机主机名分别为db1，db2，db3）123hostnamectl set-hostname db1hostnamectl set-hostname db2hostnamectl set-hostname db3编辑/etc/hosts文件12345cat /etc/hosts &lt;&lt;EOF172.16.10.101 db1172.16.10.102 db2172.16.10.103 db3EOF关闭SELINUX123setenforce 0 sed -i 's,^SELINUX=enforcing,SELINUX=disabled,g' /etc/selinux/config部署MariaDB Galera集群安装相关软件包1yum install MariaDB-server MariaDB-client MariaDB-client启用xtrabackup-v2功能需要额外安装percona提供的软件包1yum install https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.10/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.10-1.el7.x86_64.rpm启动MariaDB数据库在db1上启动MariaDB数据库，设置galera集群同步账号,进行安全初始化1234systemctl start mariadb.servicemysql -uroot -e \"grant all privileges on *.* to 'sst'@'localhost' identified by 'password';\" mysql_secure_installation systemctl stop mariadb.service编辑MariaDB配置文件在三个节点上编辑MariaDB配置文件，以开启galera集群功能123456789101112131415161718192021222324252627282930313233343536373839404142cat /etc/my.cnf.d/galera.cnf[server][mysqld]# 监听哪个地址，这里每个节点填对应的ip地址bind-address=172.16.10.101 # 监听哪个端口port = 3306 # 设置默认字符编码集collation-server = utf8_general_ciinit-connect = SET NAMES utf8character-set-server = utf8# 设置日志路径log-error = /var/log/mariadb/mariadb.log# 设置binloglog-bin = mysql-binbinlog_format=ROW# 设置默认数据目录datadir = /var/lib/mysql/ # 设置默认存储引擎default-storage-engine=innodbinnodb_autoinc_lock_mode=2 [galera]wsrep_on=ONwsrep_provider=/usr/lib64/galera/libgalera_smm.so# galera集群名字wsrep_cluster_name=\"galera_cluster\" # 本节点的主机名，这里每个节点填对应的ip地址wsrep_node_name=\"db1\" wsrep_cluster_address = \"gcomm://172.16.10.101:4567,172.16.10.102:4567,172.16.10.103:4567\"wsrep_provider_options = \"gmcast.listen_addr=tcp://172.16.10.101:4567;ist.recv_addr=172.16.10.101:4568\" wsrep_node_address=\"172.16.10.101:4567\" # 设置galera集群同步的方法和用户名密码wsrep_sst_auth=sst:passwordwsrep_sst_method=xtrabackup-v2max_connections = 10000 key_buffer_size = 64Mmax_heap_table_size = 64Mtmp_table_size = 64Minnodb_buffer_pool_size = 128M[embedded][mariadb][mariadb-10.1]启动galera集群创建集群在db1上运行galera_new_cluster命令1galera_new_cluster查看集群状态在db1上查看集群状态123456# mysql -uroot -p -e \"show status like 'wsrep_cluster_size';\"+--------------------------+--------------------------------------+| Variable_name | Value |+--------------------------+--------------------------------------+| wsrep_cluster_size | 1 |+--------------------------+--------------------------------------+监控MariaDB日志监控db1上的MariaDB日志在启动其他节点的时候，能看到其他节点加入到galera集群1tail -f /var/log/mariadb/mariadb.log启动其他节点数据库在db2和db3上运行MariaDB数据库1systemctl start mariadb检查集群状态在db1上检查集群状态123456mysql -uroot -p -e \"show status like 'wsrep_cluster_size';\"+--------------------------+--------------------------------------+| Variable_name | Value |+--------------------------+--------------------------------------+| wsrep_cluster_size | 3 |+--------------------------+--------------------------------------+验证MariaDB galera集群的同步功能是否正常在db1上创建用户、数据库123mysql -uroot -p -e \"user add testuser;\"mysql -uroot -p -e \"create database testdb;\"mysql -uroot -p -e \"grant all privileges on testdb.* to 'testuser'@'localhost' identified by 'password';\"在db2上检查用户、数据库是否存在12mysql -uroot -p -e \"select user,host from mysql.user;\"mysql -uroot -p -e \"show databases;\"在db3上删除用户和数据库12mysql -uroot -p -e \"delete user 'testuser'\"mysql -uroot -p -e \"drop database testdb\"在db1上检查用户和数据库是否还在12mysql -uroot -p -e \"select user,host from mysql.user;\"mysql -uroot -p -e \"show databases;\"至此，MariaDB galera集群已经部署完成","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/tags/Linux/"},{"name":"MySQL","slug":"MySQL","permalink":"https://luanlengli.github.io/tags/MySQL/"}]},{"title":"【弃坑】使用Docker打包shadowsocks-libev镜像","slug":"使用Docker打包shadowsocks-libev镜像","date":"2018-12-10T16:03:06.000Z","updated":"2019-07-16T03:13:01.000Z","comments":true,"path":"2018/12/11/使用Docker打包shadowsocks-libev镜像.html","link":"","permalink":"https://luanlengli.github.io/2018/12/11/使用Docker打包shadowsocks-libev镜像.html","excerpt":"","text":"介绍记录一下使用Dockerfile制作shadowsocks-libev镜像的过程基于Alpine-3.8和shadowsocks-libev-v3.2.3制作参考shadowsocks-libev项目上面的Dockerfile以下是Dockerfile内容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# README# /* BUILD IMAGE */# dokcer image build -t shadowsocks-libev:v3.2.3 .# /* RUN CONATIANER */# docker container run -d -e SERVER_PORT=111 -e PASSWORD='password' -e METHOD='aes-256-gcm' --net host --name ss-libev-port111 shadowsocks-libev:v3.2-alpine3.8# /* SS-SERVER HELP */# shadowsocks-libev 3.2.3# # maintained by Max Lv &lt;max.c.lv@gmail.com&gt; and Linus Yang &lt;laokongzi@gmail.com&gt;# # usage:# # ss-server# # -s &lt;server_host&gt; Host name or IP address of your remote server.# -p &lt;server_port&gt; Port number of your remote server.# -l &lt;local_port&gt; Port number of your local server.# -k &lt;password&gt; Password of your remote server.# -m &lt;encrypt_method&gt; Encrypt method: rc4-md5, # aes-128-gcm, aes-192-gcm, aes-256-gcm,# aes-128-cfb, aes-192-cfb, aes-256-cfb,# aes-128-ctr, aes-192-ctr, aes-256-ctr,# camellia-128-cfb, camellia-192-cfb,# camellia-256-cfb, bf-cfb,# chacha20-ietf-poly1305,# xchacha20-ietf-poly1305,# salsa20, chacha20 and chacha20-ietf.# The default cipher is chacha20-ietf-poly1305.# # [-a &lt;user&gt;] Run as another user.# [-f &lt;pid_file&gt;] The file path to store pid.# [-t &lt;timeout&gt;] Socket timeout in seconds.# [-c &lt;config_file&gt;] The path to config file.# [-n &lt;number&gt;] Max number of open files.# [-i &lt;interface&gt;] Network interface to bind.# [-b &lt;local_address&gt;] Local address to bind.# # [-u] Enable UDP relay.# [-U] Enable UDP relay and disable TCP relay.# [-6] Resovle hostname to IPv6 address first.# # [-d &lt;addr&gt;] Name servers for internal DNS resolver.# [--reuse-port] Enable port reuse.# [--fast-open] Enable TCP fast open.# with Linux kernel &gt; 3.7.0.# [--acl &lt;acl_file&gt;] Path to ACL (Access Control List).# [--manager-address &lt;addr&gt;] UNIX domain socket address.# [--mtu &lt;MTU&gt;] MTU of your network interface.# [--mptcp] Enable Multipath TCP on MPTCP Kernel.# [--no-delay] Enable TCP_NODELAY.# [--key &lt;key_in_base64&gt;] Key of your remote server.# [--plugin &lt;name&gt;] Enable SIP003 plugin. (Experimental)# [--plugin-opts &lt;options&gt;] Set SIP003 plugin options. (Experimental)# # [-v] Verbose mode.# [-h, --help] Print this message.FROM alpine:3.8ENV TZ 'Asia/Shanghai'ENV SS_VERSION 3.2.3ENV SS_DOWNLOAD_URL https://github.com/shadowsocks/shadowsocks-libev/releases/download/v$&#123;SS_VERSION&#125;/shadowsocks-libev-$&#123;SS_VERSION&#125;.tar.gzRUN apk upgrade \\ &amp;&amp; apk add bash tzdata libsodium rng-tools \\ &amp;&amp; apk add --virtual .build-deps \\ autoconf \\ automake \\ xmlto \\ build-base \\ curl \\ c-ares-dev \\ libev-dev \\ libtool \\ linux-headers \\ udns-dev \\ libsodium-dev \\ mbedtls-dev \\ pcre-dev \\ udns-dev \\ tar \\ git \\ &amp;&amp; wget -q -O - $SS_DOWNLOAD_URL | tar xz \\ &amp;&amp; (cd shadowsocks-libev-$&#123;SS_VERSION&#125; \\ &amp;&amp; ./configure --prefix=/usr --disable-documentation \\ &amp;&amp; make install) \\ &amp;&amp; ln -sf /usr/share/zoneinfo/$TZ /etc/localtime \\ &amp;&amp; echo $TZ &gt; /etc/timezone \\ &amp;&amp; apk del .build-deps \\ &amp;&amp; rm -rf shadowsocks-libev-$&#123;SS_VERSION&#125; \\ /var/cache/apk/* \\ &amp;&amp; apk add --no-cache \\ rng-tools \\ $(scanelf --needed --nobanner /usr/bin/ss-* \\ | awk '&#123; gsub(/,/, \"\\nso:\", $2); print \"so:\" $2 &#125;' \\ | sort -u) CMD [\"/usr/bin/ss-server\"]","categories":[{"name":"Docker","slug":"Docker","permalink":"https://luanlengli.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://luanlengli.github.io/tags/Docker/"}]},{"title":"二进制部署 kubernetes v1.11.x 高可用集群","slug":"二进制部署 kubernetes v1.11.x 高可用集群","date":"2018-12-08T03:34:08.000Z","updated":"2019-06-23T15:13:54.000Z","comments":true,"path":"2018/12/08/二进制部署 kubernetes v1.11.x 高可用集群.html","link":"","permalink":"https://luanlengli.github.io/2018/12/08/二进制部署 kubernetes v1.11.x 高可用集群.html","excerpt":"","text":"由于Kubernetes已经release了1.15版本，此文档不再维护！更新记录2019年2月13日由于runc逃逸漏洞CVE-2019-5736，根据kubernetes的文档建议，修改docker-ce版本为18.09.22019年1月7日添加基于ingress-nginx使用域名+HTTPS的方式访问kubernetes-Dashboard2019年1月2日添加RBAC规则，修复kube-apiserver无法访问kubelet的问题2019年1月1日调整master节点和worker节点的操作步骤，添加CoreDNS的configmap中的hosts静态解析2018年12月28日修改kube-prometheus部分，修复Prometheus的Targets无法发现的问题2018年12月26日修改kubernetes-dashboard链接指向2018年12月25日修改kubele.config.file路径问题2018年12月18日修改kubelet和kube-proxy启动时加载config file2018年12月17日添加EFK部署内容2018年12月16日添加prometheus-operator部署内容2018年12月14日添加helm部署内容，拆分etcd的server证书和client证书2018年12月13日添加rook-ceph部署内容2018年12月12日添加Metrics-Server内容2018年12月11日添加Dashboard、Ingress内容2018年12月10日添加kube-flannel、calico、CoreDNS内容2018年12月9日分拆master节点和work节点的内容2018年12月8日初稿介绍本次部署方式为二进制可执行文件的方式部署注意请根据自己的实际情况调整对于生产环境部署，请注意某些参数的选择如无特殊说明，均在k8s-m1节点上执行参考博文感谢两位大佬的文章，这里整合一下两位大佬的内容，结合自己的理解整理本文【漠然】Kubernetes 1.10.1 集群搭建【漠然】使用 Bootstrap Token 完成 TLS Bootstrapping【张馆长】二进制部署Kubernetes v1.11.x(1.12.x) HA可选软件版本kubernetes v1.11.5 【下载链接需要爬墙，自行解决】docker-ce 18.03cni-plugin v0.7.4etcd v3.3.10网络信息基于CNI的模式实现容器网络Cluster IP CIDR: 10.244.0.0/16Service Cluster IP CIDR: 10.96.0.0/12Service DNS IP: 10.96.0.10Kubernetes API VIP: 172.16.80.200节点信息操作系统可采用 Ubuntu Server 16.04+ 和 CentOS 7.4+，本文使用CentOS 7.6 (1810) Minimal由keepalived提供VIP由haproxy提供kube-apiserver四层负载均衡由于实验环境受限，以3台服务器同时作为master和worker节点运行服务器配置请根据实际情况适当调整IP地址主机名角色CPU内存172.16.80.201k8s-m1master+worker48G172.16.80.202k8s-m2master+worker48G172.16.80.203k8s-m3master+worker48G目录说明/usr/local/bin/：存放kubernetes和etcd二进制文件/opt/cni/bin/： 存放cni-plugin二进制文件/etc/etcd/：存放etcd配置文件和SSL证书/etc/kubernetes/：存放kubernetes配置和SSL证书/etc/cni/net.d/：安装CNI插件后会在这里生成配置文件$HOME/.kube/：kubectl命令会在家目录下建立此目录，用于保存访问kubernetes集群的配置和缓存$HOME/.helm/：helm命令会建立此目录，用于保存helm缓存和repository信息事前准备事情准备在所有服务器上都需要完成部署过程以root用户完成所有服务器网络互通，k8s-m1可以通过SSH证书免密登录到其他master节点，用于分发文件编辑/etc/hosts1234567cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1 localhost172.16.80.200 k8s-vip172.16.80.201 k8s-m1172.16.80.202 k8s-m2172.16.80.203 k8s-m3EOF时间同步服务集群系统需要各节点时间同步参考链接：RHEL7官方文档这里使用公网对时，如果需要内网对时，请自行配置123yum install -y chronysystemctl enable chronydsystemctl start chronyd关闭firewalld和SELINUX（可根据实际情况自行决定关闭不需要的服务）1234567891011121314151617systemctl stop firewalldsystemctl disable firewalldsystemctl mask firewalld# 清空iptables规则iptables -t filter -Fiptables -t filter -Xiptables -t nat -Fiptables -t nat -Xiptables -t mangle -Fiptables -t mangle -Xiptables -t raw -Fiptables -t raw -Xiptables -t security -Fiptables -t security -Xiptables -P INPUT ACCEPTiptables -P FORWARD ACCEPTiptables -P OUTPUT ACCEPT12setenforce 0sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config禁用swap12swapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab添加sysctl参数12345678910111213141516171819202122232425262728293031323334cat &gt; /etc/sysctl.d/centos.conf &lt;&lt;EOF # 最大文件句柄数fs.file-max=1024000# 最大文件打开数fs.nr_open=1024000# 端口最大的监听队列的长度net.core.somaxconn=4096# 在CentOS7.4引入了一个新的参数来控制内核的行为。 # /proc/sys/fs/may_detach_mounts 默认设置为0# 当系统有容器运行的时候，需要将该值设置为1。fs.may_detach_mounts = 1# 最大文件打开数fs.nr_open=1024000# 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤net.bridge.bridge-nf-call-arptables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# 关闭严格校验数据包的反向路径net.ipv4.conf.default.rp_filter=0net.ipv4.conf.all.rp_filter=0# 打开ipv4数据包转发net.ipv4.ip_forward=1# 允许应用程序能够绑定到不属于本地网卡的地址net.ipv4.ip_nonlocal_bind=1 # 表示最大限度使用物理内存，然后才是swap空间vm.swappiness = 0 # 设置系统TCP连接keepalive的持续时间，默认7200net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10EOF# 让sysctl参数生效sysctl --system确保操作系统已经最新1yum update -y安装软件包123yum groups install base -yyum install epel-release bash-completion-extras -yyum install git vim ipvsadm tree dstat iotop htop socat ipset conntrack -y加载ipvs模块12345678910111213# 开机自动加载ipvs模块cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashipvs_modules=\"ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4\"for kernel_module in \\$&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename \\$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe \\$&#123;kernel_module&#125; fidoneEOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs安装docker-ce 18.09.212345yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine -yyum install -y yum-utils device-mapper-persistent-data lvm2 -yyum-config-manager --add-repo http://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.reposed -e 's,download.docker.com,mirrors.aliyun.com/docker-ce,g' -i /etc/yum.repos.d/docker-ce.repoyum install docker-ce-18.09.2 -y创建docker配置文件12345678910111213mkdir -p /etc/dockercat&gt;/etc/docker/daemon.json&lt;&lt;EOF&#123; \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"insecure-registries\": [], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\": \"3\" &#125;, \"max-concurrent-downloads\": 10&#125;EOF配置docker命令补全12cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/source /etc/bash_completion.d/docker配置docker服务开机自启动12systemctl enable docker.servicesystemctl start docker.service查看docker信息1docker info禁用docker源12# 为避免yum update时更新docker，将docker源禁用sed -e 's,enabled=1,enabled=0,g' -i /etc/yum.repos.d/docker-ce.repo确保以最新的内核启动系统1reboot定义集群变量注意这里的变量只对当前会话生效，如果会话断开或者重启服务器，都需要重新定义变量HostArray定义集群中所有节点的主机名和IPMasterArray定义master节点的主机名和IPWorkerArray定义worker节点的主机名和IP，这里master和worker都在一起，所以MasterArray和WorkerArray一样VIP_IFACE定义keepalived的VIP绑定在哪一个网卡ETCD_SERVERS以MasterArray的信息生成etcd集群服务器列表ETCD_INITIAL_CLUSTER以MasterArray信息生成etcd集群初始化列表POD_DNS_SERVER_IP定义Pod的DNS服务器IP地址12345678910111213141516171819202122232425262728293031323334declare -A HostArray MasterArray WorkerArray# 声明所有节点的信息HostArray=(['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203)# 如果节点多，可以按照下面的方式声明Array# HostArray=(['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203 ['k8s-n1']=172.16.80.204 ['k8s-n2']=172.16.80.205)# 声明master节点信息MasterArray=(['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203)# 声明worker节点信息WorkerArray=(['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203)# VIP=\"172.16.80.200\"KUBE_APISERVER=\"https://172.16.80.200:8443\"# etcd版本号# kubeadm-v1.11.5里面使用的是v3.2.18，这里直接上到最新的v3.3.10ETCD_VERSION=\"v3.3.10\"# kubernetes版本号KUBERNETES_VERSION=\"v1.11.5\"# cni-plugin版本号# kubernetes YUM源里用的还是v0.6.0版，这里上到最新的v0.7.4CNI_PLUGIN_VERSION=\"v0.7.4\"# 声明VIP所在的网卡名称，以ens33为例VIP_IFACE=\"ens33\"# 声明etcd_serverETCD_SERVERS=$( xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#^#https://#;s#$#:2379#;$s#\\n##' | paste -d, -s - )ETCD_INITIAL_CLUSTER=$( for i in $&#123;!MasterArray[@]&#125;;do echo $i=https://$&#123;MasterArray[$i]&#125;:2380; done | sort | paste -d, -s - )# 定义POD_CLUSTER_CIDRPOD_NET_CIDR=\"10.244.0.0/16\"# 定义SVC_CLUSTER_CIDRSVC_CLUSTER_CIDR=\"10.96.0.0/12\"# 定义POD_DNS_SERVER_IPPOD_DNS_SERVER_IP=\"10.96.0.10\"下载所需软件包创建工作目录12mkdir -p /root/softwarecd /root/software二进制文件需要分发到master和worker节点1234567891011121314151617181920212223242526272829303132333435# 下载kubernetes二进制包echo \"--- 下载kubernetes $&#123;KUBERNETES_VERSION&#125; 二进制包 ---\"wget https://dl.k8s.io/$&#123;KUBERNETES_VERSION&#125;/kubernetes-server-linux-amd64.tar.gztar xzf kubernetes-server-linux-amd64.tar.gz \\ kubernetes/server/bin/hyperkube \\ kubernetes/server/bin/kube-controller-manager \\ kubernetes/server/bin/kubectl \\ kubernetes/server/bin/apiextensions-apiserver \\ kubernetes/server/bin/kube-proxy \\ kubernetes/server/bin/kube-apiserver \\ kubernetes/server/bin/kubelet \\ kubernetes/server/bin/kubeadm \\ kubernetes/server/bin/kube-aggregator \\ kubernetes/server/bin/kube-scheduler \\ kubernetes/server/bin/cloud-controller-manager \\ kubernetes/server/bin/mounterchown -R root:root kubernetes/server/bin/*chmod 0755 kubernetes/server/bin/*# 这里需要先拷贝kubectl到/usr/local/bin目录下，用于生成kubeconfig文件rsync -avpt kubernetes/server/bin/kubectl /usr/local/bin/kubectl# 下载etcd二进制包echo \"--- 下载etcd $&#123;ETCD_VERSION&#125; 二进制包 ---\"wget https://github.com/etcd-io/etcd/releases/download/$&#123;ETCD_VERSION&#125;/etcd-$&#123;ETCD_VERSION&#125;-linux-amd64.tar.gztar xzf etcd-$&#123;ETCD_VERSION&#125;-linux-amd64.tar.gz \\ etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl \\ etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdchown root:root etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdchmod 0755 etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcd# 下载CNI-pluginecho \"--- 下载cni-plugins $&#123;CNI_PLUGIN_VERSION&#125; 二进制包 ---\"wget https://github.com/containernetworking/plugins/releases/download/$&#123;CNI_PLUGIN_VERSION&#125;/cni-plugins-amd64-$&#123;CNI_PLUGIN_VERSION&#125;.tgzmkdir /root/software/cni-pluginstar xzf cni-plugins-amd64-$&#123;CNI_PLUGIN_VERSION&#125;.tgz -C /root/software/cni-plugins/生成集群Key和Certificates说明本次部署，需要为etcd-server、etcd-client、kube-apiserver、kube-controller-manager、kube-scheduler、kube-proxy生成证书。另外还需要生成sa、front-proxy-ca、front-proxy-client证书用于集群的其他功能。要注意CA JSON文件的CN(Common Name)与O(Organization)等内容是会影响Kubernetes组件认证的。CN Common Name，kube-apiserver会从证书中提取该字段作为请求的用户名（User Name）O Oragnization，kube-apiserver会从证书中提取该字段作为请求用户的所属组（Group）CA是自签名根证书，用来给后续各种证书签名kubernetes集群的所有状态信息都保存在etcd中，kubernetes组件会通过kube-apiserver读写etcd里面的信息etcd如果暴露在公网且没做SSL/TLS验证，那么任何人都能读写数据，那么很可能会无端端在kubernetes集群里面多了挖坑Pod或者肉鸡Pod本文使用CFSSL创建证书，证书有效期10年建立证书过程在k8s-m1上完成下载CFSSL工具123456wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/local/bin/cfssl-certinfowget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljsonchmod 755 /usr/local/bin/cfssl-certinfo \\ /usr/local/bin/cfssl \\ /usr/local/bin/cfssljson创建工作目录12mkdir -p /root/pki /root/master /root/workercd /root/pki创建用于生成证书的json文件ca-config.json12345678910111213141516171819202122232425262728293031323334353637cat &gt; ca-config.json &lt;&lt;EOF&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"kubernetes\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125;, \"etcd-server\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125;, \"etcd-client\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"client auth\" ], \"expiry\": \"87600h\" &#125; &#125; &#125;&#125;EOFca-csr.json123456789101112131415161718cat &gt; ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"Kubernetes\", \"OU\": \"System\" &#125; ]&#125;EOFetcd-ca-csr.json123456789101112131415161718cat &gt; etcd-ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"etcd\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"etcd\", \"OU\": \"Etcd Security\" &#125; ]&#125;EOFetcd-server-csr.json123456789101112131415161718cat &gt; etcd-server-csr.json &lt;&lt;EOF&#123; \"CN\": \"etcd-server\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"etcd\", \"OU\": \"Etcd Security\" &#125; ]&#125;EOFetcd-client-csr.json123456789101112131415161718192021cat &gt; etcd-client-csr.json &lt;&lt;EOF&#123; \"CN\": \"etcd-client\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"hosts\": [ \"\" ], \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"etcd\", \"OU\": \"Etcd Security\" &#125; ]&#125;EOFkube-apiserver-csr.json123456789101112131415161718cat &gt; kube-apiserver-csr.json &lt;&lt;EOF&#123; \"CN\": \"kube-apiserver\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"Kubernetes\", \"OU\": \"System\" &#125; ]&#125;EOFkube-manager-csr.json123456789101112131415161718cat &gt; kube-manager-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-controller-manager\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"system:kube-controller-manager\", \"OU\": \"System\" &#125; ]&#125;EOFkube-scheduler-csr.json123456789101112131415161718cat &gt; kube-scheduler-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-scheduler\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"system:kube-scheduler\", \"OU\": \"System\" &#125; ]&#125;EOFkube-proxy-csr.json123456789101112131415161718cat &gt; kube-proxy-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-proxy\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"system:kube-proxy\", \"OU\": \"System\" &#125; ]&#125;EOFkube-admin-csr.json123456789101112131415161718cat &gt; kube-admin-csr.json &lt;&lt;EOF&#123; \"CN\": \"admin\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125;EOFfront-proxy-ca-csr.json123456789cat &gt; front-proxy-ca-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;&#125;EOFfront-proxy-client-csr.json123456789cat &gt; front-proxy-client-csr.json &lt;&lt;EOF&#123; \"CN\": \"front-proxy-client\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;&#125;EOFsa-csr.json123456789101112131415161718cat &gt; sa-csr.json &lt;&lt;EOF&#123; \"CN\": \"service-accounts\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"Guangdong\", \"L\": \"Guangzhou\", \"O\": \"Kubernetes\", \"OU\": \"System\" &#125; ]&#125;EOF创建etcd证书etcd-ca证书12echo '--- 创建etcd-ca证书 ---'cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-caetcd-server证书1234567echo '--- 创建etcd-server证书 ---'cfssl gencert \\ -ca=etcd-ca.pem \\ -ca-key=etcd-ca-key.pem \\ -config=ca-config.json \\ -hostname=127.0.0.1,$(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | paste -d, -s -) \\ -profile=etcd-server etcd-server-csr.json | cfssljson -bare etcd-serveretcd-client证书123456echo '--- 创建etcd-client证书 ---'cfssl gencert \\ -ca=etcd-ca.pem \\ -ca-key=etcd-ca-key.pem \\ -config=ca-config.json \\ -profile=etcd-client etcd-client-csr.json | cfssljson -bare etcd-client创建kubernetes证书kubernetes-CA 证书123echo '--- 创建kubernetes-ca证书 ---'# 创建kubernetes-ca证书cfssl gencert -initca ca-csr.json | cfssljson -bare kube-cakube-apiserver证书12345678910echo '--- 创建kube-apiserver证书 ---'# 创建kube-apiserver证书# 这里的hostname字段中的10.96.0.1要跟上文提到的service cluster ip cidr对应cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -hostname=10.96.0.1,127.0.0.1,localhost,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,$&#123;VIP&#125;,$(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | paste -d, -s -) \\ -profile=kubernetes \\ kube-apiserver-csr.json | cfssljson -bare kube-apiserverkube-controller-manager证书12345678echo '--- 创建kube-controller-manager证书 ---'# 创建kube-controller-manager证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-manager-csr.json | cfssljson -bare kube-controller-managerkube-scheduler证书12345678echo '--- 创建kube-scheduler证书 ---'# 创建kube-scheduler证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-schedulerkube-proxy证书12345678echo '--- 创建kube-proxy证书 ---'# 创建kube-proxy证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxykube-admin证书12345678echo '--- 创建kube-admin证书 ---'# 创建kube-admin证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-admin-csr.json | cfssljson -bare kube-adminFront Proxy证书123456789echo '--- 创建Front Proxy Certificate证书 ---'# 创建Front Proxy Certificate证书cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-cacfssl gencert \\ -ca=front-proxy-ca.pem \\ -ca-key=front-proxy-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ front-proxy-client-csr.json | cfssljson -bare front-proxy-clientService Account证书12345678echo '--- 创建service account证书 ---'# 创建创建service account证书cfssl gencert \\ -ca=kube-ca.pem \\ -ca-key=kube-ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ sa-csr.json | cfssljson -bare sabootstrap-token1234567BOOTSTRAP_TOKEN=$(dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d \"=+/[:space:]\" | dd bs=32 count=1 2&gt;/dev/null)echo \"BOOTSTRAP_TOKEN: $&#123;BOOTSTRAP_TOKEN&#125;\"# 创建token.csv文件cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,\"system:bootstrappers\"EOFencryption.yaml1234567891011121314151617ENCRYPTION_TOKEN=$(head -c 32 /dev/urandom | base64)echo \"ENCRYPTION_TOKEN: $&#123;ENCRYPTION_TOKEN&#125;\"# 创建encryption.yaml文件cat &gt; encryption.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: $&#123;ENCRYPTION_TOKEN&#125; - identity: &#123;&#125;EOFaudit-policy.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166echo '--- 创建创建高级审计配置 ---'# 创建高级审计配置cat &gt;&gt; audit-policy.yaml &lt;&lt;EOFapiVersion: audit.k8s.io/v1beta1kind: Policyrules: # The following requests were manually identified as high-volume and low-risk, # so drop them. - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" # core resources: [\"endpoints\", \"services\", \"services/status\"] - level: None # Ingress controller reads 'configmaps/ingress-uid' through the unsecured port. # TODO(#46983): Change this to the ingress controller service account. users: [\"system:unsecured\"] namespaces: [\"kube-system\"] verbs: [\"get\"] resources: - group: \"\" # core resources: [\"configmaps\"] - level: None users: [\"kubelet\"] # legacy kubelet identity verbs: [\"get\"] resources: - group: \"\" # core resources: [\"nodes\", \"nodes/status\"] - level: None userGroups: [\"system:nodes\"] verbs: [\"get\"] resources: - group: \"\" # core resources: [\"nodes\", \"nodes/status\"] - level: None users: - system:kube-controller-manager - system:kube-scheduler - system:serviceaccount:kube-system:endpoint-controller verbs: [\"get\", \"update\"] namespaces: [\"kube-system\"] resources: - group: \"\" # core resources: [\"endpoints\"] - level: None users: [\"system:apiserver\"] verbs: [\"get\"] resources: - group: \"\" # core resources: [\"namespaces\", \"namespaces/status\", \"namespaces/finalize\"] - level: None users: [\"cluster-autoscaler\"] verbs: [\"get\", \"update\"] namespaces: [\"kube-system\"] resources: - group: \"\" # core resources: [\"configmaps\", \"endpoints\"] # Don't log HPA fetching metrics. - level: None users: - system:kube-controller-manager verbs: [\"get\", \"list\"] resources: - group: \"metrics.k8s.io\" # Don't log these read-only URLs. - level: None nonResourceURLs: - /healthz* - /version - /swagger* # Don't log events requests. - level: None resources: - group: \"\" # core resources: [\"events\"] # node and pod status calls from nodes are high-volume and can be large, don't log responses for expected updates from nodes - level: Request users: [\"kubelet\", \"system:node-problem-detector\", \"system:serviceaccount:kube-system:node-problem-detector\"] verbs: [\"update\",\"patch\"] resources: - group: \"\" # core resources: [\"nodes/status\", \"pods/status\"] omitStages: - \"RequestReceived\" - level: Request userGroups: [\"system:nodes\"] verbs: [\"update\",\"patch\"] resources: - group: \"\" # core resources: [\"nodes/status\", \"pods/status\"] omitStages: - \"RequestReceived\" # deletecollection calls can be large, don't log responses for expected namespace deletions - level: Request users: [\"system:serviceaccount:kube-system:namespace-controller\"] verbs: [\"deletecollection\"] omitStages: - \"RequestReceived\" # Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data, # so only log at the Metadata level. - level: Metadata resources: - group: \"\" # core resources: [\"secrets\", \"configmaps\"] - group: authentication.k8s.io resources: [\"tokenreviews\"] omitStages: - \"RequestReceived\" # Get repsonses can be large; skip them. - level: Request verbs: [\"get\", \"list\", \"watch\"] resources: - group: \"\" # core - group: \"admissionregistration.k8s.io\" - group: \"apiextensions.k8s.io\" - group: \"apiregistration.k8s.io\" - group: \"apps\" - group: \"authentication.k8s.io\" - group: \"authorization.k8s.io\" - group: \"autoscaling\" - group: \"batch\" - group: \"certificates.k8s.io\" - group: \"extensions\" - group: \"metrics.k8s.io\" - group: \"networking.k8s.io\" - group: \"policy\" - group: \"rbac.authorization.k8s.io\" - group: \"scheduling.k8s.io\" - group: \"settings.k8s.io\" - group: \"storage.k8s.io\" omitStages: - \"RequestReceived\" # Default level for known APIs - level: RequestResponse resources: - group: \"\" # core - group: \"admissionregistration.k8s.io\" - group: \"apiextensions.k8s.io\" - group: \"apiregistration.k8s.io\" - group: \"apps\" - group: \"authentication.k8s.io\" - group: \"authorization.k8s.io\" - group: \"autoscaling\" - group: \"batch\" - group: \"certificates.k8s.io\" - group: \"extensions\" - group: \"metrics.k8s.io\" - group: \"networking.k8s.io\" - group: \"policy\" - group: \"rbac.authorization.k8s.io\" - group: \"scheduling.k8s.io\" - group: \"settings.k8s.io\" - group: \"storage.k8s.io\" omitStages: - \"RequestReceived\" # Default level for all other requests. - level: Metadata omitStages: - \"RequestReceived\"EOF创建kubeconfig文件说明kubeconfig 文件用于组织关于集群、用户、命名空间和认证机制的信息。命令行工具 kubectl 从 kubeconfig 文件中得到它要选择的集群以及跟集群 API server 交互的信息。默认情况下，kubectl 会从 $HOME/.kube 目录下查找文件名为 config 的文件。注意： 用于配置集群访问信息的文件叫作 kubeconfig文件，这是一种引用配置文件的通用方式，并不是说它的文件名就是 kubeconfig。kube-controller-manager.kubeconfig1234567891011121314151617181920echo \"Create kube-controller-manager kubeconfig...\"# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=kube-ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-controller-manager.kubeconfig# 设置客户端认证参数kubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfigkube-scheduler.kubeconfig1234567891011121314151617181920echo \"Create kube-scheduler kubeconfig...\"# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=kube-ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-scheduler.kubeconfig# 设置客户端认证参数kubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfigkube-proxy.kubeconfig1234567891011121314151617181920echo \"Create kube-proxy kubeconfig...\"# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=kube-ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials system:kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfigkube-admin.kubeconfig1234567891011121314151617181920echo \"Create kube-admin kubeconfig...\"# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=kube-ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-admin.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubernetes-admin \\ --client-certificate=kube-admin.pem \\ --client-key=kube-admin-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-admin.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubernetes-admin \\ --kubeconfig=kube-admin.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-admin.kubeconfigbootstrap.kubeconfig123456789101112131415161718echo \"Create kubelet bootstrapping kubeconfig...\"# 设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=kube-ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig清理证书CSR文件12echo '--- 删除*.csr文件 ---'rm -rf *csr修改文件权限123chown root:root *pem *kubeconfig *yaml *csvchmod 0444 *pem *kubeconfig *yaml *csvchmod 0400 *key.pem检查生成的文件123456789101112131415161718192021222324252627282930313233ls -l | grep -v json-r--r--r-- 1 root root 113 Dec 6 15:36 audit-policy.yaml-r--r--r-- 1 root root 2207 Dec 6 15:36 bootstrap.kubeconfig-r--r--r-- 1 root root 240 Dec 6 15:36 encryption.yaml-r-------- 1 root root 1675 Dec 6 15:36 etcd-ca-key.pem-r--r--r-- 1 root root 1375 Dec 6 15:36 etcd-ca.pem-r-------- 1 root root 1679 Dec 6 15:36 etcd-client-key.pem-r--r--r-- 1 root root 1424 Dec 6 15:36 etcd-client.pem-r-------- 1 root root 1679 Dec 6 15:36 etcd-server-key.pem-r--r--r-- 1 root root 1468 Dec 6 15:36 etcd-server.pem-r-------- 1 root root 1679 Dec 6 15:36 front-proxy-ca-key.pem-r--r--r-- 1 root root 1143 Dec 6 15:36 front-proxy-ca.pem-r-------- 1 root root 1675 Dec 6 15:36 front-proxy-client-key.pem-r--r--r-- 1 root root 1188 Dec 6 15:36 front-proxy-client.pem-r-------- 1 root root 1679 Dec 6 15:36 kube-admin-key.pem-r--r--r-- 1 root root 6345 Dec 6 15:36 kube-admin.kubeconfig-r--r--r-- 1 root root 1419 Dec 6 15:36 kube-admin.pem-r-------- 1 root root 1675 Dec 6 15:36 kube-apiserver-key.pem-r--r--r-- 1 root root 1688 Dec 6 15:36 kube-apiserver.pem-r-------- 1 root root 1679 Dec 6 15:36 kube-ca-key.pem-r--r--r-- 1 root root 1387 Dec 6 15:36 kube-ca.pem-r-------- 1 root root 1679 Dec 6 15:36 kube-controller-manager-key.pem-r--r--r-- 1 root root 6449 Dec 6 15:36 kube-controller-manager.kubeconfig-r--r--r-- 1 root root 1476 Dec 6 15:36 kube-controller-manager.pem-r-------- 1 root root 1675 Dec 6 15:36 kube-proxy-key.pem-r--r--r-- 1 root root 6371 Dec 6 15:36 kube-proxy.kubeconfig-r--r--r-- 1 root root 1440 Dec 6 15:36 kube-proxy.pem-r-------- 1 root root 1675 Dec 6 15:36 kube-scheduler-key.pem-r--r--r-- 1 root root 6395 Dec 6 15:36 kube-scheduler.kubeconfig-r--r--r-- 1 root root 1452 Dec 6 15:36 kube-scheduler.pem-r-------- 1 root root 1675 Dec 6 15:36 sa-key.pem-r--r--r-- 1 root root 1432 Dec 6 15:36 sa.pem-r--r--r-- 1 root root 80 Dec 6 15:36 token.csvkubernetes-master节点本节介绍如何部署kubernetes master节点master节点说明原则上，master节点不应该运行业务Pod，且不应该暴露到公网环境！！边界节点，应该交由worker节点或者运行Ingress的节点来承担以kubeadm部署为例，部署完成后，会给master节点添加node-role.kubernetes.io/master=&#39;&#39;标签（Labels）并且会对带有此标签的节点添加node-role.kubernetes.io/master:NoSchedule污点（taints），这样不能容忍此污点的Pod无法调度到master节点本文中，在kubelet启动参数里，默认添加node-role.kubernetes.io/node=&#39;&#39;标签（Labels），且没有对master节点添加node-role.kubernetes.io/master:NoSchedule污点（taints）生产环境中最好参照kubeadm，对master节点添加node-role.kubernetes.io/master=&#39;&#39;标签（Labels）和node-role.kubernetes.io/master:NoSchedule污点（taints）kube-apiserver以 REST APIs 提供 Kubernetes 资源的 CRUD,如授权、认证、存取控制与 API 注册等机制。关闭默认非安全端口8080,在安全端口 6443 接收 https 请求严格的认证和授权策略 (x509、token、RBAC)开启 bootstrap token 认证，支持 kubelet TLS bootstrapping使用 https 访问 kubelet、etcd，加密通信kube-controller-manager通过核心控制循环(Core Control Loop)监听 Kubernetes API的资源来维护集群的状态，这些资源会被不同的控制器所管理，如 Replication Controller、NamespaceController 等等。而这些控制器会处理着自动扩展、滚动更新等等功能。关闭非安全端口，在安全端口 10252 接收 https 请求使用 kubeconfig 访问 kube-apiserver 的安全端口kube-scheduler负责将一个(或多个)容器依据调度策略分配到对应节点上让容器引擎(如 Docker)执行。调度受到 QoS 要求、软硬性约束、亲和性(Affinity)等等因素影响。HAProxy提供多个 API Server 的负载均衡(Load Balance)监听VIP的8443端口负载均衡到三台master节点的6443端口Keepalived提供虚拟IP位址(VIP),来让vip落在可用的master主机上供所有组件访问master节点提供健康检查脚本用于切换VIP添加用户这里强迫症发作，指定了UID和GID不指定UID和GID也可以12345678echo '--- master节点添加用户 ---'for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE ---\" ssh $&#123;NODE&#125; /usr/sbin/groupadd -r -g 10000 kube ssh $&#123;NODE&#125; /usr/sbin/groupadd -r -g 10001 etcd ssh $&#123;NODE&#125; /usr/sbin/useradd -r -g kube -u 10000 -s /bin/false kube ssh $&#123;NODE&#125; /usr/sbin/useradd -r -g etcd -u 10001 -s /bin/false etcddone创建目录12345678910111213141516171819202122232425262728293031echo '--- master节点创建目录 ---'for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE ---\" echo \"--- 创建目录 ---\" ssh $&#123;NODE&#125; /usr/bin/mkdir -p /etc/etcd/ssl \\ /etc/kubernetes/pki \\ /etc/kubernetes/manifests \\ /var/lib/etcd \\ /var/lib/kubelet \\ /var/run/kubernetes \\ /var/log/kube-audit \\ /etc/cni/net.d \\ /opt/cni/bin echo \"--- 修改目录权限 ---\" ssh $&#123;NODE&#125; /usr/bin/chmod 0755 /etc/etcd \\ /etc/etcd/ssl \\ /etc/kubernetes \\ /etc/kubernetes/pki \\ /var/lib/etcd \\ /var/lib/kubelet \\ /var/log/kube-audit \\ /var/run/kubernetes \\ /etc/cni/net.d \\ /opt/cni/bin echo \"--- 修改目录属组 ---\" ssh $&#123;NODE&#125; chown -R etcd:etcd /etc/etcd/ /var/lib/etcd ssh $&#123;NODE&#125; chown -R kube:kube /etc/kubernetes \\ /var/lib/kubelet \\ /var/log/kube-audit \\ /var/run/kubernetesdone分发证书文件和kubeconfig到master节点1234567891011121314151617181920212223242526272829303132333435for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"---- $NODE ----\" echo '---- 分发etcd证书 ----' rsync -avpt /root/pki/etcd-ca-key.pem \\ /root/pki/etcd-ca.pem \\ /root/pki/etcd-client-key.pem \\ /root/pki/etcd-client.pem \\ /root/pki/etcd-server-key.pem \\ /root/pki/etcd-server.pem \\ $NODE:/etc/etcd/ssl/ echo '---- 分发kubeconfig文件 yaml文件 token.csv ----' rsync -avpt /root/pki/kube-admin.kubeconfig \\ /root/pki/kube-controller-manager.kubeconfig \\ /root/pki/kube-scheduler.kubeconfig \\ /root/pki/audit-policy.yaml \\ /root/pki/encryption.yaml \\ /root/pki/token.csv \\ $NODE:/etc/kubernetes/ echo '---- 分发sa证书 kube证书 front-proxy证书 ----' rsync -avpt /root/pki/etcd-ca.pem \\ /root/pki/etcd-client-key.pem \\ /root/pki/etcd-client.pem \\ /root/pki/front-proxy-ca.pem \\ /root/pki/front-proxy-client-key.pem \\ /root/pki/front-proxy-client.pem \\ /root/pki/kube-apiserver-key.pem \\ /root/pki/kube-apiserver.pem \\ /root/pki/kube-ca.pem \\ /root/pki/kube-ca-key.pem \\ /root/pki/sa-key.pem \\ /root/pki/sa.pem \\ $NODE:/etc/kubernetes/pki/ ssh $NODE chown -R etcd:etcd /etc/etcd ssh $NODE chown -R kube:kube /etc/kubernetesdone分发二进制文件在k8s-m1上操作1234567891011121314151617echo '--- 分发kubernetes和etcd二进制文件 ---'for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE ---\" rsync -avpt /root/software/kubernetes/server/bin/hyperkube \\ /root/software/kubernetes/server/bin/kube-controller-manager \\ /root/software/kubernetes/server/bin/kubectl \\ /root/software/kubernetes/server/bin/apiextensions-apiserver \\ /root/software/kubernetes/server/bin/kube-apiserver \\ /root/software/kubernetes/server/bin/kubeadm \\ /root/software/kubernetes/server/bin/kube-aggregator \\ /root/software/kubernetes/server/bin/kube-scheduler \\ /root/software/kubernetes/server/bin/cloud-controller-manager \\ /root/software/kubernetes/server/bin/mounter \\ /root/software/etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl \\ /root/software/etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcd \\ $NODE:/usr/local/bin/done部署配置Keepalived和HAProxy在k8s-m1上操作切换工作目录1cd /root/master安装Keepalived和HAProxy12345for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"---- $NODE ----\" echo \"---- 安装haproxy和keepalived ----\" ssh $NODE yum install keepalived haproxy -ydone配置keepalived编辑keepalived.conf模板替换keepalived.conf的字符串编辑check_haproxy.sh1234567891011121314151617181920212223242526272829303132333435cat &gt; keepalived.conf.example &lt;&lt;EOFvrrp_script haproxy-check &#123; script \"/bin/bash /etc/keepalived/check_haproxy.sh\" interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance haproxy-vip &#123; state BACKUP priority 101 interface &#123;&#123; VIP_IFACE &#125;&#125; virtual_router_id 47 advert_int 3 unicast_peer &#123; &#125; virtual_ipaddress &#123; &#123;&#123; VIP &#125;&#125; &#125; track_script &#123; haproxy-check &#125;&#125;EOF# 替换字符sed -r -e \"s#\\&#123;\\&#123; VIP \\&#125;\\&#125;#$&#123;VIP&#125;#\" \\ -e \"s#\\&#123;\\&#123; VIP_IFACE \\&#125;\\&#125;#$&#123;VIP_IFACE&#125;#\" \\ -e '/unicast_peer/r '&lt;(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#^#\\t#') \\ keepalived.conf.example &gt; keepalived.conf12345678910111213cat &gt; check_haproxy.sh &lt;&lt;EOF#!/bin/bashVIRTUAL_IP=$&#123;VIP&#125;errorExit() &#123; echo \"*** $*\" 1&gt;&amp;2 exit 1&#125;if ip addr | grep -q \\$VIRTUAL_IP ; then curl -s --max-time 2 --insecure https://\\$&#123;VIRTUAL_IP&#125;:8443/ -o /dev/null || errorExit \"Error GET https://\\$&#123;VIRTUAL_IP&#125;:8443/\"fiEOF配置haproxy编辑haproxy.cfg模板123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051cat &gt; haproxy.cfg.example &lt;&lt;EOFglobal maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30sdefaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15sfrontend monitor-in bind $&#123;VIP&#125;:33305 mode http option httplog monitor-uri /monitorlisten stats bind $&#123;VIP&#125;:8006 mode http stats enable stats hide-version stats uri /stats stats refresh 30s stats realm Haproxy\\ Statistics stats auth admin:adminfrontend k8s-api bind $&#123;VIP&#125;:8443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-apibackend k8s-api mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100EOF# 替换字符sed -e '$r '&lt;(paste &lt;( seq -f' server k8s-api-%g' $&#123;#MasterArray[@]&#125; ) &lt;( xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#$#:6443 check#')) haproxy.cfg.example &gt; haproxy.cfg分发配置文件到master节点1234567for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"---- $NODE ----\" rsync -avpt haproxy.cfg $NODE:/etc/haproxy/ rsync -avpt keepalived.conf \\ check_haproxy.sh \\ $NODE:/etc/keepalived/done启动keepalived和haproxy1234for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"---- $NODE ----\" ssh $NODE systemctl enable --now keepalived haproxydone验证VIP需要大约十秒的时间等待keepalived和haproxy服务起来这里由于后端的kube-apiserver服务还没启动，只测试是否能ping通VIP如果VIP没起来，就要去确认一下各master节点的keepalived服务是否正常12sleep 15ping -c 4 $VIP部署etcd集群每个etcd节点的配置都需要做对应更改在k8s-m1上操作配置etcd.service文件123456789101112131415161718cat &gt; etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServiceDocumentation=https://coreos.com/etcd/docs/latest/After=network.target[Service]User=etcdType=notifyExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yamlRestart=on-failureRestartSec=10LimitNOFILE=65536[Install]WantedBy=multi-user.targetAlias=etcd3.serviceEOFetcd.config.yaml模板关于各个参数的说明可以看这里123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100cat &gt; etcd.config.yaml.example &lt;&lt;EOF# This is the configuration file for the etcd server.# Human-readable name for this member.name: '&#123;HOSTNAME&#125;'# Path to the data directory.data-dir: '/var/lib/etcd/&#123;HOSTNAME&#125;.data/'# Path to the dedicated wal directory.wal-dir: '/var/lib/etcd/&#123;HOSTNAME&#125;.wal/'# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 5000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: 'https://&#123;PUBLIC_IP&#125;:2380'# List of comma separated URLs to listen on for client traffic.listen-client-urls: 'https://&#123;PUBLIC_IP&#125;:2379,http://127.0.0.1:2379'# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 3# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: 'https://&#123;PUBLIC_IP&#125;:2380'# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: 'https://&#123;PUBLIC_IP&#125;:2379'# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: '$&#123;ETCD_INITIAL_CLUSTER&#125;'# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-k8s-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: '/etc/etcd/ssl/etcd-server.pem' # Path to the client server TLS key file. key-file: '/etc/etcd/ssl/etcd-server-key.pem' # Enable client cert authentication. client-cert-auth: true # Path to the client server TLS trusted CA cert file. trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem' # Client TLS using generated certificates auto-tls: truepeer-transport-security: # Path to the peer server TLS cert file. cert-file: '/etc/etcd/ssl/etcd-server.pem' # Path to the peer server TLS key file. key-file: '/etc/etcd/ssl/etcd-server-key.pem' # Enable peer client cert authentication. client-cert-auth: true # Path to the peer server TLS trusted CA cert file. trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem' # Peer TLS using generated certificates. auto-tls: true# Enable debug-level logging for etcd.debug: falselogger: 'zap'# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [default]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: 'periodic'auto-compaction-retention: '1'# Set level of detail for exported metrics, specify 'extensive' to include histogram metrics.# default is 'basic'metrics: 'basic'EOF分发配置文件123456789101112# 根据节点信息替换文本，分发到各etcd节点for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" sed -e \"s/&#123;HOSTNAME&#125;/$NODE/g\" \\ -e \"s/&#123;PUBLIC_IP&#125;/$&#123;MasterArray[$NODE]&#125;/g\" \\ etcd.config.yaml.example &gt; etcd.config.yaml.$&#123;NODE&#125; rsync -avpt etcd.config.yaml.$&#123;NODE&#125; $&#123;NODE&#125;:/etc/etcd/etcd.config.yaml rsync -avpt etcd.service $&#123;NODE&#125;:/usr/lib/systemd/system/etcd.service ssh $&#123;NODE&#125; systemctl daemon-reload ssh $&#123;NODE&#125; chown -R etcd:etcd /etc/etcd rm -rf etcd.config.yaml.$&#123;NODE&#125;done启动etcd集群etcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 systemctl start etcd 会卡住一段时间，为正常现象启动之后可以通过etcdctl命令查看集群状态12345for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" ssh $NODE systemctl enable etcd ssh $NODE systemctl start etcd &amp;done为方便维护，可使用alias简化etcdctl命令1234cat &gt;&gt; /root/.bashrc &lt;&lt;EOFalias etcdctl2=\"export ETCDCTL_API=2;etcdctl --ca-file '/etc/etcd/ssl/etcd-ca.pem' --cert-file '/etc/etcd/ssl/etcd-client.pem' --key-file '/etc/etcd/ssl/etcd-client-key.pem' --endpoints $&#123;ETCD_SERVERS&#125;\"alias etcdctl3=\"export ETCDCTL_API=3;etcdctl --cacert=/etc/etcd/ssl/etcd-ca.pem --cert=/etc/etcd/ssl/etcd-client.pem --key=/etc/etcd/ssl/etcd-client-key.pem --endpoints=$&#123;ETCD_SERVERS&#125;\"EOF验证etcd集群状态etcd提供v2和v3两套API，kubernetes使用v3123456789101112131415161718192021222324252627282930313233# 应用上面定义的aliassource /root/.bashrc# 使用v2 API访问etcd的集群状态etcdctl2 cluster-health# 示例输出member 222fd3b0bb4a5931 is healthy: got healthy result from https://172.16.80.203:2379member 8349ef180b115a83 is healthy: got healthy result from https://172.16.80.201:2379member f525d2d797a7c465 is healthy: got healthy result from https://172.16.80.202:2379cluster is healthy# 使用v2 API访问etcd成员列表etcdctl2 member list# 示例输出222fd3b0bb4a5931: name=k8s-m3 peerURLs=https://172.16.80.203:2380 clientURLs=https://172.16.80.203:2379 isLeader=false8349ef180b115a83: name=k8s-m1 peerURLs=https://172.16.80.201:2380 clientURLs=https://172.16.80.201:2379 isLeader=falsef525d2d797a7c465: name=k8s-m2 peerURLs=https://172.16.80.202:2380 clientURLs=https://172.16.80.202:2379 isLeader=true# 使用v3 API访问etcd的endpoint状态etcdctl3 endpoint health# 示例输出https://172.16.80.201:2379 is healthy: successfully committed proposal: took = 2.879402mshttps://172.16.80.203:2379 is healthy: successfully committed proposal: took = 6.708566mshttps://172.16.80.202:2379 is healthy: successfully committed proposal: took = 7.187607ms# 使用v3 API访问etcd成员列表etcdctl3 member list --write-out=table# 示例输出+------------------+---------+--------+----------------------------+----------------------------+| ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS |+------------------+---------+--------+----------------------------+----------------------------+| 222fd3b0bb4a5931 | started | k8s-m3 | https://172.16.80.203:2380 | https://172.16.80.203:2379 || 8349ef180b115a83 | started | k8s-m1 | https://172.16.80.201:2380 | https://172.16.80.201:2379 || f525d2d797a7c465 | started | k8s-m2 | https://172.16.80.202:2380 | https://172.16.80.202:2379 |+------------------+---------+--------+----------------------------+----------------------------+Master组件服务master组件配置模板kube-apiserver.conf--allow-privileged=true启用容器特权模式--apiserver-count=3指定集群运行模式，其它节点处于阻塞状态--audit-policy-file=/etc/kubernetes/audit-policy.yaml 基于audit-policy.yaml文件定义的内容启动审计功能--authorization-mode=Node,RBAC开启 Node 和 RBAC 授权模式，拒绝未授权的请求--disable-admission-plugins=和--enable-admission-plugins禁用和启用准入控制插件。准入控制插件会在请求通过认证和授权之后、对象被持久化之前拦截到达apiserver的请求。准入控制插件依次执行，因此需要注意顺序。如果插件序列中任何一个拒绝了请求，则整个请求将立刻被拒绝并返回错误给客户端。关于admission-plugins官方文档里面有推荐配置，这里直接采用官方配置，注意针对不同kubernetes版本都会有不一样的配置，具体可以看这里--enable-bootstrap-token-auth=true启用 kubelet bootstrap 的 token 认证--experimental-encryption-provider-config=/etc/kubernetes/encryption.yaml启用加密特性将Secret数据加密存储到etcd--insecure-port=0关闭监听非安全端口8080--runtime-config=api/all=true启用所有版本的 APIs--service-cluster-ip-range=10.96.0.0/12指定 Service Cluster IP 地址段--service-node-port-range=30000-32767指定 NodePort 的端口范围--token-auth-file=/etc/kubernetes/token.csv保存bootstrap的token信息--target-ram-mb配置缓存大小，参考值为节点数*601234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253cat &gt; kube-apiserver.conf.example &lt;&lt;EOFKUBE_APISERVER_ARGS=\" \\\\--advertise-address=&#123;PUBLIC_IP&#125; \\\\--allow-privileged=true \\\\--apiserver-count=3 \\\\--audit-log-maxage=30 \\\\--audit-log-maxbackup=3 \\\\--audit-log-maxsize=1000 \\\\--audit-log-path=/var/log/kube-audit/audit.log \\\\--audit-policy-file=/etc/kubernetes/audit-policy.yaml \\\\--authorization-mode=Node,RBAC \\\\--bind-address=0.0.0.0 \\\\--client-ca-file=/etc/kubernetes/pki/kube-ca.pem \\\\--disable-admission-plugins=PersistentVolumeLabel \\\\--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset \\\\--enable-aggregator-routing=true \\\\--enable-bootstrap-token-auth=true \\\\--enable-garbage-collector=true \\\\--etcd-compaction-interval=1h \\\\--etcd-cafile=/etc/kubernetes/pki/etcd-ca.pem \\\\--etcd-certfile=/etc/kubernetes/pki/etcd-client.pem \\\\--etcd-keyfile=/etc/kubernetes/pki/etcd-client-key.pem \\\\--etcd-servers=$ETCD_SERVERS \\\\--experimental-encryption-provider-config=/etc/kubernetes/encryption.yaml \\\\--event-ttl=1h \\\\--feature-gates=PodShareProcessNamespace=true,ExpandPersistentVolumes=true \\\\--insecure-port=0 \\\\--kubelet-client-certificate=/etc/kubernetes/pki/kube-apiserver.pem \\\\--kubelet-client-key=/etc/kubernetes/pki/kube-apiserver-key.pem \\\\--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\\\--logtostderr=true \\\\--max-mutating-requests-inflight=500 \\\\--max-requests-inflight=1500 \\\\--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\\\--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\\\--requestheader-allowed-names=aggregator \\\\--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\\\--requestheader-extra-headers-prefix=X-Remote-Extra- \\\\--requestheader-group-headers=X-Remote-Group \\\\--requestheader-username-headers=X-Remote-User \\\\--runtime-config=api/all=true \\\\--secure-port=6443 \\\\--service-account-key-file=/etc/kubernetes/pki/sa.pem \\\\--service-cluster-ip-range=10.96.0.0/12 \\\\--service-node-port-range=30000-32767 \\\\--storage-backend=etcd3 \\\\--target-ram-mb=300 \\\\--tls-cert-file=/etc/kubernetes/pki/kube-apiserver.pem \\\\--tls-private-key-file=/etc/kubernetes/pki/kube-apiserver-key.pem \\\\--token-auth-file=/etc/kubernetes/token.csv \\\\--v=2 \\\\\"EOFkube-controller-manager.conf--allocate-node-cidrs=true在cloud provider上分配和设置pod的CIDR--cluster-cidr集群内的pod的CIDR范围，需要 --allocate-node-cidrs设为true--experimental-cluster-signing-duration=8670h0m0s指定 TLS Bootstrap 证书的有效期--feature-gates=RotateKubeletServerCertificate=true开启 kublet server 证书的自动更新特性--horizontal-pod-autoscaler-use-rest-clients=true能够使用自定义资源（Custom Metrics）进行自动水平扩展--leader-elect=true集群运行模式，启用选举功能，被选为 leader 的节点负责处理工作，其它节点为阻塞状态--node-cidr-mask-size=24集群中node cidr的掩码--service-cluster-ip-range=10.96.0.0/16指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致--terminated-pod-gc-thresholdexit状态的pod超过多少会触发gc123456789101112131415161718192021222324252627282930cat &gt; kube-controller-manager.conf.example &lt;&lt;EOFKUBE_CONTROLLER_MANAGER_ARGS=\" \\\\--address=0.0.0.0 \\\\--allocate-node-cidrs=true \\\\--cluster-cidr=$POD_NET_CIDR \\\\--cluster-signing-cert-file=/etc/kubernetes/pki/kube-ca.pem \\\\--cluster-signing-key-file=/etc/kubernetes/pki/kube-ca-key.pem \\\\--concurrent-service-syncs=10 \\\\--concurrent-serviceaccount-token-syncs=20 \\\\--controllers=*,bootstrapsigner,tokencleaner \\\\--enable-garbage-collector=true \\\\--experimental-cluster-signing-duration=8670h0m0s \\\\--feature-gates=RotateKubeletServerCertificate=true,ExpandPersistentVolumes=true \\\\--horizontal-pod-autoscaler-sync-period=10s \\\\--horizontal-pod-autoscaler-use-rest-clients=true \\\\--kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\--leader-elect=true \\\\--logtostderr=true \\\\--node-cidr-mask-size=24 \\\\--node-monitor-grace-period=40s \\\\--node-monitor-period=5s \\\\--pod-eviction-timeout=2m0s \\\\--root-ca-file=/etc/kubernetes/pki/kube-ca.pem \\\\--service-account-private-key-file=/etc/kubernetes/pki/sa-key.pem \\\\--service-cluster-ip-range=$SVC_CLUSTER_CIDR \\\\--terminated-pod-gc-threshold=12500 \\\\--use-service-account-credentials=true \\\\--v=2 \\\\\"EOFkube-scheduler.conf--leader-elect=true集群运行模式，启用选举功能，被选为 leader 的节点负责处理工作，其它节点为阻塞状态12345678910cat &gt; kube-scheduler.conf.example &lt;&lt;EOFKUBE_SCHEDULER_ARGS=\"\\\\--address=0.0.0.0 \\\\--algorithm-provider=DefaultProvider \\\\--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\--leader-elect=true \\\\--logtostderr=true \\\\--v=2 \\\\\"EOFsystemd服务文件kube-apiserver.service123456789101112131415161718cat &gt; kube-apiserver.service &lt;&lt;EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]User=kubeEnvironmentFile=-/etc/kubernetes/kube-apiserver.confExecStart=/usr/local/bin/kube-apiserver \\$KUBE_APISERVER_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFkube-controller-manager.service123456789101112131415cat &gt; kube-controller-manager.service &lt;&lt;EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]User=kubeEnvironmentFile=-/etc/kubernetes/kube-controller-manager.confExecStart=/usr/local/bin/kube-controller-manager \\$KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFkube-scheduler.service123456789101112131415cat &gt; kube-scheduler.service &lt;&lt;EOF[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]User=kubeEnvironmentFile=-/etc/kubernetes/kube-scheduler.confExecStart=/usr/local/bin/kube-scheduler \\$KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF分发配置文件到各master节点根据master节点的信息替换配置文件里面的字段1234567891011for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" rsync -avpt kube*service $NODE:/usr/lib/systemd/system/ sed -e \"s/&#123;PUBLIC_IP&#125;/$&#123;MasterArray[$NODE]&#125;/g\" kube-apiserver.conf.example &gt; kube-apiserver.conf.$&#123;NODE&#125; rsync -avpt kube-apiserver.conf.$&#123;NODE&#125; $NODE:/etc/kubernetes/kube-apiserver.conf rsync -avpt kube-controller-manager.conf.example $NODE:/etc/kubernetes/kube-controller-manager.conf rsync -avpt kube-scheduler.conf.example $NODE:/etc/kubernetes/kube-scheduler.conf rm -rf *conf.$&#123;NODE&#125; ssh $NODE systemctl daemon-reload ssh $NODE chown -R kube:kube /etc/kubernetesdone启动kubernetes服务可以先在k8s-m1上面启动服务，确认正常之后再在其他master节点启动123systemctl enable --now kube-apiserver.servicesystemctl enable --now kube-controller-manager.servicesystemctl enable --now kube-scheduler.service12345678910111213kubectl --kubeconfig=/etc/kubernetes/kube-admin.kubeconfig get cs# 输出示例NAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy &#123;\"health\":\"true\"&#125; etcd-0 Healthy &#123;\"health\":\"true\"&#125; etcd-1 Healthy &#123;\"health\":\"true\"&#125;kubectl --kubeconfig=/etc/kubernetes/kube-admin.kubeconfig get endpoints# 输出示例NAME ENDPOINTS AGEkubernetes 172.16.80.201:6443 27s123456for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" ssh $NODE \"systemctl enable --now kube-apiserver\" ssh $NODE \"systemctl enable --now kube-controller-manager\" ssh $NODE \"systemctl enable --now kube-scheduler\"done三台master节点的kube-apiserver、kube-controller-manager、kube-scheduler服务启动成功后可以测试一下1234kubectl --kubeconfig=/etc/kubernetes/kube-admin.kubeconfig get endpoints# 输出示例NAME ENDPOINTS AGEkubernetes 172.16.80.201:6443,172.16.80.202:6443,172.16.80.203:6443 12m设置kubectlkubectl命令默认会加载~/.kube/config文件，如果文件不存在则连接http://127.0.0.1:8080，这显然不符合预期，这里使用之前生成的kube-admin.kubeconfig在k8s-m1上操作12345for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" ssh $NODE mkdir -p /root/.kube rsync -avpt /root/pki/kube-admin.kubeconfig $NODE:/root/.kube/configdone设置命令补全设置kubectl 命令自动补全123456789for NODE in \"$&#123;!MasterArray[@]&#125;\";do echo \"--- $NODE $&#123;MasterArray[$NODE]&#125; ---\" echo \"--- kubectl命令自动补全 ---\" ssh $NODE kubectl completion bash &gt;&gt; /etc/bash_completion.d/kubectl echo \"--- kubeadm命令自动补全 ---\" ssh $NODE kubeadm completion bash &gt;&gt; /etc/bash_completion.d/kubeadmdonesource /etc/bash_completion.d/kubectl设置kubelet的bootstrap启动所需的RBAC当集群开启了 TLS 认证后，每个节点的 kubelet 组件都要使用由 apiserver 使用的 CA 签发的有效证书才能与apiserver 通讯；此时如果节点多起来，为每个节点单独签署证书将是一件非常繁琐的事情；TLS bootstrapping 功能就是让 kubelet 先使用一个预定的低权限用户连接到 apiserver，然后向 apiserver 申请证书，kubelet 的证书由 apiserver 动态签署；在其中一个master节点上执行就可以，以k8s-m1为例创建工作目录12mkdir -p /root/yaml/tls-bootstrapcd /root/yaml/tls-bootstrap/kubelet-bootstrap-rbac.yaml12345678910111213141516# 创建yaml文件cat &gt; kubelet-bootstrap-rbac.yaml &lt;&lt;EOF# 给予 kubelet-bootstrap 用户进行 node-bootstrapper 的权限apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubelet-bootstraproleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-bootstrappersubjects:- apiGroup: rbac.authorization.k8s.io kind: User name: kubelet-bootstrapEOFtls-bootstrap-clusterrole.yaml12345678910111213# 创建yaml文件cat &gt; tls-bootstrap-clusterrole.yaml &lt;&lt;EOF# A ClusterRole which instructs the CSR approver to approve a node requesting a# serving cert matching its client cert.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverrules:- apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/selfnodeserver\"] verbs: [\"create\"]EOFnode-client-auto-approve-csr.yaml12345678910111213141516# 创建yaml文件cat &gt; node-client-auto-approve-csr.yaml &lt;&lt;EOF# 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: node-client-auto-approve-csrroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclientsubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappersEOFnode-client-auto-renew-crt.yaml12345678910111213141516# 创建yaml文件cat &gt; node-client-auto-renew-crt.yaml &lt;&lt;EOF# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: node-client-auto-renew-crtroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclientsubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodesEOFnode-server-auto-renew-crt.yaml12345678910111213141516# 创建yaml文件cat &gt; node-server-auto-renew-crt.yaml &lt;&lt;EOF# 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: node-server-auto-renew-crtroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserversubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodesEOF创建tls-bootstrap-rbac1kubectl apply -f .设置kube-apiserver获取node信息的权限说明本文部署的kubelet关闭了匿名访问，因此需要额外为kube-apiserver添加权限用于访问kubelet的信息若没添加此RBAC，则kubectl在执行logs、exec等指令的时候会提示401 Forbidden12kubectl -n kube-system logs calico-node-pc8lq Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log calico-node-pc8lq)参考文档：Kublet的认证授权创建yaml文件1234567891011121314151617181920212223242526272829303132333435cat &gt; /root/yaml/apiserver-to-kubelet-rbac.yaml &lt;&lt;EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubeletrules: - apiGroups: - \"\" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - \"*\"---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:kube-apiserver namespace: \"\"roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubeletsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kube-apiserverEOF创建RBAC1kubectl apply -f /root/yaml/apiserver-to-kubelet-rbac.yamlkubernetes worker节点worker节点说明安装Docker-ce，配置与master节点一致即可安装cni-plugins、kubelet、kube-proxy关闭防火墙和SELINUXkubelet和kube-proxy运行需要root权限这里是以k8s-m1、k8s-m2、k8s-m3作为Work节点加入集群kubelet管理容器生命周期、节点状态监控目前 kubelet 支持三种数据源来获取节点Pod信息：本地文件通过 url 从网络上某个地址来获取信息API Server：从 kubernetes master 节点获取信息使用kubeconfig与kube-apiserver通信这里启用TLS-Bootstrap实现kubelet证书动态签署证书，并自动生成kubeconfigkube-proxyKube-proxy是实现Service的关键插件，kube-proxy会在每台节点上执行，然后监听API Server的Service与Endpoint资源物件的改变，然后来依据变化调用相应的组件来实现网路的转发kube-proxy可以使用userspace（基本已废弃）、iptables（默认方式）和ipvs来实现数据报文的转发这里使用的是性能更好、适合大规模使用的ipvs使用kubeconfig与kube-apiserver通信切换工作目录在k8s-m1上操作1cd /root/workerworker组件配置模板kubelet.conf--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig指定bootstrap启动时使用的kubeconfig--network-plugin=cni定义网络插件，Pod生命周期使用此网络插件--node-labels=node-role.kubernetes.io/node=&#39;&#39;kubelet注册当前Node时设置的Label，以key=value的格式表示，多个labe以逗号分隔--pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.1Pod的pause镜像123456789101112131415cat &gt; kubelet.conf &lt;&lt;EOFKUBELET_ARGS=\" \\\\--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\\\--cert-dir=/etc/kubernetes/ssl \\\\--config=/etc/kubernetes/kubelet.config.file \\\\--cni-conf-dir=/etc/cni/net.d \\\\--cni-bin-dir=/opt/cni/bin \\\\--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\\\--logtostderr=true \\\\--network-plugin=cni \\\\--node-labels=node-role.kubernetes.io/node='' \\\\--pod-infra-container-image=gcrxio/pause:3.1 \\\\--v=2 \\\\\"EOFkubelet.config.file1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798cat &gt; kubelet.config.file &lt;&lt;EOFapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0authentication: # 匿名访问 anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: # 这里写kubernetes-ca证书的路径 clientCAFile: /etc/kubernetes/pki/kube-ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s# cgroups的驱动，可选systemd和cgroupfscgroupDriver: cgroupfscgroupsPerQOS: true# 指定Pod的DNS服务器IP地址clusterDNS:- 10.96.0.10# 集群的域名clusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5# 达到某些阈值之后，kubelet会驱逐Pod# A set of eviction thresholds (e.g. memory.available&lt;1Gi) that if met would trigger a pod eviction.# (default imagefs.available&lt;15%,memory.available&lt;100Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5%)evictionHard: imagefs.available: 15% memory.available: 1000Mi nodefs.available: 10% nodefs.inodesFree: 10%evictionPressureTransitionPeriod: 5m0s# 检测到系统已启用swap分区时kubelet会启动失败failSwapOn: false# 定义feature gatesfeatureGates: # kubelet 在证书即将到期时会自动发起一个 renew 自己证书的 CSR 请求 # 其实rotate证书已经默认开启，这里显示定义是为了方便查看 RotateKubeletClientCertificate: true RotateKubeletServerCertificate: true# 检查kubelet配置文件变更的间隔fileCheckFrequency: 20s# 允许endpoint在尝试访问自己的服务时会被负载均衡分发到自身# 可选值\"promiscuous-bridge\", \"hairpin-veth\" and \"none\"# 默认值为promiscuous-bridgehairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20s# 这里定义容器镜像触发回收空间的上限值和下限值imageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: true# kubelet进程最大能打开的文件数量，默认是1000000maxOpenFiles: 1000000# 当前节点kubelet所能运行的最大Pod数量maxPods: 110# node状态上报间隔nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1# kubelet服务端口port: 10250registryBurst: 10registryPullQPS: 5# 指定域名解析文件resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0s# 拉镜像时，同一时间只拉取一个镜像# We recommend *not* changing the default value on nodes that run docker daemon with version &lt; 1.9 or an Aufs storage backend. Issue #10959 has more details. (default true)serializeImagePulls: truestaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0sEOFkube-proxy.conf123456cat &gt; kube-proxy.conf &lt;&lt;EOFKUBE_PROXY_ARGS=\" \\\\--config=/etc/kubernetes/kube-proxy.config.file \\\\--v=2 \\\\\"EOFkube-proxy.config.file12345678910111213141516171819202122232425262728293031323334353637383940414243cat &gt; kube-proxy.config.file &lt;&lt;EOFapiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationbindAddress: 0.0.0.0clientConnection: acceptContentTypes: \"\" burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig qps: 5# 集群中pod的CIDR范围，从这个范围以外发送到服务集群IP的流量将被伪装，从POD发送到外部LoadBalanceIP的流量将被定向到各自的集群IPclusterCIDR: \"10.244.0.0/16\"configSyncPeriod: 15m0sconntrack: max: null # 每个核心最大能跟踪的NAT连接数，默认32768 maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0senableProfiling: falsehealthzBindAddress: 0.0.0.0:10256hostnameOverride: \"\"iptables: # SNAT所有通过服务集群ip发送的通信 masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30sipvs: excludeCIDRs: null minSyncPeriod: 0s # ipvs调度类型，默认是rr scheduler: \"rr\" syncPeriod: 30smetricsBindAddress: 127.0.0.1:10249mode: \"ipvs\"nodePortAddresses: nulloomScoreAdj: -999portRange: \"\"resourceContainer: /kube-proxyudpIdleTimeout: 250msEOFsystemd服务文件kubelet.service123456789101112131415161718cat &gt; kubelet.service &lt;&lt;EOF[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/kubelet.confExecStart=/usr/local/bin/kubelet \\$KUBELET_ARGSRestart=on-failureKillMode=processLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFkube-proxy.service1234567891011121314151617cat &gt; kube-proxy.service &lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/kube-proxy.conf# 这里启动时使用ipvsadm将TCP的keepalive时间设置，默认是900ExecStartPre=/usr/sbin/ipvsadm --set 900 120 300 ExecStart=/usr/local/bin/kube-proxy \\$KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF分发证书和kubeconfig文件在k8s-m1上操作在worker节点建立对应的目录123456789101112131415for NODE in \"$&#123;!WorkerArray[@]&#125;\";do echo \"--- $NODE ---\" echo \"--- 创建目录 ---\" ssh $NODE mkdir -p /opt/cni/bin \\ /etc/cni/net.d \\ /etc/kubernetes/pki \\ /etc/kubernetes/manifests \\ /var/lib/kubelet rsync -avpt /root/pki/kube-proxy.kubeconfig \\ /root/pki/bootstrap.kubeconfig \\ $NODE:/etc/kubernetes/ rsync -avpt /root/pki/kube-ca.pem \\ /root/pki/front-proxy-ca.pem \\ $NODE:/etc/kubernetes/pki/done分发二进制文件在k8s-m1上操作123456789for NODE in \"$&#123;!WorkerArray[@]&#125;\";do echo \"--- $NODE ---\" echo \"--- 分发kubernetes二进制文件 ---\" rsync -avpt /root/software/kubernetes/server/bin/kubelet \\ /root/software/kubernetes/server/bin/kube-proxy \\ $NODE:/usr/local/bin/ echo \"--- 分发CNI-Plugins ---\" rsync -avpt /root/software/cni-plugins/* $NODE:/opt/cni/bin/done分发配置文件和服务文件123456for NODE in \"$&#123;!WorkerArray[@]&#125;\";do echo \"--- $NODE ---\" rsync -avpt kubelet.conf kubelet.config.file kube-proxy.conf kube-proxy.config.file $NODE:/etc/kubernetes/ rsync -avpt kubelet.service kube-proxy.service $NODE:/usr/lib/systemd/system/ ssh $NODE systemctl daemon-reloaddone启动服务1234for NODE in \"$&#123;!WorkerArray[@]&#125;\";do echo \"--- $NODE ---\" ssh $NODE systemctl enable --now docker.service kubelet.service kube-proxy.servicedone获取节点信息此时由于未按照网络插件，所以节点状态为NotReady123456kubectl get node -o wide# 示例输出NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-m1 NotReady node 12s v1.11.5 172.16.80.201 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.1.3.el7.x86_64 docker://18.3.1k8s-m2 NotReady node 12s v1.11.5 172.16.80.202 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.1.3.el7.x86_64 docker://18.3.1k8s-m3 NotReady node 12s v1.11.5 172.16.80.203 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.1.3.el7.x86_64 docker://18.3.1kubernetes Core Addons网络组件部署（二选其一）只要符合CNI规范的网络组件都可以给kubernetes使用网络组件清单可以在这里看到Network Plugins这里只列举kube-flannel和calico，flannel和calico的区别可以自己去找资料网络组件只能选一个来部署本文使用kube-flannel部署网络组件，calico已测试可用在k8s-m1上操作创建工作目录1mkdir -p /root/yaml/network-plugin/&#123;kube-flannel,calico&#125;kube-flannel说明kube-flannel基于VXLAN的方式创建容器二层网络，使用端口8472/UDP通信flannel 第一次启动时，从 etcd 获取 Pod 网段信息，为本节点分配一个未使用的 /24 段地址，然后创建 flannel.1（也可能是其它名称，如 flannel1 等） 接口。官方提供yaml文件部署为DeamonSet若需要使用NetworkPolicy功能，可以关注这个项目canal架构图切换工作目录1cd /root/yaml/network-plugin/kube-flannel下载yaml文件1wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml官方yaml文件包含多个平台的daemonset，包括amd64、arm64、arm、ppc64le、s390x这里以amd64作为例子，其他的可以自行根据需要修改或者直接删除不需要的daemonset官方yaml文件已经配置好容器网络为10.244.0.0/16，这里需要跟kube-controller-manager.conf里面的--cluster-cidr匹配如果在kube-controller-manager.conf里面把--cluster-cidr改成了其他地址段，例如192.168.0.0/16，用以下命令替换kube-flannel.yaml相应的字段1sed -e 's,\"Network\": \"10.244.0.0/16\",\"Network\": \"192.168.0.0/16,\" -i kube-flannel.yml如果服务器有多个网卡，需要指定网卡用于flannel通信，以网卡ens33为例在args下面添加一行- --iface=ens33123456789containers:- name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=ens33修改backendflannel支持多种后端实现，可选值为VXLAN、host-gw、UDP从性能上，host-gw是最好的，VXLAN和UDP次之默认值是VXLAN，这里以修改为host-gw为例，位置大概在75行左右1234567net-conf.json: | &#123; \"Network\": \"10.244.0.0/16\", \"Backend\": &#123; \"Type\": \"host-gw\" &#125; &#125;部署kube-flannel1kubectl apply -f kube-flannel.yml检查部署情况12345kubectl -n kube-system get pod -l k8s-app=flannelNAME READY STATUS RESTARTS AGEkube-flannel-ds-27jwl 2/2 Running 0 59skube-flannel-ds-4fgv6 2/2 Running 0 59skube-flannel-ds-mvrt7 2/2 Running 0 59s如果等很久都没Running，可能是quay.io对你来说太慢了可以替换一下镜像，重新apply12sed -e 's,quay.io/coreos/,zhangguanzhang/quay.io.coreos.,g' -i kube-flannel.ymlkubectl apply -f kube-flannel.yamlCalico说明Calico 是一款纯 Layer 3 的网络，节点之间基于BGP协议来通信。这里以calico-v3.4.0来作为示例部署文档架构图切换工作目录1cd /root/yaml/network-plugin/calico下载yaml文件这里使用kubernetes API来保存网络信息12wget https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yamlwget https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calicoctl.yaml官方yaml文件默认配置容器网络为192.168.0.0/16，这里需要跟kube-controller-manager.conf里面的--cluster-cidr匹配，需要替换相应字段1sed -e \"s,192.168.0.0/16,$&#123;POD_NET_CIDR&#125;,g\" -i calico.yaml官方yaml文件定义calicoctl为Pod，而不是deployment，所以需要调整一下修改kind: Pod为kind: Deployment并补充其他字段1234567891011121314151617181920212223242526272829303132333435apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: calicoctl namespace: kube-system labels: k8s-app: calicoctlspec: replicas: 1 selector: matchLabels: k8s-app: calicoctl template: metadata: name: calicoctl namespace: kube-system labels: k8s-app: calicoctl spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node.cloudprovider.kubernetes.io/uninitialized value: \"true\" hostNetwork: true serviceAccountName: calicoctl containers: - name: calicoctl image: quay.io/calico/ctl:v3.4.0 command: [\"/bin/sh\", \"-c\", \"while true; do sleep 3600; done\"] tty: true env: - name: DATASTORE_TYPE value: kubernetes部署Calico1kubectl apply -f /root/yaml/network-plugin/calico/检查部署情况12345678910111213141516171819202122kubectl -n kube-system get pod -l k8s-app=calico-nodeNAME READY STATUS RESTARTS AGEcalico-node-fjcj4 2/2 Running 0 6mcalico-node-tzppt 2/2 Running 0 6mcalico-node-zdq64 2/2 Running 0 6mkubectl get pod -n kube-system -l k8s-app=calicoctlNAME READY STATUS RESTARTS AGEcalicoctl-58df8955f6-sp8q9 0/1 Running 0 38skubectl -n kube-system exec -it calicoctl-58df8955f6-sp8q9 -- /calicoctl get node -o wideNAME ASN IPV4 IPV6 k8s-m1 (unknown) 172.16.80.201/24 k8s-m2 (unknown) 172.16.80.202/24 k8s-m3 (unknown) 172.16.80.203/24kubectl -n kube-system exec -it calicoctl-58df8955f6-sp8q9 -- /calicoctl get profiles -o wideNAME LABELS kns.default map[] kns.kube-public map[] kns.kube-system map[]如果镜像pull不下来，可以替换一下替换完重新apply12sed -e 's,quay.io/calico/,zhangguanzhang/quay.io.calico.,g' -i *yamlkubectl apply -f .检查节点状态网络组件部署完成之后，可以看到node状态已经为Ready12345kubectl get node NAME STATUS ROLES AGE VERSIONk8s-m1 Ready node 1d v1.11.5k8s-m2 Ready node 1d v1.11.5k8s-m3 Ready node 1d v1.11.5服务发现组件部署kubernetes从v1.11之后，已经使用CoreDNS取代原来的KUBE DNS作为服务发现的组件CoreDNS 是由 CNCF 维护的开源 DNS 方案，前身是 SkyDNS在k8s-m1上操作创建工作目录1mkdir -p /root/yaml/coredns切换工作目录1cd /root/yaml/corednsCoreDNS创建yaml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193cat &gt; coredns.yaml &lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: coredns namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsrules:- apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:corednssubjects:- kind: ServiceAccount name: coredns namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors log health kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload loadbalance &#125;---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: \"CoreDNS\"spec: replicas: 2 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \"\" labels: k8s-app: kube-dns spec: serviceAccountName: coredns priorityClassName: system-cluster-critical # 使用podAntiAffinity # CoreDNS的Pod不会被调度到同一台宿主机 affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: - kube-dns topologyKey: kubernetes.io/hostname tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/master containers: - name: coredns image: gcrxio/coredns:1.2.6 imagePullPolicy: IfNotPresent args: [ \"-conf\", \"/etc/coredns/Corefile\" ] livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 10 successThreshold: 1 failureThreshold: 5 ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP resources: limits: memory: 200Mi requests: cpu: 100m memory: 70Mi securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true - name: host-time mountPath: /etc/localtime dnsPolicy: Default volumes: - name: host-time hostPath: path: /etc/localtime - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile---apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\"spec: selector: k8s-app: kube-dns clusterIP: $&#123;POD_DNS_SERVER_IP&#125; ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCPEOF修改yaml文件yaml文件里面定义了clusterIP这里需要与kubelet.config.file里面定义的cluster-dns一致如果kubelet.conf里面的--cluster-dns改成别的，例如x.x.x.x，这里也要做相应变动，不然Pod找不到DNS，无法正常工作这里定义静态的hosts解析，这样Pod可以通过hostname来访问到各节点主机用下面的命令根据HostArray的信息生成静态的hosts解析12345678sed -e '57r '&lt;(\\ echo ' hosts &#123;'; \\ for NODE in \"$&#123;!HostArray[@]&#125;\";do \\ echo \" $&#123;HostArray[$NODE]&#125; $NODE\"; \\ done;\\ echo ' fallthrough'; \\ echo ' &#125;';) \\-i coredns.yaml上面的命令的作用是，通过HostArray的信息生成hosts解析配置，顺序是打乱的，可以手工调整顺序也可以手动修改coredns.yaml文件来添加对应字段12345678910111213141516171819202122232425262728apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors log health kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; hosts &#123; 172.16.80.202 k8s-m2 172.16.80.203 k8s-m3 172.16.80.201 k8s-m1 fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload loadbalance &#125;部署CoreDNS1kubectl apply -f coredns.yaml检查部署状态1234kubectl -n kube-system get pod -l k8s-app=kube-dnsNAME READY STATUS RESTARTS AGEcoredns-5566c96697-6gzzc 1/1 Running 0 45scoredns-5566c96697-q5slk 1/1 Running 0 45s验证集群DNS服务创建一个deployment测试DNS解析123456789101112131415161718192021222324252627282930# 创建一个基于busybox的deploymentcat &gt; /root/yaml/busybox-deployment.yaml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: labels: app: busybox name: busybox namespace: defaultspec: replicas: 1 selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: busybox imagePullPolicy: IfNotPresent image: busybox:1.26 command: - sleep - \"3600\"EOF# 基于文件创建deploymentkubectl apply -f /root/yaml/busybox-deployment.yaml检查deployment部署情况123kubectl get podNAME READY STATUS RESTARTS AGEbusybox-7b9bfb5658-872gj 1/1 Running 0 6s验证集群DNS解析上一个命令获取到pod名字为busybox-7b9bfb5658-872gj通过kubectl命令连接到Pod运行nslookup命令测试使用域名来访问kube-apiserver和各节点主机123456789101112131415161718192021222324252627282930313233echo \"--- 通过CoreDNS访问kubernetes ---\"kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup kubernetes# 示例输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.localecho \"--- 通过CoreDNS访问k8s-m1 ---\"# 示例输出kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup k8s-m1Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: k8s-m1Address 1: 172.16.80.201 k8s-m1echo \"--- 通过CoreDNS访问k8s-m2 ---\"kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup k8s-m2# 示例输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: k8s-n2Address 1: 172.16.80.202 k8s-m2echo \"--- 通过CoreDNS访问并不存在的k8s-n3 ---\"kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup k8s-n3# 示例输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localnslookup: can't resolve 'k8s-n3'Metrics ServerMetrics Server是实现了 Metrics API 的元件,其目标是取代 Heapster 作位 Pod 与 Node 提供资源的 Usagemetrics,该元件会从每个 Kubernetes 节点上的 Kubelet 所公开的 Summary API 中收集 MetricsHorizontal Pod Autoscaler（HPA）控制器用于实现基于CPU使用率进行自动Pod伸缩的功能。HPA控制器基于Master的kube-controller-manager服务启动参数–horizontal-pod-autoscaler-sync-period定义是时长（默认30秒）,周期性监控目标Pod的CPU使用率,并在满足条件时对ReplicationController或Deployment中的Pod副本数进行调整,以符合用户定义的平均PodCPU使用率。在新版本的kubernetes中 Pod CPU使用率不在来源于heapster,而是来自于metrics-server官网原话是 The –horizontal-pod-autoscaler-use-rest-clients is true or unset. Setting this to false switches to Heapster-based autoscaling, which is deprecated.在k8s-m1上操作额外参数设置kube-apiserver参数，这里在配置kube-apiserver阶段已经加进去了front-proxy证书，在证书生成阶段已经完成且已分发1234567--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem--requestheader-allowed-names=aggregator--requestheader-group-headers=X-Remote-Group--requestheader-extra-headers-prefix=X-Remote-Extra---requestheader-username-headers=X-Remote-User创建工作目录1mkdir -p /root/yaml/metrics-server切换工作目录1cd /root/yaml/metrics-server下载yaml文件123456wget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/aggregated-metrics-reader.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/auth-delegator.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/auth-reader.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/metrics-apiservice.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/metrics-server-service.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/resource-reader.yaml创建metrics-server-deployment.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950cat &gt; metrics-server-deployment.yaml &lt;&lt;EOF---apiVersion: v1kind: ServiceAccountmetadata: name: metrics-server namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-serverspec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: ca-ssl hostPath: path: /etc/kubernetes/pki containers: - name: metrics-server image: gcrxio/metrics-server-amd64:v0.3.1 imagePullPolicy: IfNotPresent command: - /metrics-server - --metric-resolution=30s - --kubelet-port=10250 - --kubelet-preferred-address-types=InternalDNS,InternalIP,ExternalDNS,ExternalIP,Hostname - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem - --requestheader-username-headers=X-Remote-User - --requestheader-group-headers=X-Remote-Group - --requestheader-extra-headers-prefix=X-Remote-Extra- - --kubelet-insecure-tls - -v=2 volumeMounts: - name: ca-ssl mountPath: /etc/kubernetes/pkiEOF部署metrics-server1kubectl apply -f .查看pod状态123kubectl -n kube-system get pod -l k8s-app=metrics-serverNAME READY STATUS RESTARTS AGEpod/metrics-server-86bd9d7667-5hbn6 1/1 Running 0 1m验证metrics完成后,等待一段时间(约 30s - 1m)收集 Metrics12345678910111213# 请求metrics api的结果kubectl get --raw /apis/metrics.k8s.io/v1beta1&#123;\"kind\":\"APIResourceList\",\"apiVersion\":\"v1\",\"groupVersion\":\"metrics.k8s.io/v1beta1\",\"resources\":[&#123;\"name\":\"nodes\",\"singularName\":\"\",\"namespaced\":false,\"kind\":\"NodeMetrics\",\"verbs\":[\"get\",\"list\"]&#125;,&#123;\"name\":\"pods\",\"singularName\":\"\",\"namespaced\":true,\"kind\":\"PodMetrics\",\"verbs\":[\"get\",\"list\"]&#125;]&#125;kubectl get apiservice|grep metricsv1beta1.metrics.k8s.io 2018-12-09T08:17:26Z# 获取节点性能信息kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-m1 113m 2% 1080Mi 14% k8s-m2 133m 3% 1086Mi 14% k8s-m3 100m 2% 1029Mi 13%至此集群已具备基本功能下面的Extra Addons就是一些额外的功能kubernetes Extra AddonsDashboardDashboard 是kubernetes社区提供的GUI界面，用于图形化管理kubernetes集群，同时可以看到资源报表。官方提供yaml文件直接部署，但是需要更改image以便国内部署在k8s-m1上操作创建工作目录1mkdir -p /root/yaml/kubernetes-dashboard切换工作目录1cd /root/yaml/kubernetes-dashboard获取yaml文件1wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml修改镜像地址1sed -e 's,k8s.gcr.io/kubernetes-dashboard-amd64,gcrxio/kubernetes-dashboard-amd64,g' -i kubernetes-dashboard.yaml创建kubernetes-Dashboard1kubectl apply -f kubernetes-dashboard.yaml创建ServiceAccount RBAC官方的yaml文件，ServiceAccount绑定的RBAC权限很低，很多资源无法查看需要创建一个用于管理全局的ServiceAccount123456789101112131415161718192021222324252627cat &gt; cluster-admin.yaml &lt;&lt;EOF---# 在kube-system中创建名为admin-user的ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---# 将admin-user和cluster-admin绑定在一起# cluster-admin是kubernetes内置的clusterrole，具有集群管理员权限# 其他内置的clusterrole可以通过kubectl get clusterrole查看apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-systemEOFkubectl apply -f cluster-admin.yaml获取ServiceAccount的Token1kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '&#123;print $1&#125;')查看部署情况1kubectl get all -n kube-system --selector k8s-app=kubernetes-dashboard访问Dashboardkubernetes-dashborad的svc默认是clusterIP，需要修改为nodePort才能被外部访问随机分配NodePort，分配范围由kube-apiserver的--service-node-port-range参数指定1kubectl patch -n kube-system svc kubernetes-dashboard -p '&#123;\"spec\":&#123;\"type\":\"NodePort\"&#125;&#125;'修改完之后，通过以下命令获取访问kubernetes-Dashboard的端口123kubectl -n kube-system get svc --selector k8s-app=kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.106.183.192 &lt;none&gt; 443:30216/TCP 12s可以看到已经将节点的30216端口暴露出来IP地址不固定，只要运行了kube-proxy组件，都会在节点上添加30216端口规则用于转发请求到Podhttps://172.16.80.200:30216https://172.16.80.201:30216https://172.16.80.202:30216https://172.16.80.203:30216登录Dashboard，上面已经获取了token，这里只需要把token的值填入输入框，点击SIGN IN即可登录Dashboard UI预览图Ingress ControllerIngress 是 Kubernetes 中的一个抽象资源，其功能是通过 Web Server 的 Virtual Host概念以域名(Domain Name)方式转发到內部 Service，这避免了使用 Service 中的 NodePort 与LoadBalancer 类型所带來的限制(如 Port 数量上限)，而实现 Ingress 功能则是通过 Ingress Controller来达成，它会负责监听 Kubernetes API 中的 Ingress 与 Service 资源，并在发生资源变化时，根据资源预期的结果来设置 Web Server。Ingress Controller 有许多实现可以选择，这里只是列举一小部分Ingress NGINX：Kubernetes 官方维护的方案，本次安装使用此方案kubernetes-ingress：由nginx社区维护的方案，使用社区版nginx和nginx-plustreafik：一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置在k8s-m1上操作创建工作目录1mkdir -p /root/yaml/ingress/ingress-nginx切换工作目录1cd /root/yaml/ingress/ingress-nginx下载yaml文件12wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.20.0/deploy/mandatory.yamlwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.20.0/deploy/provider/baremetal/service-nodeport.yaml修改镜像地址123sed -e 's,k8s.gcr.io/,zhangguanzhang/gcr.io.google_containers.,g' \\ -e 's,quay.io/kubernetes-ingress-controller/,zhangguanzhang/quay.io.kubernetes-ingress-controller.,g' \\ -i mandatory.yaml创建ingress-nginx1kubectl apply -f .检查部署情况1kubectl -n ingress-nginx get pod访问ingress默认的backend会返回404123456789kubectl -n ingress-nginx get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.96.250.140 &lt;none&gt; 80:32603/TCP,443:30083/TCP 1mcurl http://172.16.80.200:32603default backend - 404curl -k https://172.16.80.200:30083default backend - 404注意这里部署之后，是deployment，且通过nodePort暴露服务也可以修改yaml文件，将Ingress-nginx部署为DaemonSet使用labels和nodeSelector来指定运行ingress-nginx的节点使用hostNetwork=true来共享主机网络命名空间，或者使用hostPort指定主机端口映射如果使用hostNetwork共享宿主机网络栈或者hostPort映射宿主机端口，记得要看看有没有端口冲突，否则无法启动修改监听端口可以在ingress-nginx启动命令中添加--http-port=8180和--https-port=8543，还有下面的端口定义也相应变更即可创建kubernetes-Dashboard的Ingresskubernetes-Dashboard默认是开启了HTTPS访问的ingress-nginx需要以HTTPS的方式反向代理kubernetes-Dashboard以HTTP方式访问kubernetes-Dashboard的时候会被重定向到HTTPS需要创建HTTPS证书，用于访问ingress-nginx的HTTPS端口创建HTTPS证书这里的CN=域名/O=域名需要跟后面的ingress主机名匹配1234567openssl req -x509 \\ -nodes \\ -days 3650 \\ -newkey rsa:2048 \\ -keyout tls.key \\ -out tls.crt \\ -subj \"/CN=dashboard.k8s.local/O=dashboard.k8s.local\"创建secret对象这里将HTTPS证书创建为kubernetes的secret对象dashboard-tlsingress创建的时候需要加载这个作为HTTPS证书1kubectl -n kube-system create secret tls dashboard-tls --key ./tls.key --cert ./tls.crt创建dashboard-ingress.yaml123456789101112131415161718192021apiVersion: extensions/v1beta1kind: Ingressmetadata: name: dashboard-ingress namespace: kube-system annotations: nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" nginx.ingress.kubernetes.io/secure-backends: \"true\"spec: tls: - hosts: - dashboard.k8s.local secretName: dashboard-tls rules: - host: dashboard.k8s.local http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443创建ingress1kubectl apply -f dashboard-ingress.yaml检查ingress123kubectl -n kube-system get ingressNAME HOSTS ADDRESS PORTS AGEdashboard-ingress dashboard.k8s.local 80, 443 16m访问kubernetes-Dashboard修改主机hosts静态域名解析，以本文为例在hosts文件里添加172.16.80.200 dashboard.k8s.local使用https://dashboard.k8s.local:30083访问kubernetesDashboard了添加了TLS之后，访问HTTP会被跳转到HTTPS端口，这里比较坑爹，没法自定义跳转HTTPS的端口此处使用的是自签名证书，浏览器会提示不安全，请忽略建议搭配external-DNS和LoadBalancer一起食用，效果更佳HelmHelm是一个kubernetes应用的包管理工具，用来管理charts——预先配置好的安装包资源，有点类似于Ubuntu的APT和CentOS中的yum。Helm chart是用来封装kubernetes原生应用程序的yaml文件，可以在你部署应用的时候自定义应用程序的一些metadata，便与应用程序的分发。Helm和charts的主要作用：应用程序封装版本管理依赖检查便于应用程序分发环境要求kubernetes v1.6及以上的版本，启用RBAC集群可以访问到chart仓库helm客户端主机能访问kubernetes集群安装客户端安装方式二选一，需要科学上网直接脚本安装12echo '--- 使用脚本安装，默认是最新版 ---'curl https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash下载二进制文件安装12345echo '--- 下载二进制文件安装 ---'wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.0-linux-amd64.tar.gztar xzf helm-v2.12.0-linux-amd64.tar.gz linux-amd64/helmmv linux-amd64/helm /usr/local/bin/rm -rf linux-amd64创建工作目录1mkdir /root/yaml/helm/切换工作目录1cd /root/yaml/helm创建RBAC规则123456789101112131415161718192021222324cat &gt; /root/yaml/helm/helm-rbac.yaml &lt;&lt;EOF# 创建名为tiller的ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---# 给tiller绑定cluster-admin权限apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tiller-cluster-ruleroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: tiller namespace: kube-systemEOFkubectl apply -f /root/yaml/helm/helm-rbac.yaml安装服务端这里指定了helm的stable repo国内镜像地址具体说明请看这里123helm init --tiller-image gcrxio/tiller:v2.12.0 \\ --service-account tiller \\ --stable-repo-url http://mirror.azure.cn/kubernetes/charts/检查安装情况123456789kubectl -n kube-system get pod -l app=helm,name=tiller# 输出示例NAME READY STATUS RESTARTS AGEtiller-deploy-84fc6cd5f9-nz4m7 1/1 Running 0 1mhelm version# 输出示例Client: &amp;version.Version&#123;SemVer:\"v2.12.0\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;Server: &amp;version.Version&#123;SemVer:\"v2.12.0\", GitCommit:\"d325d2a9c179b33af1a024cdb5a4472b6288016a\", GitTreeState:\"clean\"&#125;添加命令行补全12helm completion bash &gt; /etc/bash_completion.d/helmsource /etc/bash_completion.d/helmRook（测试用途）说明Rook是一款云原生环境下的开源分布式存储编排系统，目前已进入CNCF孵化。Rook的官方网站是https://rook.ioRook将分布式存储软件转变为自我管理，自我缩放和自我修复的存储服务。它通过自动化部署，引导、配置、供应、扩展、升级、迁移、灾难恢复、监控和资源管理来实现。 Rook使用基础的云原生容器管理、调度和编排平台提供的功能来履行其职责。Rook利用扩展点深入融入云原生环境，为调度、生命周期管理、资源管理、安全性、监控和用户体验提供无缝体验。Ceph Custom Resource Definition（CRD）已经在Rook v0.8版本升级到Beta其他特性请查看项目文档这里只用作测试环境中提供StorageClass和持久化存储请慎重考虑是否部署在生产环境中Rook与kubernetes的集成Rook架构图安装这里以Rook v0.8.3作为示例这里默认使用/var/lib/rook/osd*目录来运行OSD需要最少3个节点，否则无足够的节点启动集群可以使用yaml文件部署和使用helm chart部署，这里使用yaml文件部署创建工作目录1mkdir -p /root/yaml/rook/进入工作目录1cd /root/yaml/rook/下载yaml文件1234# operator实现自定义API用于管理rook-cephwget https://raw.githubusercontent.com/rook/rook/v0.8.3/cluster/examples/kubernetes/ceph/operator.yaml# cluster用于部署rook-ceph集群wget https://raw.githubusercontent.com/rook/rook/v0.8.3/cluster/examples/kubernetes/ceph/cluster.yaml部署operator1kubectl apply -f operator.yaml检查operator安装情况1234567891011121314151617181920kubectl -n rook-ceph-system get all# 输出示例NAME READY STATUS RESTARTS AGEpod/rook-ceph-agent-4qwvd 1/1 Running 0 11mpod/rook-ceph-agent-v5ghj 1/1 Running 0 11mpod/rook-ceph-agent-zv8s6 1/1 Running 0 11mpod/rook-ceph-operator-745f756bd8-9gdpk 1/1 Running 0 12mpod/rook-discover-44lx5 1/1 Running 0 11mpod/rook-discover-4d6mn 1/1 Running 0 11mpod/rook-discover-mvqfv 1/1 Running 0 11mNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdaemonset.apps/rook-ceph-agent 3 3 3 3 3 &lt;none&gt; 11mdaemonset.apps/rook-discover 3 3 3 3 3 &lt;none&gt; 11mNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeployment.apps/rook-ceph-operator 1 1 1 1 12mNAME DESIRED CURRENT READY AGEreplicaset.apps/rook-ceph-operator-745f756bd8 1 1 1 12m部署cluster1kubectl apply -f cluster.yaml检查cluster部署情况1234567891011121314151617181920212223242526272829303132333435363738394041kubectl -n rook-ceph get all# 输出示例NAME READY STATUS RESTARTS AGEpod/rook-ceph-mgr-a-7944d8d79b-pvrsf 1/1 Running 0 10mpod/rook-ceph-mon0-ll7fc 1/1 Running 0 11mpod/rook-ceph-mon1-cd2gb 1/1 Running 0 11mpod/rook-ceph-mon2-vlmfc 1/1 Running 0 10mpod/rook-ceph-osd-id-0-745486df7b-4dxdc 1/1 Running 0 10mpod/rook-ceph-osd-id-1-85fdf4cd64-ftmc4 1/1 Running 0 10mpod/rook-ceph-osd-id-2-6bc4fbb457-295pn 1/1 Running 0 10mpod/rook-ceph-osd-prepare-k8s-m1-klv5j 0/1 Completed 0 10mpod/rook-ceph-osd-prepare-k8s-m2-dt2pl 0/1 Completed 0 10mpod/rook-ceph-osd-prepare-k8s-m3-ndqpl 0/1 Completed 0 10mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/rook-ceph-mgr ClusterIP 10.100.158.219 &lt;none&gt; 9283/TCP 10mservice/rook-ceph-mgr-dashboard ClusterIP 10.107.141.138 &lt;none&gt; 7000/TCP 10mservice/rook-ceph-mgr-dashboard-external NodePort 10.99.89.12 &lt;none&gt; 7000:30660/TCP 10mservice/rook-ceph-mon0 ClusterIP 10.100.50.229 &lt;none&gt; 6790/TCP 11mservice/rook-ceph-mon1 ClusterIP 10.110.105.207 &lt;none&gt; 6790/TCP 11mservice/rook-ceph-mon2 ClusterIP 10.103.223.166 &lt;none&gt; 6790/TCP 10mNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeployment.apps/rook-ceph-mgr-a 1 1 1 1 10mdeployment.apps/rook-ceph-osd-id-0 1 1 1 1 10mdeployment.apps/rook-ceph-osd-id-1 1 1 1 1 10mdeployment.apps/rook-ceph-osd-id-2 1 1 1 1 10mNAME DESIRED CURRENT READY AGEreplicaset.apps/rook-ceph-mgr-a-7944d8d79b 1 1 1 10mreplicaset.apps/rook-ceph-mon0 1 1 1 11mreplicaset.apps/rook-ceph-mon1 1 1 1 11mreplicaset.apps/rook-ceph-mon2 1 1 1 10mreplicaset.apps/rook-ceph-osd-id-0-745486df7b 1 1 1 10mreplicaset.apps/rook-ceph-osd-id-1-85fdf4cd64 1 1 1 10mreplicaset.apps/rook-ceph-osd-id-2-6bc4fbb457 1 1 1 10mNAME DESIRED SUCCESSFUL AGEjob.batch/rook-ceph-osd-prepare-k8s-m1 1 1 10mjob.batch/rook-ceph-osd-prepare-k8s-m2 1 1 10mjob.batch/rook-ceph-osd-prepare-k8s-m3 1 1 10m检查ceph集群状态上面命令已经获取ceph-mon0节点的pod名rook-ceph-mon0-ll7fc，以此pod为例运行以下命令12345678910111213141516kubectl -n rook-ceph exec -it rook-ceph-mon0-ll7fc -- ceph -s# 输出示例 cluster: id: 1fcee02c-fd98-4b13-bfed-de7b6605a237 health: HEALTH_OK services: mon: 3 daemons, quorum rook-ceph-mon0,rook-ceph-mon2,rook-ceph-mon1 mgr: a(active) osd: 3 osds: 3 up, 3 in data: pools: 1 pools, 100 pgs objects: 0 objects, 0 bytes usage: 22767 MB used, 96979 MB / 116 GB avail pgs: 100 active+clean暴露ceph-mgr的dashboard12wget https://raw.githubusercontent.com/rook/rook/v0.8.3/cluster/examples/kubernetes/ceph/dashboard-external.yamlkubectl apply -f dashboard-external.yaml访问已暴露的dashboard123456789kubectl -n rook-ceph get svc# 输出示例NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGErook-ceph-mgr ClusterIP 10.100.158.219 &lt;none&gt; 9283/TCP 12mrook-ceph-mgr-dashboard ClusterIP 10.107.141.138 &lt;none&gt; 7000/TCP 12mrook-ceph-mgr-dashboard-external NodePort 10.99.89.12 &lt;none&gt; 7000:30660/TCP 11mrook-ceph-mon0 ClusterIP 10.100.50.229 &lt;none&gt; 6790/TCP 13mrook-ceph-mon1 ClusterIP 10.110.105.207 &lt;none&gt; 6790/TCP 13mrook-ceph-mon2 ClusterIP 10.103.223.166 &lt;none&gt; 6790/TCP 12m可以见到这里暴露30660端口，通过此端口可以访问Dashboard添加StorageClass添加多副本存储池注释部分是创建纠删码存储池添加StorageClass指定使用多副本存储池，格式化为xfs12345678910111213141516171819202122232425262728293031cat &gt; rbd-storageclass.yaml &lt;&lt;EOFapiVersion: ceph.rook.io/v1beta1kind: Poolmetadata: name: replicapool namespace: rook-cephspec: replicated: size: 3 # For an erasure-coded pool, comment out the replication size above and uncomment the following settings. # Make sure you have enough OSDs to support the replica size or erasure code chunks. #erasureCoded: # dataChunks: 2 # codingChunks: 1---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: rook-ceph-blockprovisioner: ceph.rook.io/blockparameters: pool: replicapool # Specify the namespace of the rook cluster from which to create volumes. # If not specified, it will use `rook` as the default namespace of the cluster. # This is also the namespace where the cluster will be clusterNamespace: rook-ceph # Specify the filesystem type of the volume. If not specified, it will use `ext4`. fstype: xfsEOFkubectl apply -f rbd-storageclass.yaml还可以添加cephFS和object类型的存储池，然后创建对应的StorageClass具体可以看filesystem.yaml和object.yaml检查StorageClass创建sc时，会在rook-ceph上创建对应的Pool这里以rbd-storageclass.yaml为例123456789101112131415161718192021222324252627kubectl get sc# 输出示例NAME PROVISIONER AGErook-ceph-block ceph.rook.io/block 15mkubectl describe sc rook-ceph-block # 输出示例Name: rook-ceph-blockIsDefaultClass: NoAnnotations: kubectl.kubernetes.io/last-applied-configuration=&#123;\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":&#123;\"annotations\":&#123;&#125;,\"name\":\"rook-ceph-block\",\"namespace\":\"\"&#125;,\"parameters\":&#123;\"clusterNamespace\":\"rook-ceph\",\"fstype\":\"xfs\",\"pool\":\"replicapool\"&#125;,\"provisioner\":\"ceph.rook.io/block\"&#125;Provisioner: ceph.rook.io/blockParameters: clusterNamespace=rook-ceph,fstype=xfs,pool=replicapoolAllowVolumeExpansion: &lt;unset&gt;MountOptions: &lt;none&gt;ReclaimPolicy: DeleteVolumeBindingMode: ImmediateEvents: &lt;none&gt;kubectl -n rook-ceph exec -it rook-ceph-mon0-ll7fc -- ceph df# 输出示例GLOBAL: SIZE AVAIL RAW USED %RAW USED 116G 96979M 22767M 19.01 POOLS: NAME ID USED %USED MAX AVAIL OBJECTS replicapool 1 0 0 29245M 0卸载Rook-ceph这里提供卸载的操作步骤，请按需操作！删除StorageClass1kubectl delete -f rbd-storageclass.yaml删除Rook-Ceph-Cluster1kubectl delete -f cluster.yaml删除Rook-Operator1kubectl delete -f operator.yaml清理目录注意！这里是所有运行rook-ceph集群的节点都需要做清理1rm -rf /var/lib/rookPrometheus Operator说明Prometheus Operator 是 CoreOS 开发的基于 Prometheus 的 Kubernetes 监控方案，也可能是目前功能最全面的开源方案。Prometheus Operator 通过 Grafana 展示监控数据，预定义了一系列的 Dashboard要求kubernetes版本大于等于1.8.0CoreOS/Prometheus-Operator项目地址PrometheusPrometheus 是一套开源的系统监控报警框架，启发于 Google 的 borgmon 监控系统，作为社区开源项目进行开发，并成为CNCF第二个毕业的项目（第一个是kubernetes）特点强大的多维度数据模型灵活而强大的查询语句（PromQL）易于管理，高效使用 pull 模式采集时间序列数据，这样不仅有利于本机测试而且可以避免有问题的服务器推送坏的 metrics。可以采用 push gateway 的方式把时间序列数据推送至 Prometheus server 端可以通过服务发现或者静态配置去获取监控的 targets有多种可视化图形界面易于伸缩Prometheus组成架构Prometheus Server: 用于收集和存储时间序列数据Client Library: 客户端库，为需要监控的服务生成相应的 metrics 并暴露给Prometheus serverPush Gateway: 主要用于短期的 jobs。 jobs 可以直接向 Prometheus server 端推送它们的metrics。这种方式主要用于服务层面的 metrics。Exporters: 用于暴露已有的第三方服务的 metrics 给 Prometheus。Alertmanager: 从 Prometheus server 端接收到 alerts后，会进行去除重复数据，分组，并路由到对收的接受方式，发出报警。架构图Operator架构Operator即 Prometheus Operator，在 Kubernetes 中以 Deployment 运行。其职责是部署和管理Prometheus Server，根据 ServiceMonitor 动态更新 Prometheus Server 的监控对象。Prometheus ServerPrometheus Server 会作为 Kubernetes 应用部署到集群中。为了更好地在 Kubernetes 中管理 Prometheus，CoreOS 的开发人员专门定义了一个命名为 Prometheus 类型的 Kubernetes 定制化资源。我们可以把 Prometheus看作是一种特殊的 Deployment，它的用途就是专门部署 Prometheus Server。Service这里的Service 就是 Cluster 中的 Service 资源，也是 Prometheus 要监控的对象，在 Prometheus 中叫做Target。每个监控对象都有一个对应的 Service。比如要监控 Kubernetes Scheduler，就得有一个与 Scheduler对应的 Service。当然，Kubernetes 集群默认是没有这个 Service 的，Prometheus Operator会负责创建。ServiceMonitorOperator能够动态更新 Prometheus 的 Target 列表，ServiceMonitor 就是 Target 的抽象。比如想监控Kubernetes Scheduler，用户可以创建一个与 Scheduler Service 相映射的 ServiceMonitor对象。Operator 则会发现这个新的 ServiceMonitor，并将 Scheduler 的 Target 添加到 Prometheus的监控列表中。ServiceMonitor 也是 Prometheus Operator 专门开发的一种 Kubernetes 定制化资源类型。Alertmanager除了 Prometheus 和 ServiceMonitor，Alertmanager 是 Operator 开发的第三种 Kubernetes 定制化资源。我们可以把 Alertmanager 看作是一种特殊的 Deployment，它的用途就是专门部署 Alertmanager 组件。部署Prometheus-Operator切换工作目录12mkdir -p /root/yaml/prometheus-operatorcd /root/yaml/prometheus-operator添加coreos源12# 添加coreos源helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/创建命名空间1kubectl create namespace monitoring部署prometheus-operator这里通过--set指定了image的地址1234567helm install coreos/prometheus-operator \\ --name coreos-prometheus-operator \\ --namespace monitoring \\ --set global.hyperkube.repository=zhangguanzhang/quay.io.coreos.hyperkube \\ --set image.repository=zhangguanzhang/quay.io.coreos.prometheus-operator \\ --set prometheusConfigReloader.repository=zhangguanzhang/quay.io.coreos.prometheus-config-reloader \\ --set rbacEnable=true部署kube-prometheus通过运行helm命令安装时，指定一些变量来达到自定义配置的目的定义grafana初始admin密码为password，默认值是admin定义alertmanager和prometheus使用名为rook-ceph-block的StorageClass，访问模式为ReadWriteOnce，大小5Gi，默认是50Gi定义grafana、alertmanager、prometheus的Service类型为NodePort，默认是ClusterIP这里的--set可以定义很多变量，具体可以在这里，查看里面每个文件夹的values.yaml这里配置的变量请自己根据情况修改12345678910111213141516171819202122232425262728293031helm install coreos/kube-prometheus \\ --name kube-prometheus \\ --namespace monitoring \\ --set alertmanager.image.repository=\"zhangguanzhang/quay.io.prometheus.alertmanager\" \\ --set alertmanager.service.type=\"NodePort\" \\ --set alertmanager.storageSpec.volumeClaimTemplate.spec.storageClassName=\"rook-ceph-block\" \\ --set alertmanager.storageSpec.volumeClaimTemplate.spec.accessModes[0]=\"ReadWriteOnce\" \\ --set alertmanager.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=\"5Gi\" \\ --set grafana.adminPassword=\"password\" \\ --set grafana.service.type=\"NodePort\" \\ --set prometheus.image.repository=\"zhangguanzhang/quay.io.prometheus.prometheus\" \\ --set prometheus.service.type=\"NodePort\" \\ --set prometheus.storageSpec.volumeClaimTemplate.spec.storageClassName=\"rook-ceph-block\" \\ --set prometheus.storageSpec.volumeClaimTemplate.spec.accessModes[0]=\"ReadWriteOnce\" \\ --set prometheus.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=\"5Gi\" \\ --set prometheus.deployCoreDNS=true \\ --set prometheus.deployKubeDNS=false \\ --set prometheus.deployKubeEtcd=true \\ --set exporter-kube-controller-manager.endpoints[0]=\"172.16.80.201\" \\ --set exporter-kube-controller-manager.endpoints[1]=\"172.16.80.202\" \\ --set exporter-kube-controller-manager.endpoints[2]=\"172.16.80.203\" \\ --set exporter-kube-etcd.etcdPort=2379 \\ --set exporter-kube-etcd.scheme=\"https\" \\ --set exporter-kube-etcd.endpoints[0]=\"172.16.80.201\" \\ --set exporter-kube-etcd.endpoints[1]=\"172.16.80.202\" \\ --set exporter-kube-etcd.endpoints[2]=\"172.16.80.203\" \\ --set exporter-kube-scheduler.endpoints[0]=\"172.16.80.201\" \\ --set exporter-kube-scheduler.endpoints[1]=\"172.16.80.202\" \\ --set exporter-kube-scheduler.endpoints[2]=\"172.16.80.203\" \\ --set exporter-kube-state.kube_state_metrics.image.repository=\"gcrxio/kube-state-metrics\" \\ --set exporter-kube-state.addon_resizer.image.repository=\"gcrxio/addon-resizer\"检查部署情况1234567891011121314151617181920212223242526272829303132333435363738kubectl -n monitoring get all# 输出示例NAME READY STATUS RESTARTS AGEpod/alertmanager-kube-prometheus-0 2/2 Running 0 43mpod/kube-prometheus-exporter-kube-state-66b8849c9b-cq5pp 2/2 Running 0 42mpod/kube-prometheus-exporter-node-p6z67 1/1 Running 0 43mpod/kube-prometheus-exporter-node-qnmjt 1/1 Running 0 43mpod/kube-prometheus-exporter-node-vr4sp 1/1 Running 0 43mpod/kube-prometheus-grafana-f869c754-x5x7n 2/2 Running 0 43mpod/prometheus-kube-prometheus-0 3/3 Running 1 43mpod/prometheus-operator-5db9df7ffc-dxtqh 1/1 Running 0 49mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/alertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,6783/TCP 43mservice/kube-prometheus NodePort 10.97.183.252 &lt;none&gt; 9090:30900/TCP 43mservice/kube-prometheus-alertmanager NodePort 10.105.140.173 &lt;none&gt; 9093:30903/TCP 43mservice/kube-prometheus-exporter-kube-state ClusterIP 10.108.236.146 &lt;none&gt; 80/TCP 43mservice/kube-prometheus-exporter-node ClusterIP 10.96.14.75 &lt;none&gt; 9100/TCP 43mservice/kube-prometheus-grafana NodePort 10.109.4.170 &lt;none&gt; 80:30164/TCP 43mservice/prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 43mNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdaemonset.apps/kube-prometheus-exporter-node 3 3 3 3 3 &lt;none&gt; 43mNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeployment.apps/kube-prometheus-exporter-kube-state 1 1 1 1 43mdeployment.apps/kube-prometheus-grafana 1 1 1 1 43mdeployment.apps/prometheus-operator 1 1 1 1 49mNAME DESIRED CURRENT READY AGEreplicaset.apps/kube-prometheus-exporter-kube-state-658f46b8dd 0 0 0 43mreplicaset.apps/kube-prometheus-exporter-kube-state-66b8849c9b 1 1 1 42mreplicaset.apps/kube-prometheus-grafana-f869c754 1 1 1 43mreplicaset.apps/prometheus-operator-5db9df7ffc 1 1 1 49mNAME DESIRED CURRENT AGEstatefulset.apps/alertmanager-kube-prometheus 1 1 43mstatefulset.apps/prometheus-kube-prometheus 1 1 43m访问Prometheus-Operator部署时已经定义alertmanager、prometheus、grafana的Service为NodePort根据检查部署的情况，得知kube-prometheus的NodePort为30900kube-prometheus-alertmanager的NodePort为30903kube-prometheus-grafana的NodePort为30164直接通过这些端口访问即可grafana已内嵌了基础的Dashboard模板，以admin用户登录即可见EFK说明官方提供简单的fluentd-elasticsearch样例，可以作为测试用途已经包含在kubernetes项目当中链接这里使用kubernetes-server-linux-amd64.tar.gz里面的kubernetes-src.tar.gz提供的Addons修改elasticsearch使用rook-ceph提供的StorageClass作为持久化存储，默认是使用emptyDir注意EFK集群部署之后，kibana和elasticsearch初始化过程会极大的消耗服务器资源请保证你的环境能撑的住！！！！配置不够，服务器真的会失去响应实测3节点4C 16G SSD硬盘，CPU持续十几分钟的满载解压源代码12345678910tar xzf kubernetes-server-linux-amd64.tar.gz kubernetes/kubernetes-src.tar.gzcd kubernetestar xzf kubernetes/kubernetes-src.tar.gztar xzf kubernetes-src.tar.gz \\cluster/addons/fluentd-elasticsearch/es-service.yaml \\cluster/addons/fluentd-elasticsearch/es-statefulset.yaml \\cluster/addons/fluentd-elasticsearch/fluentd-es-configmap.yaml \\cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yaml \\cluster/addons/fluentd-elasticsearch/kibana-deployment.yaml \\cluster/addons/fluentd-elasticsearch/kibana-service.yaml切换工作目录1cd cluster/addons/fluentd-elasticsearch/修改yaml文件删除es-statefuleset.yaml里面的字段，位置大概在100行左右123volumes: - name: elasticsearch-logging emptyDir: &#123;&#125;添加volumeClaimTemplates字段，声明使用rook-ceph提供的StorageClass，大小5Gi位置在StatefulSet.spec，大概67行左右123456789volumeClaimTemplates:- metadata: name: elasticsearch-logging spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: \"rook-ceph-block\" resources: requests: storage: 5Gi修改后，es-statefulset.yaml内容如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119# RBAC authn and authzapiVersion: v1kind: ServiceAccountmetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: elasticsearch-logging labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilerules:- apiGroups: - \"\" resources: - \"services\" - \"namespaces\" - \"endpoints\" verbs: - \"get\"---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: kube-system name: elasticsearch-logging labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilesubjects:- kind: ServiceAccount name: elasticsearch-logging namespace: kube-system apiGroup: \"\"roleRef: kind: ClusterRole name: elasticsearch-logging apiGroup: \"\"---# Elasticsearch deployment itselfapiVersion: apps/v1kind: StatefulSetmetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging version: v5.6.4 kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcilespec: serviceName: elasticsearch-logging replicas: 2 selector: matchLabels: k8s-app: elasticsearch-logging version: v5.6.4 volumeClaimTemplates: - metadata: name: elasticsearch-logging spec: accessModes: [ \"ReadWriteOnce\" ] storageClassName: rook-ceph-block resources: requests: storage: 5Gi template: metadata: labels: k8s-app: elasticsearch-logging version: v5.6.4 kubernetes.io/cluster-service: \"true\" spec: serviceAccountName: elasticsearch-logging containers: - image: gcrxio/elasticsearch:v5.6.4 name: elasticsearch-logging resources: # need more cpu upon initialization, therefore burstable class limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: elasticsearch-logging mountPath: /data env: - name: \"NAMESPACE\" valueFrom: fieldRef: fieldPath: metadata.namespace volumes: - name: elasticsearch-logging emptyDir: &#123;&#125; # Elasticsearch requires vm.max_map_count to be at least 262144. # If your OS already sets up this number to a higher value, feel free # to remove this init container. initContainers: - image: alpine:3.6 command: [\"/sbin/sysctl\", \"-w\", \"vm.max_map_count=262144\"] name: elasticsearch-logging-init securityContext: privileged: true注释kibana-deployment.yaml定义的环境变量大概在35行左右12# - name: SERVER_BASEPATH# value: /api/v1/namespaces/kube-system/services/kibana-logging/proxy修改镜像地址默认yaml定义的镜像地址是k8s.gcr.io，需要科学上网变更成gcrxio1sed -e 's,k8s.gcr.io,gcrxio,g' -i *yaml给节点打Labelfluentd-es-ds.yaml有nodeSelector字段定义了运行在带有beta.kubernetes.io/fluentd-ds-ready: &quot;true&quot;标签的节点上这里为了方便，直接给所有节点都打上标签1kubectl label node --all beta.kubernetes.io/fluentd-ds-ready=true部署EFK1kubectl apply -f .查看部署情况12345678910111213141516171819202122kubectl -n kube-system get pod -l k8s-app=elasticsearch-loggingNAME READY STATUS RESTARTS AGEelasticsearch-logging-0 1/1 Running 1 10melasticsearch-logging-1 1/1 Running 0 10mkubectl -n kube-system get pod -l k8s-app=kibana-loggingNAME READY STATUS RESTARTS AGEkibana-logging-56fb9d765-l95kj 1/1 Running 1 37mkubectl -n kube-system get pod -l k8s-app=fluentd-esNAME READY STATUS RESTARTS AGEfluentd-es-v2.0.4-2mwz7 1/1 Running 0 3mfluentd-es-v2.0.4-7mk4d 1/1 Running 0 3mfluentd-es-v2.0.4-zqtpc 1/1 Running 0 3mkubectl -n kube-system get svc -l k8s-app=elasticsearch-loggingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEelasticsearch-logging ClusterIP 10.111.107.21 &lt;none&gt; 9200/TCP 39mkubectl -n kube-system get svc -l k8s-app=kibana-loggingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkibana-logging ClusterIP 10.96.170.77 &lt;none&gt; 5601/TCP 39m访问EFK修改elasticsearch和kibana的svc为nodePort12kubectl patch -n kube-system svc elasticsearch-logging -p '&#123;\"spec\":&#123;\"type\":\"NodePort\"&#125;&#125;'kubectl patch -n kube-system svc kibana-logging -p '&#123;\"spec\":&#123;\"type\":\"NodePort\"&#125;&#125;'查看分配的nodePort1234567kubectl -n kube-system get svc -l k8s-app=elasticsearch-loggingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEelasticsearch-logging NodePort 10.111.107.21 &lt;none&gt; 9200:30542/TCP 42mkubectl -n kube-system get svc -l k8s-app=kibana-loggingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkibana-logging NodePort 10.96.170.77 &lt;none&gt; 5601:30998/TCP 42m可以看到端口分别为30542和30998在github上获取yaml文件如果不想用kubernetes-src.tar.gz里面的Addons可以直接下载github上面的文件，也是一样的123456wget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/es-service.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/es-statefulset.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/fluentd-es-configmap.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/kibana-deployment.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/kibana-service.yaml本文至此结束","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://luanlengli.github.io/tags/Kubernetes/"},{"name":"Docker","slug":"Docker","permalink":"https://luanlengli.github.io/tags/Docker/"}]},{"title":"CentOS-7.6(1810)虚拟机模板制作","slug":"CentOS-7-6-1810-虚拟机模板制作","date":"2018-12-05T04:23:05.000Z","updated":"2020-01-10T08:18:27.111Z","comments":true,"path":"2018/12/05/CentOS-7-6-1810-虚拟机模板制作.html","link":"","permalink":"https://luanlengli.github.io/2018/12/05/CentOS-7-6-1810-虚拟机模板制作.html","excerpt":"","text":"CentOS-7.6(1810)虚拟机模板制作基于RHEL7.6的CentOS-7.6(1810)在12月初正式发布了发行注记顺便更新一下虚拟机模板，这里记录一下操作过程。下载镜像可以在CentOS官网下载，也可以通过国内镜像源下载下载镜像名CentOS-7-x86_64-Minimal-1810.isoCentOS官网下载链接阿里云下载链接创建虚拟机这里使用VMware Workstation 14 Pro 版本号14.1.3 build-9474260虚拟机规格客户机操作系统版本Red Hat Enterprise Linux 7 64 位处理器数量1内存2GB硬盘40GB网络适配器NAT模式安装操作系统语言选择English软件包选择Minimal Install硬盘分区/dev/sda1boot分区、1GB、EXT4/dev/sda2/分区、39GB、XFS这里不使用swap分区，有需要可以自己增加swap分区网络设置NAT地址段为172.16.80.0/24这里设置为IP地址172.16.80.200子网掩码255.255.255.0网关172.16.80.2DNS114.114.114.114时区选择Asia/Shanghai打开网络对时KDUMP看情况选择打开或者关闭，这里我选择关闭设置ROOT密码操作系统启动后初始化设置关闭SELINUX12sed -i 's,^SELINUX=.*,SELINUX=disabled,' /etc/selinux/configsetenforce 0关闭防火墙服务1systemctl disable --now firewalld.service清空iptables规则1iptables -F &amp;&amp; iptables -t nat -F &amp;&amp; iptables -t mangle -F &amp;&amp; iptables -X配置ssh证书登录通过ssh命令生成密钥对将~/.ssh/id_rsa.pub提取出来1ssh-keygen -t rsa -b 4096 -N \"\" -f ~/.ssh/id_rsa添加sysctl参数fs参数123456789101112cat &gt; /etc/sysctl.d/99-fs.conf &lt;&lt;EOF# 最大文件句柄数fs.file-max=1048576# 最大文件打开数fs.nr_open=1048576# 同一时间异步IO请求数fs.aio-max-nr=1048576# 在CentOS7.4引入了一个新的参数来控制内核的行为。 # /proc/sys/fs/may_detach_mounts 默认设置为0# 当系统有容器运行的时候，需要将该值设置为1。fs.may_detach_mounts=1EOFvm参数12345678910cat &gt; /etc/sysctl.d/99-vm.conf &lt;&lt;EOF# 内存耗尽才使用swap分区vm.swappiness=10# 当内存耗尽时，内核会触发OOM killer根据oom_score杀掉最耗内存的进程vm.panic_on_oom=0# 允许overcommitvm.overcommit_memory=1# 定义了进程能拥有的最多内存区域，默认65536vm.max_map_count=262144EOFnet参数1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465cat &gt; /etc/sysctl.d/99-net.conf &lt;&lt;EOF# 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤net.bridge.bridge-nf-call-arptables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# 关闭严格校验数据包的反向路径，默认值1net.ipv4.conf.default.rp_filter=0net.ipv4.conf.all.rp_filter=0# 进程间通信发送数据, 默认100net.unix.max_dgram_qlen=512# 设置 conntrack 的上限net.netfilter.nf_conntrack_max=1048576# 设置连接跟踪表中处于TIME_WAIT状态的超时时间net.netfilter.nf_conntrack_tcp_timeout_timewait=30# 设置连接跟踪表中TCP连接超时时间net.netfilter.nf_conntrack_tcp_timeout_established=1200# 端口最大的监听队列的长度net.core.somaxconn=21644# 接收自网卡、但未被内核协议栈处理的报文队列长度net.core.netdev_max_backlog=262144# 系统无内存压力、启动压力模式阈值、最大值，单位为页的数量#net.ipv4.tcp_mem=1541646 2055528 3083292# 内核socket接收缓存区字节数min/default/maxnet.core.rmem=4096 65536 8388608# 内核socket发送缓存区字节数min/default/maxnet.core.wmem=4096 65536 8388608# 开启自动调节缓存模式net.ipv4.tcp_moderate_rcvbuf=1# TCP阻塞控制算法BBR，Linux内核版本4.9开始内置BBR算法#net.ipv4.tcp_congestion_control=bbr#net.core.default_qdisc=fq# 用作本地随机TCP端口的范围net.ipv4.ip_local_port_range=10000 65000# 打开ipv4数据包转发net.ipv4.ip_forward=1# 允许应用程序能够绑定到不属于本地网卡的地址net.ipv4.ip_nonlocal_bind=1# 系统中处于 SYN_RECV 状态的 TCP 连接数量net.ipv4.tcp_max_syn_backlog=16384# 内核中管理 TIME_WAIT 状态的数量net.ipv4.tcp_max_tw_buckets=5000# 指定重发 SYN/ACK 的次数net.ipv4.tcp_synack_retries=2# TCP连接中TIME_WAIT sockets的快速回收net.ipv4.tcp_tw_recycle=0# 不属于任何进程的tcp socket最大数量. 超过这个数量的socket会被reset, 并告警net.ipv4.tcp_max_orphans=1024# TCP FIN报文重试次数net.ipv4.tcp_orphan_retries=8# 加快系统关闭处于 FIN_WAIT2 状态的 TCP 连接net.ipv4.tcp_fin_timeout=15# TCP连接keepalive的持续时间，默认7200net.ipv4.tcp_keepalive_time=600# TCP keepalive探测包发送间隔net.ipv4.tcp_keepalive_intvl=30# TCP keepalive探测包重试次数net.ipv4.tcp_keepalive_probes=10# TCP FastOpen# 0:关闭 ; 1:作为客户端时使用 ; 2:作为服务器端时使用 ; 3:无论作为客户端还是服务器端都使用net.ipv4.tcp_fastopen=3# 限制TCP重传次数net.ipv4.tcp_retries1=3# TCP重传次数到达上限时，关闭TCP连接net.ipv4.tcp_retries2=15EOF修改limits参数1234cat &gt; /etc/security/limits.d/99-centos.conf &lt;&lt;EOF* - nproc 1048576* - nofile 1048576EOF修改journal设置12345sed -e 's,^#Compress=yes,Compress=yes,' \\ -e 's,^#SystemMaxUse=,SystemMaxUse=2G,' \\ -e 's,^#Seal=yes,Seal=yes,' \\ -e 's,^#RateLimitBurst=1000,RateLimitBurst=5000,' \\ -i /etc/systemd/journald.conf修改终端提示符123export PS1=\"[\\t]\\[$(tput setaf 1)\\][\\u@\\h:\\W]\\[$(tput setaf 7)\\]\\\\$ \\[$(tput sgr0)\\]\"echo 'export PS1=\"[\\t]\\[$(tput setaf 1)\\][\\u@\\h:\\W]\\[$(tput setaf 7)\\]\\\\$ \\[$(tput sgr0)\\]\"' &gt;&gt; ~/.bashrc修改网卡配置信息CentOS安装设置网卡后，会添加很多不需要的字段，例如UUID、HWADDR什么的删减后字段信息如下123456789101112cat /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=EthernetBOOTPROTO=noneNAME=ens33DEVICE=ens33ONBOOT=yesIPADDR=172.16.80.200NETMASK=255.255.255.0GATEWAY=172.16.80.2DNS1=114.114.114.114NM_CONTROLLED=noUSERCTL=no修改YUM源替换成阿里云的源1234sed -e 's,^mirrorlist,#mirrorlist,g' \\ -e 's,^#baseurl,baseurl,g' \\ -e 's,http://mirror.centos.org,https://mirrors.aliyun.com,g' \\ -i /etc/yum.repos.d/*.repo安装EPEL源1yum install epel-release -y修改EPEL源12345sed -e 's,^#baseurl,baseurl,g' \\ -e 's,^metalink,#metalink,g' \\ -e 's,^mirrorlist=,#mirrorlist=,g' \\ -e 's,http://download.fedoraproject.org/pub,https://mirrors.aliyun.com,g' \\ -i /etc/yum.repos.d/epel.repo更新软件包通常来说，安装完操作系统，都需要更新一下软件包1yum update -y安装常用软件包CentOS最小化安装不能满足我的使用，需要额外安装一些软件包1234567891011121314151617181920yum groups install base -yyum install -y nc \\ git \\ vim \\ ipvsadm \\ tree \\ dstat \\ iotop \\ htop \\ socat \\ ipset\\ conntrack \\ bash-completion-extras \\ tcpdump \\ wireshark \\ bcc-tools \\ perf \\ trace-cmd \\ systemtap \\ nethogs修改HISTORY参数1234567cat &gt; /etc/profile.d/history.sh &lt;&lt;EOFexport HISTSIZE=10000export HISTFILESIZE=10000export HISTCONTROL=ignoredupsexport HISTTIMEFORMAT=\"`whoami` %F %T \"export HISTIGNORE=\"ls:pwd:ll:ls -l:ls -a:ll -a\"EOF修改时区1timedatectl set-timezone Asia/Shanghai启动NTP网络对时1systemctl enable chronyd.service修改LANG默认值123localectl set-locale LANG=en_US.UTF-8localectl set-keymap uslocalectl set-x11-keymap us修改SSH配置这里禁用root通过密码方式登录，修改默认端口22→22331234sed -e 's,^PermitRootLogin.*,PermitRootLogin without-password,' \\ -e 's,^#UseDNS no,UseDNS no,' \\ -e 's,^#Port 22,Port 2233,' \\ -i /etc/ssh/sshd_config修改CPUfreq调节器为了让 CPU 发挥最大性能，将 CPUfreq 调节器模式设置为 performance 模式。查看调节模式通过cpupower命令查看系统支持的调节模式1cpupower frequency-info --governors示例输出，可以看到支持performance和powersave两种模式12analyzing CPU 0: available cpufreq governors: performance powersave如果命令输出如下，则不支持CPUfreq调节12analyzing CPU 0: available cpufreq governors: Not Available查看当前调节模式通过 cpupower 命令查看系统当前的 CPUfreq 调节器模式1cpupower frequency-info --policy示例输出1234analyzing CPU 0: current policy: frequency should be within 800 MHz and 4.60 GHz. The governor \"powersave\" may decide which speed to use within this range.可以看到当前的调节器模式是powersave修改调节模式通过cpupower命令修改CPUfreq调节模式1cpupower frequency-set --governor performance可选操作禁用IPV6设置123456cat &gt; /etc/sysctl.d/99-disable-ipv6.conf &lt;&lt;EOF# 禁用ipv6net.ipv6.conf.all.disable_ipv6=1net.ipv6.conf.default.disable_ipv6=1net.ipv6.conf.lo.disable_ipv6=1EOF禁用ICMP1234cat &gt; /etc/sysctl.d/99-disable-icmp.conf &lt;&lt;EOFnet.ipv4.icmp_echo_ignore_all=1net.ipv4.icmp_echo_ignore_broadcasts=1EOF添加vim设置将vim设置写入~/.vimrc文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263cat &gt; ~/.vimrc &lt;&lt;EOF\" 显示行号set number\" 高亮光标所在行set cursorline\" 打开语法显示syntax on\" 关闭备份set nobackup\" 没有保存或文件只读时弹出确认set confirm\" 禁用modeline功能set nomodeline\" tab缩进set tabstop=4set shiftwidth=4set expandtabset smarttab\" 默认缩进4个空格大小 set shiftwidth=4 \" 文件自动检测外部更改set autoread\" 高亮查找匹配set hlsearch\" 显示匹配set showmatch\" 背景色设置为黑色set background=dark\" 浅色高亮显示当前行autocmd InsertLeave * se nocul\" 显示输入的命令set showcmd\" 字符编码set encoding=utf-8\" 开启终端256色显示set t_Co=256\" 增量式搜索 set incsearch\" 设置默认进行大小写不敏感查找set ignorecase\" 如果有一个大写字母，则切换到大小写敏感查找set smartcase\" 不产生swap文件set noswapfile\" 设置备份时的行为为覆盖set backupcopy=yes\" 关闭提示音set noerrorbells\" 历史记录set history=10000\" 显示行尾空格set listchars=tab:»■,trail:■\" 显示非可见字符set list\" c文件自动缩进set cindent\" 文件自动缩进set autoindent\" 检测文件类型filetype on\" 智能缩进set smartindentEOF配置开发语言国内源Python1234pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/pip config set global.trusted-host mirrors.aliyun.compip config set global.timeout 120pip config set install.trusted-host mirrors.aliyun.comNPM123456789npm config set registry https://mirrors.huaweicloud.com/repository/npm/npm config set disturl https://mirrors.huaweicloud.com/nodejsnpm config set sass_binary_site https://mirrors.huaweicloud.com/node-sassnpm config set phantomjs_cdnurl https://mirrors.huaweicloud.com/phantomjsnpm config set chromedriver_cdnurl https://mirrors.huaweicloud.com/chromedrivernpm config set operadriver_cdnurl https://mirrors.huaweicloud.com/operadrivernpm config set electron_mirror https://mirrors.huaweicloud.com/electron/npm config set python_mirror https://mirrors.huaweicloud.com/python npm cache clean -fGolangGolang 1.11+开启Go Module12go env -w GO111MODULE=ongo env -w GOPROXY='https://mirrors.aliyun.com/goproxy/'针对Golang 1.13+12go env -w GO111MODULE=ongo env -w GOPROXY='https://goproxy.cn,https://mirrors.aliyun.com/goproxy/,http://mirrors.huaweicloud.com/goproxy,direct'Maven阿里云代理了Maven的公共仓库，这里是说明文档Maven配置打开 Maven 的配置文件(windows机器一般在maven安装目录的conf/settings.xml)，在&lt;mirrors&gt;&lt;/mirrors&gt;标签中添加 mirror 子节点:123456&lt;mirror&gt; &lt;id&gt;aliyunmaven&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;aliyunmaven&lt;/name&gt; &lt;url&gt;https://maven.aliyun.com/repository/public&lt;/url&gt;&lt;/mirror&gt;如果想使用其它代理仓库,可在&lt;repository&gt;&lt;/repository&gt;节点中加入对应的仓库使用地址。以使用spring代理仓为例：12345678910&lt;repository&gt; &lt;id&gt;spring&lt;/id&gt; &lt;url&gt;https://maven.aliyun.com/repository/spring&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt;&lt;/repository&gt;Gradle配置在 build.gradle 文件中加入以下代码:1234567allprojects &#123; repositories &#123; maven &#123; url 'https://maven.aliyun.com/repository/public/' &#125; mavenLocal() mavenCentral() &#125;&#125;如果想使用 maven.aliyun.com 提供的其它代理仓，以使用 spring 仓为例，代码如下:12345678allprojects &#123; repositories &#123; maven &#123; url 'https://maven.aliyun.com/repository/public/' &#125; maven &#123; url 'https://maven.aliyun.com/repository/spring/'&#125; mavenLocal() mavenCentral() &#125;&#125;配置内核模块配置lvs模块LVS的调度算法简介12345678910111213cat &gt; /etc/modules-load.d/ipvs.conf &lt;&lt;EOFip_vs# 负载均衡调度算法-最少连接ip_vs_lc# 负载均衡调度算法-加权最少连接ip_vs_wlc# 负载均衡调度算法-轮询ip_vs_rr# 负载均衡调度算法-加权轮询ip_vs_wrr# 源地址散列调度算法ip_vs_shEOF配置连接状态跟踪模块12345cat &gt; /etc/modules-load.d/nf_conntrack.conf &lt;&lt;EOFnf_conntracknf_conntrack_ipv4#nf_conntrack_ipv6EOF配置kvm模块12345678910cat &gt; /etc/modules-load.d/kvm.conf &lt;&lt;EOF# Intel CPU开启嵌套虚拟化options kvm-intel nested=1options kvm-intel enable_shadow_vmcs=1options kvm-intel enable_apicv=1options kvm-intel ept=1# AMD CPU开启嵌套虚拟化#options kvm-amd nested=1EOF安装docker版本可以自行选择自带的Docker1.13或者docker-ce这里以docker-ce 最新版本为例清理旧Docker包1yum remove -y docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine安装Docker依赖包1yum install -y yum-utils device-mapper-persistent-data lvm2添加Docker-CE源1yum-config-manager --add-repo http://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo修改Docker—CE源地址1sed -e 's,https://download.docker.com,https://mirrors.aliyun.com/docker-ce,g' -i /etc/yum.repos.d/docker-ce.repo安装Docker-CE1yum install docker-ce -y配置Docker启动参数12345678910111213141516171819mkdir -p /etc/dockercat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\"], \"insecure-registries\": [], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\": \"3\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"data-root\": \"/var/lib/docker\", \"max-concurrent-downloads\": 10&#125;EOF设置Docker命令补全1cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/禁用Docker-CE源1yum-config-manager --disable docker-ce-stable清理现场清理yum缓存1yum clean all关闭操作系统1history -c &amp;&amp; sys-unconfig","categories":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://luanlengli.github.io/tags/Linux/"}]},{"title":"hello world~","slug":"hello-world","date":"2018-12-01T04:50:07.000Z","updated":"2018-12-09T06:07:41.000Z","comments":true,"path":"2018/12/01/hello-world.html","link":"","permalink":"https://luanlengli.github.io/2018/12/01/hello-world.html","excerpt":"","text":"参考了Hexo文档，简单搭建完hexo之后，又花了一些时间来调整主题什么的。慢慢会将以前积累的文档，放到这里来。用输出倒逼输入","categories":[],"tags":[{"name":"杂谈","slug":"杂谈","permalink":"https://luanlengli.github.io/tags/杂谈/"}]}]}