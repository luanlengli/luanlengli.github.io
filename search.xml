<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【施工中】etcd学习笔记]]></title>
    <url>%2F2018%2F12%2F22%2Fetcd%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html</url>
    <content type="text"><![CDATA[说明 Etcd 是 CoreOS 推出的分布式一致性键值存储，用于共享配置和服务发现 Etcd 支持集群模式部署，从而实现自身高可用 本文以CentOS-7.6和etcd-v3.3.10为例 etcd安装二进制文件安装下载1234567# 下载并解压wget -q -O - https://github.com/etcd-io/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz | tar xz# 查看解压后的文件ls etcd-v3.3.10-linux-amd64Documentation etcd etcdctl README-etcdctl.md README.md READMEv2-etcdctl.md# 将二进制执行文件移动到/usr/local/bin/mv etcd-v3.3.10-linux-amd64/etcd etcd-v3.3.10-linux-amd64/etcdctl /usr/local/bin/ 配置创建用户12groupadd -r etcduseradd -r -g etcd -s /bin/false etcd 创建目录1mkdir -p /var/lib/etcd /etc/etcd/ 配置文件创建配置文件etcd.config.yaml，内容如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# This is the configuration file for the etcd server.# Human-readable name for this member.name: 'default'# Path to the data directory.data-dir: /var/lib/etcd/default.etcd# Path to the dedicated wal directory.wal-dir: /var/lib/etcd/wal# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 10000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: http://localhost:2380# List of comma separated URLs to listen on for client traffic.listen-client-urls: http://localhost:2379# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 5# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: http://localhost:2380# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: http://localhost:2379# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster:# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: # Path to the client server TLS key file. key-file: # Enable client cert authentication. client-cert-auth: false # Path to the client server TLS trusted CA cert file. trusted-ca-file: # Client TLS using generated certificates auto-tls: falsepeer-transport-security: # Path to the peer server TLS cert file. cert-file: # Path to the peer server TLS key file. key-file: # Enable peer client cert authentication. client-cert-auth: false # Path to the peer server TLS trusted CA cert file. trusted-ca-file: # Peer TLS using generated certificates. auto-tls: false# Enable debug-level logging for etcd.debug: falselogger: zap# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [stderr]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: periodicauto-compaction-retention: "1" 创建服务文件使用systemd托管etcd的服务 1234567891011121314151617cat &gt; /usr/lib/systemd/system/etcd.service &lt;&lt;EOF[Unit]Description=etcd key-value storeDocumentation=https://github.com/etcd-io/etcdAfter=network.target[Service]User=etcdType=notifyExecStart=/usr/local/bin/etcd --config-file /etc/etcd/etcd.config.yamlRestart=alwaysRestartSec=10sLimitNOFILE=65535[Install]WantedBy=multi-user.targetEOF 运行etcd123chown -R etcd:etcd /var/lib/etcd /etc/etcdsystemctl daemon-reloadsystemctl start etcd.service 验证etcd服务123etcdctl cluster-healthmember 8e9e05c52164694d is healthy: got healthy result from http://localhost:2379cluster is healthy etcd集群部署构建集群的方式静态发现预先已知 Etcd 集群中有哪些节点，在启动时直接指定好 Etcd 的各个 node 节点地址 动态发现通过已有的 Etcd 集群作为数据交互点，然后在扩展新的集群时实现通过已有集群进行服务发现的机制 DNS动态发现通过 DNS 查询方式获取其他节点地址信息 节点信息这里只提供静态发现部署etcd集群的流程 IP地址 主机名 CPU 内存 172.16.80.201 etcd1 4 8G 172.16.80.202 etcd2 4 8G 172.16.80.203 etcd3 4 8G 静态发现部署etcd集群创建配置文件etcd11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# This is the configuration file for the etcd server.# Human-readable name for this member.name: 'etcd1'# Path to the data directory.data-dir: /var/lib/etcd/etcd1.etcd# Path to the dedicated wal directory.wal-dir: /var/lib/etcd/wal# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 10000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: 'https://172.16.80.201:2380'# List of comma separated URLs to listen on for client traffic.listen-client-urls: 'https://172.16.80.201:2379,http://127.0.0.1:2379'# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 5# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: 'https://172.16.80.201:2380'# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: 'https://172.16.80.201:2379'# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: 'etcd1=http://172.16.80.201:2380,etcd2=http://172.16.80.202:2380,etcd3=http://172.16.80.203:2380'# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: # Path to the client server TLS key file. key-file: # Enable client cert authentication. client-cert-auth: false # Path to the client server TLS trusted CA cert file. trusted-ca-file: # Client TLS using generated certificates auto-tls: falsepeer-transport-security: # Path to the peer server TLS cert file. cert-file: # Path to the peer server TLS key file. key-file: # Enable peer client cert authentication. client-cert-auth: false # Path to the peer server TLS trusted CA cert file. trusted-ca-file: # Peer TLS using generated certificates. auto-tls: false# Enable debug-level logging for etcd.debug: falselogger: zap# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [stderr]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: periodicauto-compaction-retention: "1" etcd21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# This is the configuration file for the etcd server.# Human-readable name for this member.name: 'etcd2'# Path to the data directory.data-dir: /var/lib/etcd/etcd2.etcd# Path to the dedicated wal directory.wal-dir: /var/lib/etcd/wal# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 10000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: 'https://172.16.80.202:2380'# List of comma separated URLs to listen on for client traffic.listen-client-urls: 'https://172.16.80.202:2379,http://127.0.0.1:2379'# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 5# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: 'https://172.16.80.202:2380'# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: 'https://172.16.80.202:2379'# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: 'etcd1=http://172.16.80.201:2380,etcd2=http://172.16.80.202:2380,etcd3=http://172.16.80.203:2380'# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: # Path to the client server TLS key file. key-file: # Enable client cert authentication. client-cert-auth: false # Path to the client server TLS trusted CA cert file. trusted-ca-file: # Client TLS using generated certificates auto-tls: falsepeer-transport-security: # Path to the peer server TLS cert file. cert-file: # Path to the peer server TLS key file. key-file: # Enable peer client cert authentication. client-cert-auth: false # Path to the peer server TLS trusted CA cert file. trusted-ca-file: # Peer TLS using generated certificates. auto-tls: false# Enable debug-level logging for etcd.debug: falselogger: zap# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [stderr]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: periodicauto-compaction-retention: "1" etcd31234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# This is the configuration file for the etcd server.# Human-readable name for this member.name: 'etcd3'# Path to the data directory.data-dir: /var/lib/etcd/etcd3.etcd# Path to the dedicated wal directory.wal-dir: /var/lib/etcd/wal# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 10000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: 'https://172.16.80.203:2380'# List of comma separated URLs to listen on for client traffic.listen-client-urls: 'https://172.16.80.203:2379,http://127.0.0.1:2379'# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 5# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: 'https://172.16.80.203:2380'# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: 'https://172.16.80.203:2379'# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: 'etcd1=http://172.16.80.201:2380,etcd2=http://172.16.80.202:2380,etcd3=http://172.16.80.203:2380'# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: # Path to the client server TLS key file. key-file: # Enable client cert authentication. client-cert-auth: false # Path to the client server TLS trusted CA cert file. trusted-ca-file: # Client TLS using generated certificates auto-tls: falsepeer-transport-security: # Path to the peer server TLS cert file. cert-file: # Path to the peer server TLS key file. key-file: # Enable peer client cert authentication. client-cert-auth: false # Path to the peer server TLS trusted CA cert file. trusted-ca-file: # Peer TLS using generated certificates. auto-tls: false# Enable debug-level logging for etcd.debug: falselogger: zap# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [stderr]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: periodicauto-compaction-retention: "1" 启动etcd集群1234for NODE in 172.16.80.201 172.16.80.202 172.16.80.203;do ssh $NODE systemctl enable etcd ssh $NODE systemctl start etcd &amp;done 检查etcd集群123456789101112export ETCDCTL_API=2etcdctl --endpoints 'http://172.16.80.201:2379,http://172.16.80.202:2379,http://172.16.80.202:2379' cluster-healthmember 222fd3b0bb4a5931 is healthy: got healthy result from http://172.16.80.203:2379member 8349ef180b115a83 is healthy: got healthy result from http://172.16.80.201:2379member f525d2d797a7c465 is healthy: got healthy result from http://172.16.80.202:2379cluster is healthyexport ETCDCTL_API=3etcdctl --endpoints='http://172.16.80.201:2379,http://172.16.80.202:2379,http://172.16.80.202:2379' endpoint healthhttp://172.16.80.201:2379 is healthy: successfully committed proposal: took = 2.879402mshttp://172.16.80.203:2379 is healthy: successfully committed proposal: took = 6.708566mshttp://172.16.80.202:2379 is healthy: successfully committed proposal: took = 7.187607ms SSL/TLS加密此段翻译自官方文档 etcd支持自动TLS、客户端证书身份认证、客户端到服务器端以及对等集群的加密通信 生成证书为方便起见，这里使用CFSSL工具生成证书 下载CFSSL12345mkdir ~/bincurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64curl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x ~/bin/&#123;cfssl,cfssljson&#125;export PATH=$PATH:~/bin 创建工作目录12mkdir ~/cfsslcd ~/cfssl 创建默认配置文件12cfssl print-defaults config &gt; ca-config.jsoncfssl print-defaults csr &gt; ca-csr.json 证书类型介绍 客户端证书用于服务器验证客户端身份 服务器端证书用于客户端验证服务器端身份 对等证书由etcd集群成员使用，同时使用客户端认证和服务器端认证 配置CA修改ca-config.json 说明 expiry定义过期时间，这里的43800h为5年 usages字段定义用途 signing代表可以用于签发其他证书 key encipherment代表将密钥加密 server auth client auth 12345678910111213141516171819202122232425262728293031323334&#123; "signing": &#123; "default": &#123; "expiry": "43800h" &#125;, "profiles": &#123; "server": &#123; "expiry": "43800h", "usages": [ "signing", "key encipherment", "server auth" ] &#125;, "client": &#123; "expiry": "43800h", "usages": [ "signing", "key encipherment", "client auth" ] &#125;, "peer": &#123; "expiry": "43800h", "usages": [ "signing", "key encipherment", "server auth", "client auth" ] &#125; &#125; &#125;&#125; 配置证书请求修改ca-csr.json，可以根据自己的需求修改对应字段 1234567891011121314151617&#123; "CN": "My own CA", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "US", "L": "CA", "O": "My Company Name", "ST": "San Francisco", "OU": "Org Unit 1", "OU": "Org Unit 2" &#125; ]&#125; 生成CA证书运行以下命令生成CA证书 1cfssl gencert -initca ca-csr.json | cfssljson -bare ca - 生成以下文件 123ca-key.pemca.csrca.pem ca-key.pem为CA的私钥，请妥善保管 csr文件为证书请求文件，可以删除 生成服务器端证书1cfssl print-defaults csr &gt; server.json 修改server.json的CN和hosts字段，names字段按需修改 说明 hosts字段为列表，服务器端需要将自己作为客户端访问集群，可以使用hostname或者IP地址的形式定义hosts 123456789101112131415161718192021&#123; "CN": "example.net", "hosts": [ "127.0.0.1", "192.168.1.1", "ext.example.com", "coreos1.local", "coreos1" ], "key": &#123; "algo": "ecdsa", "size": 256 &#125;, "names": [ &#123; "C": "US", "L": "CA", "ST": "San Francisco" &#125; ]&#125; 创建服务器端证书和私钥 1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server 生成以下文件 123server-key.pemserver.csrserver.pem 生成客户端证书1cfssl print-defaults csr &gt; client.json 修改client.json，客户端证书不需要hosts字段，只需要CN字段设置为client 1234... "CN": "client", "hosts": [""],... 创建客户端证书和私钥 1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client 生成以下文件 123client-key.pemclient.csrclient.pem 生成对等证书1cfssl print-defaults csr &gt; member1.json 修改member1.json的CN字段和hosts字段 123456789... "CN": "member1", "hosts": [ "192.168.122.101", "ext.example.com", "member1.local", "member1" ],... 创建member1的证书和密钥 1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer member1.json | cfssljson -bare member1 生成以下文件 123member1-key.pemmember1.csrmember1.pem 对于多个member需要重复此操作，用于生成相对应的对等证书 验证证书1234openssl x509 -in ca.pem -text -nooutopenssl x509 -in server.pem -text -nooutopenssl x509 -in client.pem -text -nooutopenssl x509 -in member1.pem -text -noout 示例1、客户端使用HTTPS传输数据给服务器端准备CA证书ca.pem，密钥对server.pem server-key.pem 启动服务器端启动参数如下 123456etcd --name infra0 \--data-dir /var/lib/etcd/infra0 \--cert-file=/path/to/server.pem \--key-file=/path/to/server-key.pem \--advertise-client-urls=https://127.0.0.1:2379 \--listen-client-urls=https://127.0.0.1:2379 客户端使用HTTPS访问服务器端使用curl加载CA证书测试HTTPS连接 12345curl --cacert /path/to/ca.pem \https://127.0.0.1:2379/v2/keys/foo \-X PUT \-d value=bar \-v 示例2、客户端使用客户端证书作为身份验证访问服务器端在示例1的基础上，需要客户端证书client.pem和client-key.pem 启动服务器端启动参数如下，这里比示例1多了client-cert-auth和truested-ca-file 12345678etcd --name infra0 \--data-dir /var/lib/etcd/infra0 \--cert-file=/path/to/server.pem \--key-file=/path/to/server-key.pem \--advertise-client-urls=https://127.0.0.1:2379 \--listen-client-urls=https://127.0.0.1:2379 \--client-cert-auth \--trusted-ca-file=/path/to/ca.crt 重复示例1的访问 1curl --cacert /path/to/ca.crt https://127.0.0.1:2379/v2/keys/foo -XPUT -d value=bar -v 此命令结果会提示被服务器端拒绝 123...routines:SSL3_READ_BYTES:sslv3 alert bad certificate... 使用客户端证书访问服务器端 1234567curl --cacert /path/to/ca.pem \--cert /path/to/client.pem \--key /path/to/client-key.pem \-L https://127.0.0.1:2379/v2/keys/foo \-X PUT \-d value=bar \-v 命令结果包含以下信息 身份认证成功 1234...SSLv3, TLS handshake, CERT verify (15):...TLS handshake, Finished (20) 示例3、在集群中传输安全和客户端证书这里需要为每个member配备对应的member证书，操作步骤见生成证书部分 假设有2个member，这两个member都已生成对应的证书(member1.pem、member1-key.pem、member2.pem、member2-key.pem) etcd 成员将组成一个集群，集群中成员之间的所有通信将使用客户端证书进行加密和验证。 etcd的输出将显示其连接的地址使用HTTPS。 启动服务器端从https://discovery.etcd.io/new获取discovery_url作为启动集群的发现服务 发现服务可以在内网环境搭建，详见github地址 1DISCOVERY_URL=$(curl https://discovery.etcd.io/new) member1123456789etcd --name infra1 \--data-dir /var/lib/etcd/infra1 \--peer-client-cert-auth \--peer-trusted-ca-file=/path/to/ca.pem \--peer-cert-file=/path/to/member1.pem \--peer-key-file=/path/to/member1-key.pem \--initial-advertise-peer-urls=https://10.0.1.11:2380 \--listen-peer-urls=https://10.0.1.11:2380 \--discovery $&#123;DISCOVERY_URL&#125; member2123456789etcd --name infra2 \--data-dir /var/lib/etcd/infra2 \--peer-client-cert-auth \--peer-trusted-ca-file=/path/to/ca.pem \--peer-cert-file=/path/to/member2.pem \--peer-key-file=/path/to/member2-key.pem \--initial-advertise-peer-urls=https://10.0.1.12:2380 \--listen-peer-urls=https://10.0.1.12:2380 \--discovery $&#123;DISCOVERY_URL&#125; 示例4、自动自签名对于只需要加密传输数据而不需要身份验证的场景，etcd支持使用自动生成的自签名证书加密传输数据 启动服务器端1DISCOVERY_URL=$(curl https://discovery.etcd.io/new) member11234567etcd --name infra1 \--data-dir /var/lib/etcd/infra1 \--auto-tls \--peer-auto-tls \--initial-advertise-peer-urls=https://10.0.1.11:2380 \--listen-peer-urls=https://10.0.1.11:2380 \--discovery $&#123;DISCOVERY_URL&#125; member21234567etcd --name infra2 \--data-dir /var/lib/etcd/infra2 \--auto-tls \--peer-auto-tls \--initial-advertise-peer-urls=https://10.0.1.12:2380 \--listen-peer-urls=https://10.0.1.12 :2380 \--discovery $&#123;DISCOVERY_URL&#125; 注意由于自签名证书不会进行身份认证，因此curl会返回错误，因此需要添加-k参数禁用证书链检查 etcd维护操作快照条目数量调整--snapshot-count：指定有多少事务（transaction）被提交时，触发截取快照保存到磁盘。从v3.2开始，--snapshot-count的默认值已从10000更改为100000。 注意 此参数具体数值可以通过根据实际情况调整 过低会带来频繁的IO压力，影响集群可用性和写入吞吐量。 过高则导致内存占用过高以及会让etcd的GC变慢 历史数据压缩（针对v3的API）由于etcd保存了key的历史记录，因此能通过MVCC机制获取多版本的数据，需要定期压缩历史记录避免性能下降和空间耗尽。 到达上限阈值时，集群将处于只读和只能删除key的状态，无法写操作。 历史数据压缩只是针对数据的历史版本进行清理，清理之后只能读取到清理点之后的历史版本 手动压缩清理revision为3之前的历史数据 12export ETCDCTL_API=3etcdctl compact 3 清理之后，访问revision3之前的数据会提示不存在 123export ETCDCTL_API=3etcdctl get KEY_NAME --rev=2Error: etcdserver: mvcc: required revision has been compacted 自动压缩启动参数中添加--auto-compaction-retention=1即为每小时压缩一次 碎片整理(针对v3的API)在数据压缩操作之后，旧的revision被压缩，会产生内部碎片，这些内部碎片可以被etcd使用，但是仍消耗磁盘空间。 碎片整理就是将这部分空间释放出来。 12export ETCDCTL_API=3etcdctl defrag 空间配额etcd通过--quota-backend-bytes参数来限制etcd数据库的大小，以字节为单位。 默认是2147483648即2GB，最大值为8589934592即8GB。 容量限制见官方文档 数据备份(针对v3的API)快照备份通过快照etcd集群可以作备份数据的用途。 可以通过快照备份的数据，将etcd集群恢复到快照的时间点。 12export ETCDCTL_API=3etcdctl snapshot save /path/to/snapshot.db 检查快照状态 123456etcd --write-out=table snapshot status /path/to/snapshot.db+----------+----------+------------+------------+| HASH | REVISION | TOTAL KEYS | TOTAL SIZE |+----------+----------+------------+------------+| dd97719a | 24276 | 1113 | 3.0 MB |+----------+----------+------------+------------+ 基于快照的定期备份脚本12345678910111213#!/bin/shTIME=$(date +%Y%m%d)HOUR=$(date +%H)BACKUP_DIR="/data/etcd_backup/$&#123;TIME&#125;"mkdir -p $BACKUP_DIRexport ETCDCTL_API=3/usr/local/bin/etcdctl --cacert=/etc/etcd/ssl/etcd-ca.pem \ --cert=/etc/etcd/ssl/etcd-client.pem \ --key=/etc/etcd/ssl/etcd-client-key.pem \ --endpoints=https://member1:2379,https://member2:2379,https://member3:2379 \ snapshot save $BACKUP_DIR/snapshot-$&#123;HOUR&#125;.db# 清理2天前的etcd备份find /data/etcd_backup -type d -mtime +2 -exec rm -rf &#123;&#125; \; etcd镜像集群(针对v3的API)通过mirror-maker实时做镜像的方式同步数据，如果出现主机房服务挂了可以通过切换域名的形式切换到灾备机房；这个过程中数据是可以保持一致的。 提前部署好两套etcd集群之后，可以在主集群上面运行以下命令 1234567891011121314151617export ETCDCTL_API=3etcdctl make-mirror --no-dest-prefix=true http://mirror1:2379,http://mirror2:2379,http://mirror3:2379# 输出示例48854660466272077883689495010091067112511831241 make-mirror的输出为30s一次，程序为前台运行，可以通过nohup &gt;/path/to/log 2&gt;&amp;1 &amp;的方式扔到后台运行 etcd监控debug endpoint启动参数中添加--debug即可打开debug模式，etcd会在http://x.x.x.x:2379/debug路径下输出debug信息。 由于debug信息很多，会导致性能下降。 /debug/pprof为go语言runtime的endpoint，可以用于分析CPU、heap、mutex和goroutine利用率。 这里示例为使用go命令获取etcd最耗时的 12345678910111213141516171819202122232425go tool pprof http://127.0.0.1:2379/debug/pprof/profileFetching profile over HTTP from http://127.0.0.1:2379/debug/pprof/profileSaved profile in /root/pprof/pprof.etcd-3.2.24.samples.cpu.001.pb.gzFile: etcd-3.2.24Type: cpuTime: Feb 10, 2019 at 9:57pm (CST)Duration: 30s, Total samples = 60ms ( 0.2%)Entering interactive mode (type "help" for commands, "o" for options)(pprof) (pprof) (pprof) top10Showing nodes accounting for 60ms, 100% of 60ms totalShowing top 10 nodes out of 25 flat flat% sum% cum cum% 60ms 100% 100% 60ms 100% runtime.futex 0 0% 100% 10ms 16.67% github.com/coreos/etcd/cmd/vendor/github.com/coreos/etcd/etcdserver.(*raftNode).start.func1 0 0% 100% 10ms 16.67% github.com/coreos/etcd/cmd/vendor/github.com/coreos/etcd/etcdserver.(*raftNode).tick 0 0% 100% 10ms 16.67% github.com/coreos/etcd/cmd/vendor/github.com/coreos/etcd/raft.(*node).Tick 0 0% 100% 20ms 33.33% runtime.chansend 0 0% 100% 30ms 50.00% runtime.exitsyscall 0 0% 100% 30ms 50.00% runtime.exitsyscallfast 0 0% 100% 30ms 50.00% runtime.exitsyscallfast.func1 0 0% 100% 30ms 50.00% runtime.exitsyscallfast_pidle 0 0% 100% 60ms 100% runtime.futexwakeup(pprof) exit metrics endpoint每个etcd节点都会在/metrics路径下输出监控信息，监控软件可以通过此路径获取指标信息 具体的metrics信息可以参看官方文档 --listen-metrics-urls定义metrics的location。 --metrics可以定义basic和extensive 这里通过curl命令来获取metrics信息 1curl http://127.0.0.1:2379/metrics health check这里通过curl命令来获取health信息，返回结果为json 1curl http://127.0.0.1:2379/health 返回结果如下 123&#123; "health": "true"&#125; 对接Prometheus配置文件123456global: scrape_interval: 10sscrape_configs: - job_name: etcd-cluster-monitoring static_configs: - targets: ['10.240.0.32:2379','10.240.0.33:2379','10.240.0.34:2379'] 监控告警使用Alertmanager进行监控告警 Prometheus 1.x 范例 Prometheus 2.x 范例 监控指标展示使用Grafana读取Prometheus的数据展示监控数据，Dashboard模板 etcd故障处理leader节点故障 leader节点故障，etcd集群会自动选举出新的leader。 故障检测模型是基于超时的，因此选举新的leader节点不会在旧的leader节点故障之后立刻发生。 选举leader期间，集群不会处理写入操作。选举期间的写入请求会进入队列等待处理，直至选出新的leader节点。 已经发送给故障leader但尚未提交的数据可能会丢失。这是因为新的leader节点有权对旧leader节点的数据进行修改 客户端会发现一些写入请求可能会超时，没有提交的数据会丢失。 follower节点故障 follower故障节点数量少于集群节点的一半时，etcd集群是可以正常工作的。 例如3个节点故障了1个，5个节点故障了2个 follower节点故障后，客户端的etcd库应该自动连接到etcd集群的其他成员。 超过半数节点故障 由于Raft算法的原理所限，超过半数的集群节点故障会导致etcd集群进入不可写入的状态。 只要正常工作的节点超过集群节点的一半，那么etcd集群会自动选举leader节点并且自动恢复到健康状态 如果无法修复多数节点，那么就需要走灾难恢复的操作流程 网络分区 由于网络故障，导致etcd集群被切分成两个或者更多的部分。 那么占有多数节点的一方会成为可用集群，少数节点的一方不可写入。 如果对等切分了集群，那么每个部分都不可用。 这是因为Raft一致性算法保证了etcd是不存在脑裂现象。 只要网络分区的故障解除，少数节点的一方会自动从多数节点一方识别出leader节点，然后恢复状态。 集群启动失败只有超过半数成员启动完成之后，集群的bootstrap才会成功。 Raft一致性算法保证了集群节点的数据一致性和稳定性，因此对于节点的恢复，更多的是恢复etcd节点服务，然后恢复数据 新的集群可以删除所有成员的数据目录，然后重新走创建集群的步骤 已有集群这个就要看无法启动的节点是数据文件损坏，还是其他原因导致的。 这里以数据文件损坏为例。 寻找正常的节点，使用etcdctl snapshot save命令保存出快照文件 将故障节点的数据目录清空，使用etcdctl snapshot restore命令将数据恢复到数据目录 使用etcdctl member list确认故障节点的信息 使用etcdctl member remove删除故障节点 使用etcdctl member add MEMBER_NAME --peer-urls=http://member:2379重新添加成员 修改etcd启动参数--initial-cluster-state=existing启动故障节点的etcd服务 etcd灾难恢复这里的灾难恢复，只能恢复v2或者v3的数据，不能同时恢复v2和v3。 两套API是相互隔离的。 针对v3的APIetcd v3的API提供了快照和恢复功能，可以在不损失快照点数据的情况下重建集群 快照备份数据1ETCDCTL_API=3 etcdctl --endpoints http://member1:2379,http://member2:2379,http://member3:2379 snapshot save /path/to/snapshot.db 恢复集群 恢复etcd集群，只需要快照文件db即可。 使用etcdctl snapshot restore命令还原数据时会自动创建新的etcd数据目录。 恢复过程会覆盖快照文件里面的一些metadata（特别是member id和cluster id），该member会失去之前的id。覆盖metadata可以防止新成员无意中加入现有集群。 从快照中恢复集群，必须以新集群启动。 恢复时可以选择验证快照完整性hash。 使用etcdctl snapshot save生成的快照，则具有完整性hash 如果是直接从数据目录拷贝数据快照，则没有完整性hash，需要使用--skip-hash-check跳过检查 恢复节点数据 这里假定原有的集群节点为member1、member2、member3 在member1、member2、member3上分别恢复快照数据 123456789101112131415$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \ --name member1 \ --initial-cluster member1=http://member1:2380,member2=http://member2:2380,member3=http://member3:2380 \ --initial-cluster-token etcd-cluster-1 \ --initial-advertise-peer-urls http://member1:2380$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \ --name member2 \ --initial-cluster member1=http://member1:2380,member2=http://member2:2380,member3=http://member3:2380 \ --initial-cluster-token etcd-cluster-1 \ --initial-advertise-peer-urls http://member2:2380$ ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \ --name member3 \ --initial-cluster member1=http://member1:2380,member2=http://member2:2380,member3=http://member3:2380 \ --initial-cluster-token etcd-cluster-1 \ --initial-advertise-peer-urls http://member3:2380 启动etcd集群在member1、member2、member3上分别启动集群 123456789101112131415$ etcd \ --name member1 \ --listen-client-urls http://member1:2379 \ --advertise-client-urls http://member1:2379 \ --listen-peer-urls http://member1:2380 &amp;$ etcd \ --name member2 \ --listen-client-urls http://member2:2379 \ --advertise-client-urls http://member2:2379 \ --listen-peer-urls http://member2:2380 &amp;$ etcd \ --name member3 \ --listen-client-urls http://member3:2379 \ --advertise-client-urls http://member3:2379 \ --listen-peer-urls http://member3:2380 &amp; 针对v2的API备份数据123456ETCDCTL_API=3etcdctl backup \ --data-dir /path/to/data-dir \ --wal-dir /path/to/wal_dir \ --backup-dir /path/to/backup_data_dir \ --backup-wal-dir /path/to/backup_wal_dir 清理数据目录12rm -rf /path/to/data-dirrm -rf /path/to/wal-dir 恢复数据12mv /path/to/backup_data_dir /path/to/data-dirmv /path/to/backup_wal_dir /path/to/wal_dir 启动etcd集群启动参数需要添加--force-new-cluster 123etcd --data-dir /path/to/data-dir \ --wal-dir /path/to/wal_dir \ --force-new-cluster etcd版本升级这里可以参考etcd的升级文档 etcd的FAQ摘自Frequently Asked Questions (FAQ) 客户端是否需要向etcd集群的leader节点发送请求 leader节点负责处理所有需要集群共识的请求（例如写请求）。 客户端不需要知道哪个节点是leader，follower节点会将所有需要集群共识的请求转发给leader节点。 所有节点都可以处理不需要集群共识的请求（例如序列化读取）。 listen-client-urls、listen-peer-urls、advertise-client-urls、initial-advertise-peer-urls的区别 listen-client-urls和listen-peer-urls指定etcd服务端用于接收传入连接的本地地址，要监听所有地址，请指定0.0.0.0作为监听地址。 advertise-client-urls 和initial-advertise-peer-urls指定etcd的客户端及集群其他成员访问etcd服务的地址，此地址必须要被外部访问，因此不能设置127.0.0.1或者0.0.0.0等地址。 为什么不能通过更改listen-peer-urls或者initial-advertise-peer-urls来更新etcdctl member list中列出的advertise peer urls 每个member的advertise-peer-urls来自初始化集群时的initial-advertise-peer-urls参数 在member启动完成后修改listen-peer-urls或者initial-advertise-peer-urls参数不会影响现有的advertise-peer-urls，因为修改此参数需要通过集群仲裁以避免出现脑裂 修改advertise-peer-url请使用etcd member update命令操作 系统要求 etcd会将数据写入磁盘，因此高性能的磁盘会更好，推荐使用SSD 默认存储配额为2GB，最大值为8GB 为了避免使用swap或者内存不足，服务器内存至少要超过存储配额 为什么etcd需要奇数个集群成员 etcd集群需要通过大多数节点仲裁才能将集群状态更新到一致 仲裁为(n/2)+1 双数个集群成员并不比奇数个节点容错性强 集群容错性列表 Cluster Size Majority Failure Tolerance 1 1 0 2 2 0 3 2 1 4 3 1 5 3 2 6 4 2 7 4 3 8 5 3 9 5 4 集群最大节点数量 理论上没有硬性限制，一般不超过7个节点 建议5个节点，5个节点可以容忍2个节点故障下线，在大多数情况下已经足够 更多的节点可以提供更好的可用性，但是写入性能会有影响 部署跨数据中心的etcd集群是否合适 跨数据中心的etcd集群可以提高可用性 数据中心之间的网络延迟可能会影响节点的election 默认的etcd配置可能会因为网络延迟频繁选举或者心跳超时，需要调整对应的参数 为什么etcd会因为磁盘IO延迟而重新选举 这是故意设计的 磁盘IO延迟是leader节点存活指标的一部分 磁盘IO延迟很高导致选举超时，即使leader节点在选举间隔内能处理网络信息（例如发送心跳），但它实际上是不可用的，因为它无法及时提交新的提议 如果经常出现因磁盘IO延迟而重新选举，请关注一下磁盘或者修改etcd时间参数 etcd性能压测这里参考官方文档 性能指标 延迟 完成操作所需的时间 吞吐量 一段时间内完成的总操作数量 通常情况下，平均延迟会随着吞吐量的增加而增加。 etcd使用Raft一致性算法完成成员之间的数据同步并达成集群共识。 集群的共识性能，尤其是提交延迟，主要受到两个方面限制。 网络IO延迟 磁盘IO延迟 提交延迟的构成 成员之间的网络往返时间RTT 同一个数据中心内部的RTT是ms级别 跨数据中心的RTT就需要考虑物理限制和网络质量 fdatasync数据落盘时间 机械硬盘fdatasync延迟通常在10ms左右 固态硬盘则低于1ms 其他延迟构成 序列化etcd请求需要通过etcd后端boltdb的MVVC机制来完成，通常会在10ms完成。 etcd定期将最近提交的请求快照，然后跟磁盘上的快照合并，这个操作过程会导致延迟出现峰值。 正在进行的数据压缩也会影响到延迟，所以要跟业务错开 benchmark跑分etcd自带的benchmark命令行工具可以用来测试etcd性能 写入请求123456789101112131415161718192021222324# 假定 HOST_1 是 leader, 写入请求发到 leaderbenchmark --endpoints=$&#123;HOST_1&#125; \ --conns=1 \ --clients=1 \ put --key-size=8 \ --sequential-keys \ --total=10000 \ --val-size=256benchmark --endpoints=$&#123;HOST_1&#125; \ --conns=100 \ --clients=1000 \ put --key-size=8 \ --sequential-keys \ --total=100000 \ --val-size=256 # 写入发到所有成员benchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \ --conns=100 \ --clients=1000 \ put --key-size=8 \ --sequential-keys \ --total=100000 \ --val-size=256 序列化读取123456789101112131415161718192021222324252627# Single connection read requestsbenchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \ --conns=1 \ --clients=1 \ range YOUR_KEY \ --consistency=l \ --total=10000benchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \ --conns=1 \ --clients=1 \ range YOUR_KEY \ --consistency=s \ --total=10000# Many concurrent read requestsbenchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \ --conns=100 \ --clients=1000 \ range YOUR_KEY \ --consistency=l \ --total=100000benchmark --endpoints=$&#123;HOST_1&#125;,$&#123;HOST_2&#125;,$&#123;HOST_3&#125; \ --conns=100 \ --clients=1000 \ range YOUR_KEY \ --consistency=s \ --total=100000 etcd性能调优参考官方文档 etcd默认配置是基于同一个数据中心，网络延迟较低的情况。 对于网络延迟较高，那么就需要优化心跳间隔和选举超时时间 时间参数（time parameter）延迟不止有网络延迟，还可能受到节点磁盘IO影响。 每一次超时设置应该包括请求发出到响应成功的时间。 心跳间隔（heartbeat interval）leader节点通知各follower节点自己的存活信息。 最佳实践是通过ping命令获取RTT最大值，然后设置为RTT的0.5~1.5倍。 默认是100ms。 选举超时（election timeout）follower节点在多久之后没收到leader节点的心跳信息，就开始选举新leader节点。 默认是1000ms。选举超时应该设置为至少是RTT的10倍，以避免网络出现波动导致重新选举。 快照（snapshot）etcd会将所有变更的key追加写入到wal日志文件中。 一行记录一个key的变更，因此日志会不断增长。 为避免日志过大，etcd会定期做快照。 快照操作会保存当前系统状态并移除旧的日志。 snapshot-count参数控制快照的频率，默认是10000，即每10000次变更会触发一次快照操作。 如果内存使用率高并且磁盘使用率高，可以尝试调低这个参数。 磁盘etcd集群对磁盘IO延迟非常的敏感。 etcd需要存储变更日志、快照等操作，可能会导致磁盘IO出现很高的fsync延迟。 磁盘IO延迟高会导致leader节点心跳信息超时、请求超时、重新选举等。 etcd所使用的磁盘与系统盘分开 data目录和wal目录分开 有条件推荐使用SSD固态硬盘 使用ionice调高etcd进程的IO优先级（这个针对etcd数据目录在系统盘的情况） 12&gt; ionice -c2 -n0 -p `pgrep etcd`&gt; 网络如果leader节点接收来自客户端的大量请求，无法及时处理follower的请求，那么follower节点处理的请求也会因此出现延迟。 具体表现为follower会提示sending bufer is full。 可以通过调高leader的网络优先级或者通过流量管控机制来提高对follower的请求响应。]]></content>
      <tags>
        <tag>kubernetes</tag>
        <tag>etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7使用社区YUM源安装Mariadb Galera集群]]></title>
    <url>%2F2018%2F12%2F11%2FCentOS7%E4%BD%BF%E7%94%A8%E7%A4%BE%E5%8C%BAYUM%E6%BA%90%E5%AE%89%E8%A3%85Mariadb-Galera%E9%9B%86%E7%BE%A4.html</url>
    <content type="text"><![CDATA[简介 本文以MariaDB官方文档为基础，记录操作步骤 安装Mariadb数据库前的准备工作准备虚拟机三台 IP地址 主机名 CPU 内存 172.16.10.101 db1 2 3G 172.16.10.102 db2 2 3G 172.16.10.103 db3 2 3G 添加Mariadb官方YUM源，下面以Mariadb 10.1为例官方YUM源编辑器 使用以下命令快速添加YUM源 1234567tee /etc/yum.repos.d/mariadb.repo &lt;&lt;-'EOF'[mariadb]name = MariaDBbaseurl = https://mirrors.ustc.edu.cn/mariadb/yum/10.1/centos7-amd64gpgkey=https://mirrors.ustc.edu.cn/mariadb/yum/RPM-GPG-KEY-MariaDBgpgcheck=1 EOF 刷新YUM缓存1yum makecache 查看Mariadb相关的安装包，注意软件包版本和对应的YUM源名字1yum list MariaDB* galera 关闭firewalld防火墙1systemctl disable firewalld --now 设置主机名（设置三台虚拟机主机名分别为db1，db2，db3）123hostnamectl set-hostname db1hostnamectl set-hostname db2hostnamectl set-hostname db3 编辑/etc/hosts文件12345cat /etc/hosts &lt;&lt;EOF172.16.10.101 db1172.16.10.102 db2172.16.10.103 db3EOF 关闭SELINUX123setenforce 0 sed -i 's,^SELINUX=enforcing,SELINUX=disabled,g' /etc/selinux/config 部署MariaDB Galera集群安装相关软件包1yum install MariaDB-server MariaDB-client MariaDB-client 启用xtrabackup-v2功能 需要额外安装percona提供的软件包 1yum install https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.10/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.10-1.el7.x86_64.rpm 启动MariaDB数据库 在db1上启动MariaDB数据库，设置galera集群同步账号,进行安全初始化 1234systemctl start mariadb.servicemysql -uroot -e "grant all privileges on *.* to 'sst'@'localhost' identified by 'password';" mysql_secure_installation systemctl stop mariadb.service 编辑MariaDB配置文件 在三个节点上编辑MariaDB配置文件，以开启galera集群功能 123456789101112131415161718192021222324252627282930313233343536373839404142cat /etc/my.cnf.d/galera.cnf[server][mysqld]# 监听哪个地址，这里每个节点填对应的ip地址bind-address=172.16.10.101 # 监听哪个端口port = 3306 # 设置默认字符编码集collation-server = utf8_general_ciinit-connect = SET NAMES utf8character-set-server = utf8# 设置日志路径log-error = /var/log/mariadb/mariadb.log# 设置binloglog-bin = mysql-binbinlog_format=ROW# 设置默认数据目录datadir = /var/lib/mysql/ # 设置默认存储引擎default-storage-engine=innodbinnodb_autoinc_lock_mode=2 [galera]wsrep_on=ONwsrep_provider=/usr/lib64/galera/libgalera_smm.so# galera集群名字wsrep_cluster_name="galera_cluster" # 本节点的主机名，这里每个节点填对应的ip地址wsrep_node_name="db1" wsrep_cluster_address = "gcomm://172.16.10.101:4567,172.16.10.102:4567,172.16.10.103:4567"wsrep_provider_options = "gmcast.listen_addr=tcp://172.16.10.101:4567;ist.recv_addr=172.16.10.101:4568" wsrep_node_address="172.16.10.101:4567" # 设置galera集群同步的方法和用户名密码wsrep_sst_auth=sst:passwordwsrep_sst_method=xtrabackup-v2max_connections = 10000 key_buffer_size = 64Mmax_heap_table_size = 64Mtmp_table_size = 64Minnodb_buffer_pool_size = 128M[embedded][mariadb][mariadb-10.1] 启动galera集群创建集群 在db1上运行galera_new_cluster命令 1galera_new_cluster 查看集群状态 在db1上查看集群状态 123456# mysql -uroot -p -e "show status like 'wsrep_cluster_size';"+--------------------------+--------------------------------------+| Variable_name | Value |+--------------------------+--------------------------------------+| wsrep_cluster_size | 1 |+--------------------------+--------------------------------------+ 监控MariaDB日志 监控db1上的MariaDB日志 在启动其他节点的时候，能看到其他节点加入到galera集群 1tail -f /var/log/mariadb/mariadb.log 启动其他节点数据库 在db2和db3上运行MariaDB数据库 1systemctl start mariadb 检查集群状态 在db1上检查集群状态 123456mysql -uroot -p -e "show status like 'wsrep_cluster_size';"+--------------------------+--------------------------------------+| Variable_name | Value |+--------------------------+--------------------------------------+| wsrep_cluster_size | 3 |+--------------------------+--------------------------------------+ 验证MariaDB galera集群的同步功能是否正常在db1上创建用户、数据库123mysql -uroot -p -e "user add testuser;"mysql -uroot -p -e "create database testdb;"mysql -uroot -p -e "grant all privileges on testdb.* to 'testuser'@'localhost' identified by 'password';" 在db2上检查用户、数据库是否存在12mysql -uroot -p -e "select user,host from mysql.user;"mysql -uroot -p -e "show databases;" 在db3上删除用户和数据库12mysql -uroot -p -e "delete user 'testuser'"mysql -uroot -p -e "drop database testdb" 在db1上检查用户和数据库是否还在12mysql -uroot -p -e "select user,host from mysql.user;"mysql -uroot -p -e "show databases;" 至此，MariaDB galera集群已经部署完成]]></content>
      <tags>
        <tag>Mariadb/MySQL</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker打包shadowsocks-libev镜像]]></title>
    <url>%2F2018%2F12%2F11%2F%E4%BD%BF%E7%94%A8Docker%E6%89%93%E5%8C%85shadowsocks-libev%E9%95%9C%E5%83%8F.html</url>
    <content type="text"><![CDATA[介绍记录一下使用Dockerfile制作shadowsocks-libev镜像的过程 基于Alpine-3.8和shadowsocks-libev-v3.2.3制作 参考shadowsocks-libev项目上面的Dockerfile 以下是Dockerfile内容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# README# /* BUILD IMAGE */# dokcer image build -t shadowsocks-libev:v3.2.3 .# /* RUN CONATIANER */# docker container run -d -e SERVER_PORT=111 -e PASSWORD='password' -e METHOD='aes-256-gcm' --net host --name ss-libev-port111 shadowsocks-libev:v3.2-alpine3.8# /* SS-SERVER HELP */# shadowsocks-libev 3.2.3# # maintained by Max Lv &lt;max.c.lv@gmail.com&gt; and Linus Yang &lt;laokongzi@gmail.com&gt;# # usage:# # ss-server# # -s &lt;server_host&gt; Host name or IP address of your remote server.# -p &lt;server_port&gt; Port number of your remote server.# -l &lt;local_port&gt; Port number of your local server.# -k &lt;password&gt; Password of your remote server.# -m &lt;encrypt_method&gt; Encrypt method: rc4-md5, # aes-128-gcm, aes-192-gcm, aes-256-gcm,# aes-128-cfb, aes-192-cfb, aes-256-cfb,# aes-128-ctr, aes-192-ctr, aes-256-ctr,# camellia-128-cfb, camellia-192-cfb,# camellia-256-cfb, bf-cfb,# chacha20-ietf-poly1305,# xchacha20-ietf-poly1305,# salsa20, chacha20 and chacha20-ietf.# The default cipher is chacha20-ietf-poly1305.# # [-a &lt;user&gt;] Run as another user.# [-f &lt;pid_file&gt;] The file path to store pid.# [-t &lt;timeout&gt;] Socket timeout in seconds.# [-c &lt;config_file&gt;] The path to config file.# [-n &lt;number&gt;] Max number of open files.# [-i &lt;interface&gt;] Network interface to bind.# [-b &lt;local_address&gt;] Local address to bind.# # [-u] Enable UDP relay.# [-U] Enable UDP relay and disable TCP relay.# [-6] Resovle hostname to IPv6 address first.# # [-d &lt;addr&gt;] Name servers for internal DNS resolver.# [--reuse-port] Enable port reuse.# [--fast-open] Enable TCP fast open.# with Linux kernel &gt; 3.7.0.# [--acl &lt;acl_file&gt;] Path to ACL (Access Control List).# [--manager-address &lt;addr&gt;] UNIX domain socket address.# [--mtu &lt;MTU&gt;] MTU of your network interface.# [--mptcp] Enable Multipath TCP on MPTCP Kernel.# [--no-delay] Enable TCP_NODELAY.# [--key &lt;key_in_base64&gt;] Key of your remote server.# [--plugin &lt;name&gt;] Enable SIP003 plugin. (Experimental)# [--plugin-opts &lt;options&gt;] Set SIP003 plugin options. (Experimental)# # [-v] Verbose mode.# [-h, --help] Print this message.FROM alpine:3.8ENV TZ 'Asia/Shanghai'ENV SS_VERSION 3.2.3ENV SS_DOWNLOAD_URL https://github.com/shadowsocks/shadowsocks-libev/releases/download/v$&#123;SS_VERSION&#125;/shadowsocks-libev-$&#123;SS_VERSION&#125;.tar.gzRUN apk upgrade \ &amp;&amp; apk add bash tzdata libsodium rng-tools \ &amp;&amp; apk add --virtual .build-deps \ autoconf \ automake \ xmlto \ build-base \ curl \ c-ares-dev \ libev-dev \ libtool \ linux-headers \ udns-dev \ libsodium-dev \ mbedtls-dev \ pcre-dev \ udns-dev \ tar \ git \ &amp;&amp; wget -q -O - $SS_DOWNLOAD_URL | tar xz \ &amp;&amp; (cd shadowsocks-libev-$&#123;SS_VERSION&#125; \ &amp;&amp; ./configure --prefix=/usr --disable-documentation \ &amp;&amp; make install) \ &amp;&amp; ln -sf /usr/share/zoneinfo/$TZ /etc/localtime \ &amp;&amp; echo $TZ &gt; /etc/timezone \ &amp;&amp; apk del .build-deps \ &amp;&amp; rm -rf shadowsocks-libev-$&#123;SS_VERSION&#125; \ /var/cache/apk/* \ &amp;&amp; apk add --no-cache \ rng-tools \ $(scanelf --needed --nobanner /usr/bin/ss-* \ | awk '&#123; gsub(/,/, "\nso:", $2); print "so:" $2 &#125;' \ | sort -u) CMD ["/usr/bin/ss-server"]]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【不定时更新】二进制部署 kubernetes v1.11.x 高可用集群]]></title>
    <url>%2F2018%2F12%2F08%2F%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2%20kubernetes%20v1.11.x%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4.html</url>
    <content type="text"><![CDATA[更新记录 2019年1月7日添加基于ingress-nginx使用域名+HTTPS的方式访问kubernetes-Dashboard 2019年1月2日添加RBAC规则，修复kube-apiserver无法访问kubelet的问题 2019年1月1日调整master节点和worker节点的操作步骤，添加CoreDNS的configmap中的hosts静态解析 2018年12月28日修改kube-prometheus部分，修复Prometheus的Targets无法发现的问题 2018年12月26日修改kubernetes-dashboard链接指向 2018年12月25日修改kubele.config.file路径问题 2018年12月18日修改kubelet和kube-proxy启动时加载config file 2018年12月17日添加EFK部署内容 2018年12月16日添加prometheus-operator部署内容 2018年12月14日添加helm部署内容，拆分etcd的server证书和client证书 2018年12月13日添加rook-ceph部署内容 2018年12月12日添加Metrics-Server内容 2018年12月11日添加Dashboard、Ingress内容 2018年12月10日添加kube-flannel、calico、CoreDNS内容 2018年12月9日分拆master节点和work节点的内容 2018年12月8日初稿 介绍本次部署方式为二进制可执行文件的方式部署 注意请根据自己的实际情况调整 对于生产环境部署，请注意某些参数的选择 如无特殊说明，均在k8s-m1节点上执行 参考博文感谢两位大佬的文章，这里整合一下两位大佬的内容，结合自己的理解整理本文 【漠然】Kubernetes 1.10.1 集群搭建 【漠然】使用 Bootstrap Token 完成 TLS Bootstrapping 【张馆长】二进制部署Kubernetes v1.11.x(1.12.x) HA可选 软件版本 kubernetes v1.11.5 【下载链接需要爬墙，自行解决】&gt; docker-ce 18.03 cni-plugin v0.7.4 etcd v3.3.10 网络信息 基于CNI的模式实现容器网络 Cluster IP CIDR: 10.244.0.0/16 Service Cluster IP CIDR: 10.96.0.0/12 Service DNS IP: 10.96.0.10 Kubernetes API VIP: 172.16.80.200 节点信息 操作系统可采用 Ubuntu Server 16.04+ 和 CentOS 7.4+，本文使用CentOS 7.6 (1810) Minimal 由keepalived提供VIP 由haproxy提供kube-apiserver四层负载均衡 由于实验环境受限，以3台服务器同时作为master和worker节点运行 服务器配置请根据实际情况适当调整 IP地址 主机名 角色 CPU 内存 172.16.80.201 k8s-m1 master+worker 4 8G 172.16.80.202 k8s-m2 master+worker 4 8G 172.16.80.203 k8s-m3 master+worker 4 8G 目录说明 /usr/local/bin/：存放kubernetes和etcd二进制文件 /opt/cni/bin/： 存放cni-plugin二进制文件 /etc/etcd/：存放etcd配置文件和SSL证书 /etc/kubernetes/：存放kubernetes配置和SSL证书 /etc/cni/net.d/：安装CNI插件后会在这里生成配置文件 $HOME/.kube/：kubectl命令会在家目录下建立此目录，用于保存访问kubernetes集群的配置和缓存 $HOME/.helm/：helm命令会建立此目录，用于保存helm缓存和repository信息 事前准备 事情准备在所有服务器上都需要完成 部署过程以root用户完成 所有服务器网络互通，k8s-m1可以通过SSH证书免密登录到其他master节点，用于分发文件 编辑/etc/hosts 1234567cat &gt; /etc/hosts &lt;&lt;EOF127.0.0.1 localhost172.16.80.200 k8s-vip172.16.80.201 k8s-m1172.16.80.202 k8s-m2172.16.80.203 k8s-m3EOF 时间同步服务 集群系统需要各节点时间同步 参考链接：RHEL7官方文档 这里使用公网对时，如果需要内网对时，请自行配置 123yum install -y chronysystemctl enable chronydsystemctl start chronyd 关闭firewalld和SELINUX（可根据实际情况自行决定关闭不需要的服务） 12345678systemctl stop firewalldsystemctl disable firewalldsystemctl mask firewalld# 清空iptables规则iptables -Ziptables -P INPUT ACCEPTiptables -P FORWARD ACCEPTiptables -P OUTPUT ACCEPT 12setenforce 0sed -ri '/^[^#]*SELINUX=/s#=.+$#=disabled#' /etc/selinux/config 禁用swap 12swapoff -a &amp;&amp; sysctl -w vm.swappiness=0sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab 添加sysctl参数 123456789101112131415161718192021222324252627282930cat &gt; /etc/sysctl.d/centos.conf &lt;&lt;EOF # 最大文件句柄数fs.file-max=1024000# 在CentOS7.4引入了一个新的参数来控制内核的行为。 # /proc/sys/fs/may_detach_mounts 默认设置为0# 当系统有容器运行的时候，需要将该值设置为1。fs.may_detach_mounts = 1# 最大文件打开数fs.nr_open=1024000# 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤net.bridge.bridge-nf-call-arptables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# 关闭严格校验数据包的反向路径net.ipv4.conf.default.rp_filter=0net.ipv4.conf.all.rp_filter=0# 打开ipv4数据包转发net.ipv4.ip_forward=1# 允许应用程序能够绑定到不属于本地网卡的地址net.ipv4.ip_nonlocal_bind=1 # 表示最大限度使用物理内存，然后才是swap空间vm.swappiness = 0 # 设置系统TCP连接keepalive的持续时间，默认7200net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10EOF# 让sysctl参数生效sysctl --system 确保操作系统已经最新 1yum update -y 安装软件包 123yum groups install base -yyum install epel-release bash-completion-extras -yyum install git vim ipvsadm tree dstat iotop htop socat ipset conntrack -y 加载ipvs模块 12345678910111213# 开机自动加载ipvs模块cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4"for kernel_module in \$&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe \$&#123;kernel_module&#125; fidoneEOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep ip_vs 安装docker-ce 18.03 12345yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine -yyum install -y yum-utils device-mapper-persistent-data lvm2 -yyum-config-manager --add-repo http://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.reposed -e 's,download.docker.com,mirrors.aliyun.com/docker-ce,g' -i /etc/yum.repos.d/docker-ce.repoyum install docker-ce-18.03.1.ce -y 创建docker配置文件 12345678910111213mkdir -p /etc/dockercat&gt;/etc/docker/daemon.json&lt;&lt;EOF&#123; "registry-mirrors": ["https://registry.docker-cn.com"], "insecure-registries": [], "log-driver": "json-file", "log-opts": &#123; "max-size": "100m", "max-file": "3" &#125;, "max-concurrent-downloads": 10&#125;EOF 配置docker命令补全 12cp /usr/share/bash-completion/completions/docker /etc/bash_completion.d/source /etc/bash_completion.d/docker 配置docker服务开机自启动 12systemctl enable docker.servicesystemctl start docker.service 查看docker信息 1docker info 禁用docker源 12# 为避免yum update时更新docker，将docker源禁用sed -e 's,enabled=1,enabled=0,g' -i /etc/yum.repos.d/docker-ce.repo 确保以最新的内核启动系统 1reboot 定义集群变量 注意 这里的变量只对当前会话生效，如果会话断开或者重启服务器，都需要重新定义变量 HostArray定义集群中所有节点的主机名和IP MasterArray定义master节点的主机名和IP WorkerArray定义worker节点的主机名和IP，这里master和worker都在一起，所以MasterArray和WorkerArray一样 VIP_IFACE定义keepalived的VIP绑定在哪一个网卡 ETCD_SERVERS以MasterArray的信息生成etcd集群服务器列表 ETCD_INITIAL_CLUSTER以MasterArray信息生成etcd集群初始化列表 POD_DNS_SERVER_IP定义Pod的DNS服务器IP地址 12345678910111213141516171819202122232425262728293031323334declare -A HostArray MasterArray WorkerArray# 声明所有节点的信息HostArray=(['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203)# 如果节点多，可以按照下面的方式声明Array# HostArray=(['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203 ['k8s-n1']=172.16.80.204 ['k8s-n2']=172.16.80.205)# 声明master节点信息MasterArray=(['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203)# 声明worker节点信息WorkerArray=(['k8s-m1']=172.16.80.201 ['k8s-m2']=172.16.80.202 ['k8s-m3']=172.16.80.203)# VIP="172.16.80.200"KUBE_APISERVER="https://172.16.80.200:8443"# etcd版本号# kubeadm-v1.11.5里面使用的是v3.2.18，这里直接上到最新的v3.3.10ETCD_VERSION="v3.3.10"# kubernetes版本号KUBERNETES_VERSION="v1.11.5"# cni-plugin版本号# kubernetes YUM源里用的还是v0.6.0版，这里上到最新的v0.7.4CNI_PLUGIN_VERSION="v0.7.4"# 声明VIP所在的网卡名称，以ens33为例VIP_IFACE="ens33"# 声明etcd_serverETCD_SERVERS=$( xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#^#https://#;s#$#:2379#;$s#\n##' | paste -d, -s - )ETCD_INITIAL_CLUSTER=$( for i in $&#123;!MasterArray[@]&#125;;do echo $i=https://$&#123;MasterArray[$i]&#125;:2380; done | sort | paste -d, -s - )# 定义POD_CLUSTER_CIDRPOD_NET_CIDR="10.244.0.0/16"# 定义SVC_CLUSTER_CIDRSVC_CLUSTER_CIDR="10.96.0.0/12"# 定义POD_DNS_SERVER_IPPOD_DNS_SERVER_IP="10.96.0.10" 下载所需软件包 创建工作目录 12mkdir -p /root/softwarecd /root/software 二进制文件需要分发到master和worker节点 1234567891011121314151617181920212223242526272829303132333435# 下载kubernetes二进制包echo "--- 下载kubernetes $&#123;KUBERNETES_VERSION&#125; 二进制包 ---"wget https://dl.k8s.io/$&#123;KUBERNETES_VERSION&#125;/kubernetes-server-linux-amd64.tar.gztar xzf kubernetes-server-linux-amd64.tar.gz \ kubernetes/server/bin/hyperkube \ kubernetes/server/bin/kube-controller-manager \ kubernetes/server/bin/kubectl \ kubernetes/server/bin/apiextensions-apiserver \ kubernetes/server/bin/kube-proxy \ kubernetes/server/bin/kube-apiserver \ kubernetes/server/bin/kubelet \ kubernetes/server/bin/kubeadm \ kubernetes/server/bin/kube-aggregator \ kubernetes/server/bin/kube-scheduler \ kubernetes/server/bin/cloud-controller-manager \ kubernetes/server/bin/mounterchown -R root:root kubernetes/server/bin/*chmod 0755 kubernetes/server/bin/*# 这里需要先拷贝kubectl到/usr/local/bin目录下，用于生成kubeconfig文件rsync -avpt kubernetes/server/bin/kubectl /usr/local/bin/kubectl# 下载etcd二进制包echo "--- 下载etcd $&#123;ETCD_VERSION&#125; 二进制包 ---"wget https://github.com/etcd-io/etcd/releases/download/$&#123;ETCD_VERSION&#125;/etcd-$&#123;ETCD_VERSION&#125;-linux-amd64.tar.gztar xzf etcd-$&#123;ETCD_VERSION&#125;-linux-amd64.tar.gz \ etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl \ etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdchown root:root etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdchmod 0755 etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcd# 下载CNI-pluginecho "--- 下载cni-plugins $&#123;CNI_PLUGIN_VERSION&#125; 二进制包 ---"wget https://github.com/containernetworking/plugins/releases/download/$&#123;CNI_PLUGIN_VERSION&#125;/cni-plugins-amd64-$&#123;CNI_PLUGIN_VERSION&#125;.tgzmkdir /root/software/cni-pluginstar xzf cni-plugins-amd64-$&#123;CNI_PLUGIN_VERSION&#125;.tgz -C /root/software/cni-plugins/ 生成集群Key和Certificates说明本次部署，需要为etcd-server、etcd-client、kube-apiserver、kube-controller-manager、kube-scheduler、kube-proxy生成证书。另外还需要生成sa、front-proxy-ca、front-proxy-client证书用于集群的其他功能。 要注意CA JSON文件的CN(Common Name)与O(Organization)等内容是会影响Kubernetes组件认证的。 CN Common Name，kube-apiserver会从证书中提取该字段作为请求的用户名（User Name） O Oragnization，kube-apiserver会从证书中提取该字段作为请求用户的所属组（Group） CA是自签名根证书，用来给后续各种证书签名 kubernetes集群的所有状态信息都保存在etcd中，kubernetes组件会通过kube-apiserver读写etcd里面的信息 etcd如果暴露在公网且没做SSL/TLS验证，那么任何人都能读写数据，那么很可能会无端端在kubernetes集群里面多了挖坑Pod或者肉鸡Pod 本文使用CFSSL创建证书，证书有效期10年 建立证书过程在k8s-m1上完成 下载CFSSL工具123456wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/local/bin/cfssl-certinfowget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljsonchmod 755 /usr/local/bin/cfssl-certinfo \ /usr/local/bin/cfssl \ /usr/local/bin/cfssljson 创建工作目录12mkdir -p /root/pki /root/master /root/workercd /root/pki 创建用于生成证书的json文件ca-config.json12345678910111213141516171819202122232425262728293031323334353637cat &gt; ca-config.json &lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125;, "etcd-server": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125;, "etcd-client": &#123; "usages": [ "signing", "key encipherment", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125;EOF ca-csr.json123456789101112131415161718cat &gt; ca-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "Kubernetes", "OU": "System" &#125; ]&#125;EOF etcd-ca-csr.json123456789101112131415161718cat &gt; etcd-ca-csr.json &lt;&lt;EOF&#123; "CN": "etcd", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "etcd", "OU": "Etcd Security" &#125; ]&#125;EOF etcd-server-csr.json123456789101112131415161718cat &gt; etcd-server-csr.json &lt;&lt;EOF&#123; "CN": "etcd-server", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "etcd", "OU": "Etcd Security" &#125; ]&#125;EOF etcd-client-csr.json123456789101112131415161718192021cat &gt; etcd-client-csr.json &lt;&lt;EOF&#123; "CN": "etcd-client", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "hosts": [ "" ], "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "etcd", "OU": "Etcd Security" &#125; ]&#125;EOF kube-apiserver-csr.json123456789101112131415161718cat &gt; kube-apiserver-csr.json &lt;&lt;EOF&#123; "CN": "kube-apiserver", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "Kubernetes", "OU": "System" &#125; ]&#125;EOF kube-manager-csr.json123456789101112131415161718cat &gt; kube-manager-csr.json &lt;&lt;EOF&#123; "CN": "system:kube-controller-manager", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "system:kube-controller-manager", "OU": "System" &#125; ]&#125;EOF kube-scheduler-csr.json123456789101112131415161718cat &gt; kube-scheduler-csr.json &lt;&lt;EOF&#123; "CN": "system:kube-scheduler", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "system:kube-scheduler", "OU": "System" &#125; ]&#125;EOF kube-proxy-csr.json123456789101112131415161718cat &gt; kube-proxy-csr.json &lt;&lt;EOF&#123; "CN": "system:kube-proxy", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "system:kube-proxy", "OU": "System" &#125; ]&#125;EOF kube-admin-csr.json123456789101112131415161718cat &gt; kube-admin-csr.json &lt;&lt;EOF&#123; "CN": "admin", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "system:masters", "OU": "System" &#125; ]&#125;EOF front-proxy-ca-csr.json123456789cat &gt; front-proxy-ca-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;&#125;EOF front-proxy-client-csr.json123456789cat &gt; front-proxy-client-csr.json &lt;&lt;EOF&#123; "CN": "front-proxy-client", "key": &#123; "algo": "rsa", "size": 2048 &#125;&#125;EOF sa-csr.json123456789101112131415161718cat &gt; sa-csr.json &lt;&lt;EOF&#123; "CN": "service-accounts", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "Guangdong", "L": "Guangzhou", "O": "Kubernetes", "OU": "System" &#125; ]&#125;EOF 创建etcd证书etcd-ca证书12echo '--- 创建etcd-ca证书 ---'cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca etcd-server证书1234567echo '--- 创建etcd-server证书 ---'cfssl gencert \ -ca=etcd-ca.pem \ -ca-key=etcd-ca-key.pem \ -config=ca-config.json \ -hostname=127.0.0.1,$(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | paste -d, -s -) \ -profile=etcd-server etcd-server-csr.json | cfssljson -bare etcd-server etcd-client证书123456echo '--- 创建etcd-client证书 ---'cfssl gencert \ -ca=etcd-ca.pem \ -ca-key=etcd-ca-key.pem \ -config=ca-config.json \ -profile=etcd-client etcd-client-csr.json | cfssljson -bare etcd-client 创建kubernetes证书kubernetes-CA 证书123echo '--- 创建kubernetes-ca证书 ---'# 创建kubernetes-ca证书cfssl gencert -initca ca-csr.json | cfssljson -bare kube-ca kube-apiserver证书12345678910echo '--- 创建kube-apiserver证书 ---'# 创建kube-apiserver证书# 这里的hostname字段中的10.96.0.1要跟上文提到的service cluster ip cidr对应cfssl gencert \ -ca=kube-ca.pem \ -ca-key=kube-ca-key.pem \ -config=ca-config.json \ -hostname=10.96.0.1,127.0.0.1,localhost,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.default.svc.cluster.local,$&#123;VIP&#125;,$(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | paste -d, -s -) \ -profile=kubernetes \ kube-apiserver-csr.json | cfssljson -bare kube-apiserver kube-controller-manager证书12345678echo '--- 创建kube-controller-manager证书 ---'# 创建kube-controller-manager证书cfssl gencert \ -ca=kube-ca.pem \ -ca-key=kube-ca-key.pem \ -config=ca-config.json \ -profile=kubernetes \ kube-manager-csr.json | cfssljson -bare kube-controller-manager kube-scheduler证书12345678echo '--- 创建kube-scheduler证书 ---'# 创建kube-scheduler证书cfssl gencert \ -ca=kube-ca.pem \ -ca-key=kube-ca-key.pem \ -config=ca-config.json \ -profile=kubernetes \ kube-scheduler-csr.json | cfssljson -bare kube-scheduler kube-proxy证书12345678echo '--- 创建kube-proxy证书 ---'# 创建kube-proxy证书cfssl gencert \ -ca=kube-ca.pem \ -ca-key=kube-ca-key.pem \ -config=ca-config.json \ -profile=kubernetes \ kube-proxy-csr.json | cfssljson -bare kube-proxy kube-admin证书12345678echo '--- 创建kube-admin证书 ---'# 创建kube-admin证书cfssl gencert \ -ca=kube-ca.pem \ -ca-key=kube-ca-key.pem \ -config=ca-config.json \ -profile=kubernetes \ kube-admin-csr.json | cfssljson -bare kube-admin Front Proxy证书123456789echo '--- 创建Front Proxy Certificate证书 ---'# 创建Front Proxy Certificate证书cfssl gencert -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-cacfssl gencert \ -ca=front-proxy-ca.pem \ -ca-key=front-proxy-ca-key.pem \ -config=ca-config.json \ -profile=kubernetes \ front-proxy-client-csr.json | cfssljson -bare front-proxy-client Service Account证书12345678echo '--- 创建service account证书 ---'# 创建创建service account证书cfssl gencert \ -ca=kube-ca.pem \ -ca-key=kube-ca-key.pem \ -config=ca-config.json \ -profile=kubernetes \ sa-csr.json | cfssljson -bare sa bootstrap-token1234567BOOTSTRAP_TOKEN=$(dd if=/dev/urandom bs=128 count=1 2&gt;/dev/null | base64 | tr -d "=+/[:space:]" | dd bs=32 count=1 2&gt;/dev/null)echo "BOOTSTRAP_TOKEN: $&#123;BOOTSTRAP_TOKEN&#125;"# 创建token.csv文件cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,"system:bootstrappers"EOF encryption.yaml1234567891011121314151617ENCRYPTION_TOKEN=$(head -c 32 /dev/urandom | base64)echo "ENCRYPTION_TOKEN: $&#123;ENCRYPTION_TOKEN&#125;"# 创建encryption.yaml文件cat &gt; encryption.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: $&#123;ENCRYPTION_TOKEN&#125; - identity: &#123;&#125;EOF audit-policy.yaml123456789echo '--- 创建创建高级审计配置 ---'# 创建高级审计配置cat &gt;&gt; audit-policy.yaml &lt;&lt;EOF# Log all requests at the Metadata level.apiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: MetadataEOF 创建kubeconfig文件说明 kubeconfig 文件用于组织关于集群、用户、命名空间和认证机制的信息。 命令行工具 kubectl 从 kubeconfig 文件中得到它要选择的集群以及跟集群 API server 交互的信息。 默认情况下，kubectl 会从 $HOME/.kube 目录下查找文件名为 config 的文件。 注意： 用于配置集群访问信息的文件叫作 kubeconfig文件，这是一种引用配置文件的通用方式，并不是说它的文件名就是 kubeconfig。 kube-controller-manager.kubeconfig1234567891011121314151617181920echo "Create kube-controller-manager kubeconfig..."# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=kube-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-controller-manager.kubeconfig# 设置客户端认证参数kubectl config set-credentials system:kube-controller-manager \ --client-certificate=kube-controller-manager.pem \ --client-key=kube-controller-manager-key.pem \ --embed-certs=true \ --kubeconfig=kube-controller-manager.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=system:kube-controller-manager \ --kubeconfig=kube-controller-manager.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig kube-scheduler.kubeconfig1234567891011121314151617181920echo "Create kube-scheduler kubeconfig..."# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=kube-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-scheduler.kubeconfig# 设置客户端认证参数kubectl config set-credentials system:kube-scheduler \ --client-certificate=kube-scheduler.pem \ --client-key=kube-scheduler-key.pem \ --embed-certs=true \ --kubeconfig=kube-scheduler.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=system:kube-scheduler \ --kubeconfig=kube-scheduler.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig kube-proxy.kubeconfig1234567891011121314151617181920echo "Create kube-proxy kubeconfig..."# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=kube-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials system:kube-proxy \ --client-certificate=kube-proxy.pem \ --client-key=kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=system:kube-proxy \ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig kube-admin.kubeconfig1234567891011121314151617181920echo "Create kube-admin kubeconfig..."# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=kube-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-admin.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubernetes-admin \ --client-certificate=kube-admin.pem \ --client-key=kube-admin-key.pem \ --embed-certs=true \ --kubeconfig=kube-admin.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubernetes-admin \ --kubeconfig=kube-admin.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-admin.kubeconfig bootstrap.kubeconfig123456789101112131415161718echo "Create kubelet bootstrapping kubeconfig..."# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=kube-ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig 清理证书CSR文件12echo '--- 删除*.csr文件 ---'rm -rf *csr 修改文件权限123chown root:root *pem *kubeconfig *yaml *csvchmod 0444 *pem *kubeconfig *yaml *csvchmod 0400 *key.pem 检查生成的文件123456789101112131415161718192021222324252627282930313233ls -l | grep -v json-r--r--r-- 1 root root 113 Dec 6 15:36 audit-policy.yaml-r--r--r-- 1 root root 2207 Dec 6 15:36 bootstrap.kubeconfig-r--r--r-- 1 root root 240 Dec 6 15:36 encryption.yaml-r-------- 1 root root 1675 Dec 6 15:36 etcd-ca-key.pem-r--r--r-- 1 root root 1375 Dec 6 15:36 etcd-ca.pem-r-------- 1 root root 1679 Dec 6 15:36 etcd-client-key.pem-r--r--r-- 1 root root 1424 Dec 6 15:36 etcd-client.pem-r-------- 1 root root 1679 Dec 6 15:36 etcd-server-key.pem-r--r--r-- 1 root root 1468 Dec 6 15:36 etcd-server.pem-r-------- 1 root root 1679 Dec 6 15:36 front-proxy-ca-key.pem-r--r--r-- 1 root root 1143 Dec 6 15:36 front-proxy-ca.pem-r-------- 1 root root 1675 Dec 6 15:36 front-proxy-client-key.pem-r--r--r-- 1 root root 1188 Dec 6 15:36 front-proxy-client.pem-r-------- 1 root root 1679 Dec 6 15:36 kube-admin-key.pem-r--r--r-- 1 root root 6345 Dec 6 15:36 kube-admin.kubeconfig-r--r--r-- 1 root root 1419 Dec 6 15:36 kube-admin.pem-r-------- 1 root root 1675 Dec 6 15:36 kube-apiserver-key.pem-r--r--r-- 1 root root 1688 Dec 6 15:36 kube-apiserver.pem-r-------- 1 root root 1679 Dec 6 15:36 kube-ca-key.pem-r--r--r-- 1 root root 1387 Dec 6 15:36 kube-ca.pem-r-------- 1 root root 1679 Dec 6 15:36 kube-controller-manager-key.pem-r--r--r-- 1 root root 6449 Dec 6 15:36 kube-controller-manager.kubeconfig-r--r--r-- 1 root root 1476 Dec 6 15:36 kube-controller-manager.pem-r-------- 1 root root 1675 Dec 6 15:36 kube-proxy-key.pem-r--r--r-- 1 root root 6371 Dec 6 15:36 kube-proxy.kubeconfig-r--r--r-- 1 root root 1440 Dec 6 15:36 kube-proxy.pem-r-------- 1 root root 1675 Dec 6 15:36 kube-scheduler-key.pem-r--r--r-- 1 root root 6395 Dec 6 15:36 kube-scheduler.kubeconfig-r--r--r-- 1 root root 1452 Dec 6 15:36 kube-scheduler.pem-r-------- 1 root root 1675 Dec 6 15:36 sa-key.pem-r--r--r-- 1 root root 1432 Dec 6 15:36 sa.pem-r--r--r-- 1 root root 80 Dec 6 15:36 token.csv kubernetes-master节点本节介绍如何部署kubernetes master节点 master节点说明 原则上，master节点不应该运行业务Pod，且不应该暴露到公网环境！！ 边界节点，应该交由worker节点或者运行Ingress的节点来承担 以kubeadm部署为例，部署完成后，会给master节点添加node-role.kubernetes.io/master=&#39;&#39;标签（Labels）并且会对带有此标签的节点添加node-role.kubernetes.io/master:NoSchedule污点（taints），这样不能容忍此污点的Pod无法调度到master节点 本文中，在kubelet启动参数里，默认添加node-role.kubernetes.io/node=&#39;&#39;标签（Labels），且没有对master节点添加node-role.kubernetes.io/master:NoSchedule污点（taints） 生产环境中最好参照kubeadm，对master节点添加node-role.kubernetes.io/master=&#39;&#39;标签（Labels）和node-role.kubernetes.io/master:NoSchedule污点（taints） kube-apiserver 以 REST APIs 提供 Kubernetes 资源的 CRUD,如授权、认证、存取控制与 API 注册等机制。 关闭默认非安全端口8080,在安全端口 6443 接收 https 请求 严格的认证和授权策略 (x509、token、RBAC) 开启 bootstrap token 认证，支持 kubelet TLS bootstrapping 使用 https 访问 kubelet、etcd，加密通信 kube-controller-manager 通过核心控制循环(Core Control Loop)监听 Kubernetes API的资源来维护集群的状态，这些资源会被不同的控制器所管理，如 Replication Controller、NamespaceController 等等。而这些控制器会处理着自动扩展、滚动更新等等功能。 关闭非安全端口，在安全端口 10252 接收 https 请求 使用 kubeconfig 访问 kube-apiserver 的安全端口 kube-scheduler 负责将一个(或多个)容器依据调度策略分配到对应节点上让容器引擎(如 Docker)执行。 调度受到 QoS 要求、软硬性约束、亲和性(Affinity)等等因素影响。 HAProxy 提供多个 API Server 的负载均衡(Load Balance) 监听VIP的8443端口负载均衡到三台master节点的6443端口 Keepalived 提供虚拟IP位址(VIP),来让vip落在可用的master主机上供所有组件访问master节点 提供健康检查脚本用于切换VIP 添加用户 这里强迫症发作，指定了UID和GID 不指定UID和GID也可以 12345678echo '--- master节点添加用户 ---'for NODE in "$&#123;!MasterArray[@]&#125;";do echo "--- $NODE ---" ssh $&#123;NODE&#125; /usr/sbin/groupadd -r -g 10000 kube ssh $&#123;NODE&#125; /usr/sbin/groupadd -r -g 10001 etcd ssh $&#123;NODE&#125; /usr/sbin/useradd -r -g kube -u 10000 -s /bin/false kube ssh $&#123;NODE&#125; /usr/sbin/useradd -r -g etcd -u 10001 -s /bin/false etcddone 创建目录12345678910111213141516171819202122232425262728293031echo '--- master节点创建目录 ---'for NODE in "$&#123;!MasterArray[@]&#125;";do echo "--- $NODE ---" echo "--- 创建目录 ---" ssh $&#123;NODE&#125; /usr/bin/mkdir -p /etc/etcd/ssl \ /etc/kubernetes/pki \ /etc/kubernetes/manifests \ /var/lib/etcd \ /var/lib/kubelet \ /var/run/kubernetes \ /var/log/kube-audit \ /etc/cni/net.d \ /opt/cni/bin echo "--- 修改目录权限 ---" ssh $&#123;NODE&#125; /usr/bin/chmod 0755 /etc/etcd \ /etc/etcd/ssl \ /etc/kubernetes \ /etc/kubernetes/pki \ /var/lib/etcd \ /var/lib/kubelet \ /var/log/kube-audit \ /var/run/kubernetes \ /etc/cni/net.d \ /opt/cni/bin echo "--- 修改目录属组 ---" ssh $&#123;NODE&#125; chown -R etcd:etcd /etc/etcd/ /var/lib/etcd ssh $&#123;NODE&#125; chown -R kube:kube /etc/kubernetes \ /var/lib/kubelet \ /var/log/kube-audit \ /var/run/kubernetesdone 分发证书文件和kubeconfig到master节点1234567891011121314151617181920212223242526272829303132333435for NODE in "$&#123;!MasterArray[@]&#125;";do echo "---- $NODE ----" echo '---- 分发etcd证书 ----' rsync -avpt /root/pki/etcd-ca-key.pem \ /root/pki/etcd-ca.pem \ /root/pki/etcd-client-key.pem \ /root/pki/etcd-client.pem \ /root/pki/etcd-server-key.pem \ /root/pki/etcd-server.pem \ $NODE:/etc/etcd/ssl/ echo '---- 分发kubeconfig文件 yaml文件 token.csv ----' rsync -avpt /root/pki/kube-admin.kubeconfig \ /root/pki/kube-controller-manager.kubeconfig \ /root/pki/kube-scheduler.kubeconfig \ /root/pki/audit-policy.yaml \ /root/pki/encryption.yaml \ /root/pki/token.csv \ $NODE:/etc/kubernetes/ echo '---- 分发sa证书 kube证书 front-proxy证书 ----' rsync -avpt /root/pki/etcd-ca.pem \ /root/pki/etcd-client-key.pem \ /root/pki/etcd-client.pem \ /root/pki/front-proxy-ca.pem \ /root/pki/front-proxy-client-key.pem \ /root/pki/front-proxy-client.pem \ /root/pki/kube-apiserver-key.pem \ /root/pki/kube-apiserver.pem \ /root/pki/kube-ca.pem \ /root/pki/kube-ca-key.pem \ /root/pki/sa-key.pem \ /root/pki/sa.pem \ $NODE:/etc/kubernetes/pki/ ssh $NODE chown -R etcd:etcd /etc/etcd ssh $NODE chown -R kube:kube /etc/kubernetesdone 分发二进制文件 在k8s-m1上操作 1234567891011121314151617echo '--- 分发kubernetes和etcd二进制文件 ---'for NODE in "$&#123;!MasterArray[@]&#125;";do echo "--- $NODE ---" rsync -avpt /root/software/kubernetes/server/bin/hyperkube \ /root/software/kubernetes/server/bin/kube-controller-manager \ /root/software/kubernetes/server/bin/kubectl \ /root/software/kubernetes/server/bin/apiextensions-apiserver \ /root/software/kubernetes/server/bin/kube-apiserver \ /root/software/kubernetes/server/bin/kubeadm \ /root/software/kubernetes/server/bin/kube-aggregator \ /root/software/kubernetes/server/bin/kube-scheduler \ /root/software/kubernetes/server/bin/cloud-controller-manager \ /root/software/kubernetes/server/bin/mounter \ /root/software/etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcdctl \ /root/software/etcd-$&#123;ETCD_VERSION&#125;-linux-amd64/etcd \ $NODE:/usr/local/bin/done 部署配置Keepalived和HAProxy 在k8s-m1上操作 切换工作目录1cd /root/master 安装Keepalived和HAProxy12345for NODE in "$&#123;!MasterArray[@]&#125;";do echo "---- $NODE ----" echo "---- 安装haproxy和keepalived ----" ssh $NODE yum install keepalived haproxy -ydone 配置keepalived 编辑keepalived.conf模板 替换keepalived.conf的字符串 编辑check_haproxy.sh 1234567891011121314151617181920212223242526272829303132333435cat &gt; keepalived.conf.example &lt;&lt;EOFvrrp_script haproxy-check &#123; script "/bin/bash /etc/keepalived/check_haproxy.sh" interval 3 weight -2 fall 10 rise 2&#125;vrrp_instance haproxy-vip &#123; state BACKUP priority 101 interface &#123;&#123; VIP_IFACE &#125;&#125; virtual_router_id 47 advert_int 3 unicast_peer &#123; &#125; virtual_ipaddress &#123; &#123;&#123; VIP &#125;&#125; &#125; track_script &#123; haproxy-check &#125;&#125;EOF# 替换字符sed -r -e "s#\&#123;\&#123; VIP \&#125;\&#125;#$&#123;VIP&#125;#" \ -e "s#\&#123;\&#123; VIP_IFACE \&#125;\&#125;#$&#123;VIP_IFACE&#125;#" \ -e '/unicast_peer/r '&lt;(xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#^#\t#') \ keepalived.conf.example &gt; keepalived.conf 12345678910111213cat &gt; check_haproxy.sh &lt;&lt;EOF#!/bin/bashVIRTUAL_IP=$&#123;VIP&#125;errorExit() &#123; echo "*** $*" 1&gt;&amp;2 exit 1&#125;if ip addr | grep -q \$VIRTUAL_IP ; then curl -s --max-time 2 --insecure https://\$&#123;VIRTUAL_IP&#125;:8443/ -o /dev/null || errorExit "Error GET https://\$&#123;VIRTUAL_IP&#125;:8443/"fiEOF 配置haproxy 编辑haproxy.cfg模板 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051cat &gt; haproxy.cfg.example &lt;&lt;EOFglobal maxconn 2000 ulimit-n 16384 log 127.0.0.1 local0 err stats timeout 30sdefaults log global mode http option httplog timeout connect 5000 timeout client 50000 timeout server 50000 timeout http-request 15s timeout http-keep-alive 15sfrontend monitor-in bind $&#123;VIP&#125;:33305 mode http option httplog monitor-uri /monitorlisten stats bind $&#123;VIP&#125;:8006 mode http stats enable stats hide-version stats uri /stats stats refresh 30s stats realm Haproxy\ Statistics stats auth admin:adminfrontend k8s-api bind $&#123;VIP&#125;:8443 mode tcp option tcplog tcp-request inspect-delay 5s default_backend k8s-apibackend k8s-api mode tcp option tcplog option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100EOF# 替换字符sed -e '$r '&lt;(paste &lt;( seq -f' server k8s-api-%g' $&#123;#MasterArray[@]&#125; ) &lt;( xargs -n1&lt;&lt;&lt;$&#123;MasterArray[@]&#125; | sort | sed 's#$#:6443 check#')) haproxy.cfg.example &gt; haproxy.cfg 分发配置文件到master节点1234567for NODE in "$&#123;!MasterArray[@]&#125;";do echo "---- $NODE ----" rsync -avpt haproxy.cfg $NODE:/etc/haproxy/ rsync -avpt keepalived.conf \ check_haproxy.sh \ $NODE:/etc/keepalived/done 启动keepalived和haproxy1234for NODE in "$&#123;!MasterArray[@]&#125;";do echo "---- $NODE ----" ssh $NODE systemctl enable --now keepalived haproxydone 验证VIP 需要大约十秒的时间等待keepalived和haproxy服务起来 这里由于后端的kube-apiserver服务还没启动，只测试是否能ping通VIP 如果VIP没起来，就要去确认一下各master节点的keepalived服务是否正常 12sleep 15ping -c 4 $VIP 部署etcd集群 每个etcd节点的配置都需要做对应更改 在k8s-m1上操作 配置etcd.service文件123456789101112131415161718cat &gt; etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServiceDocumentation=https://coreos.com/etcd/docs/latest/After=network.target[Service]User=etcdType=notifyExecStart=/usr/local/bin/etcd --config-file=/etc/etcd/etcd.config.yamlRestart=on-failureRestartSec=10LimitNOFILE=65536[Install]WantedBy=multi-user.targetAlias=etcd3.serviceEOF etcd.config.yaml模板 关于各个参数的说明可以看这里 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899cat &gt; etcd.config.yaml.example &lt;&lt;EOF# This is the configuration file for the etcd server.# Human-readable name for this member.name: '&#123;HOSTNAME&#125;'# Path to the data directory.data-dir: '/var/lib/etcd/&#123;HOSTNAME&#125;.data/'# Path to the dedicated wal directory.wal-dir: '/var/lib/etcd/&#123;HOSTNAME&#125;.wal/'# Number of committed transactions to trigger a snapshot to disk.snapshot-count: 5000# Time (in milliseconds) of a heartbeat interval.heartbeat-interval: 100# Time (in milliseconds) for an election to timeout.election-timeout: 1000# Raise alarms when backend size exceeds the given quota. 0 means use the# default quota.quota-backend-bytes: 0# List of comma separated URLs to listen on for peer traffic.listen-peer-urls: 'https://&#123;PUBLIC_IP&#125;:2380'# List of comma separated URLs to listen on for client traffic.listen-client-urls: 'https://&#123;PUBLIC_IP&#125;:2379,http://127.0.0.1:2379'# Maximum number of snapshot files to retain (0 is unlimited).max-snapshots: 3# Maximum number of wal files to retain (0 is unlimited).max-wals: 5# Comma-separated white list of origins for CORS (cross-origin resource sharing).cors:# List of this member's peer URLs to advertise to the rest of the cluster.# The URLs needed to be a comma-separated list.initial-advertise-peer-urls: 'https://&#123;PUBLIC_IP&#125;:2380'# List of this member's client URLs to advertise to the public.# The URLs needed to be a comma-separated list.advertise-client-urls: 'https://&#123;PUBLIC_IP&#125;:2379'# Discovery URL used to bootstrap the cluster.discovery:# Valid values include 'exit', 'proxy'discovery-fallback: 'proxy'# HTTP proxy to use for traffic to discovery service.discovery-proxy:# DNS domain used to bootstrap initial cluster.discovery-srv:# Initial cluster configuration for bootstrapping.initial-cluster: '$&#123;ETCD_INITIAL_CLUSTER&#125;'# Initial cluster token for the etcd cluster during bootstrap.initial-cluster-token: 'etcd-k8s-cluster'# Initial cluster state ('new' or 'existing').initial-cluster-state: 'new'# Reject reconfiguration requests that would cause quorum loss.strict-reconfig-check: false# Accept etcd V2 client requestsenable-v2: true# Enable runtime profiling data via HTTP serverenable-pprof: true# Valid values include 'on', 'readonly', 'off'proxy: 'off'# Time (in milliseconds) an endpoint will be held in a failed state.proxy-failure-wait: 5000# Time (in milliseconds) of the endpoints refresh interval.proxy-refresh-interval: 30000# Time (in milliseconds) for a dial to timeout.proxy-dial-timeout: 1000# Time (in milliseconds) for a write to timeout.proxy-write-timeout: 5000# Time (in milliseconds) for a read to timeout.proxy-read-timeout: 0client-transport-security: # Path to the client server TLS cert file. cert-file: '/etc/etcd/ssl/etcd-server.pem' # Path to the client server TLS key file. key-file: '/etc/etcd/ssl/etcd-server-key.pem' # Enable client cert authentication. client-cert-auth: true # Path to the client server TLS trusted CA cert file. trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem' # Client TLS using generated certificates auto-tls: truepeer-transport-security: # Path to the peer server TLS cert file. cert-file: '/etc/etcd/ssl/etcd-server.pem' # Path to the peer server TLS key file. key-file: '/etc/etcd/ssl/etcd-server-key.pem' # Enable peer client cert authentication. client-cert-auth: true # Path to the peer server TLS trusted CA cert file. trusted-ca-file: '/etc/etcd/ssl/etcd-ca.pem' # Peer TLS using generated certificates. auto-tls: true# Enable debug-level logging for etcd.debug: falselogger: 'zap'# Specify 'stdout' or 'stderr' to skip journald logging even when running under systemd.log-outputs: [default]# Force to create a new one member cluster.force-new-cluster: falseauto-compaction-mode: 'periodic'auto-compaction-retention: '1'# Set level of detail for exported metrics, specify 'extensive' to include histogram metrics.metrics: 'basic'EOF 分发配置文件123456789101112# 根据节点信息替换文本，分发到各etcd节点for NODE in "$&#123;!MasterArray[@]&#125;";do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" sed -e "s/&#123;HOSTNAME&#125;/$NODE/g" \ -e "s/&#123;PUBLIC_IP&#125;/$&#123;MasterArray[$NODE]&#125;/g" \ etcd.config.yaml.example &gt; etcd.config.yaml.$&#123;NODE&#125; rsync -avpt etcd.config.yaml.$&#123;NODE&#125; $&#123;NODE&#125;:/etc/etcd/etcd.config.yaml rsync -avpt etcd.service $&#123;NODE&#125;:/usr/lib/systemd/system/etcd.service ssh $&#123;NODE&#125; systemctl daemon-reload ssh $&#123;NODE&#125; chown -R etcd:etcd /etc/etcd rm -rf etcd.config.yaml.$&#123;NODE&#125;done 启动etcd集群 etcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 systemctl start etcd 会卡住一段时间，为正常现象 启动之后可以通过etcdctl命令查看集群状态 12345for NODE in "$&#123;!MasterArray[@]&#125;";do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" ssh $NODE systemctl enable etcd ssh $NODE systemctl start etcd &amp;done 为方便维护，可使用alias简化etcdctl命令 1234cat &gt;&gt; /root/.bashrc &lt;&lt;EOFalias etcdctl2="export ETCDCTL_API=2;etcdctl --ca-file '/etc/etcd/ssl/etcd-ca.pem' --cert-file '/etc/etcd/ssl/etcd-client.pem' --key-file '/etc/etcd/ssl/etcd-client-key.pem' --endpoints $&#123;ETCD_SERVERS&#125;"alias etcdctl3="export ETCDCTL_API=3;etcdctl --cacert=/etc/etcd/ssl/etcd-ca.pem --cert=/etc/etcd/ssl/etcd-client.pem --key=/etc/etcd/ssl/etcd-client-key.pem --endpoints=$&#123;ETCD_SERVERS&#125;"EOF 验证etcd集群状态 etcd提供v2和v3两套API，kubernetes使用v3 123456789101112131415161718192021222324252627282930313233# 应用上面定义的aliassource /root/.bashrc# 使用v2 API访问etcd的集群状态etcdctl2 cluster-health# 示例输出member 222fd3b0bb4a5931 is healthy: got healthy result from https://172.16.80.203:2379member 8349ef180b115a83 is healthy: got healthy result from https://172.16.80.201:2379member f525d2d797a7c465 is healthy: got healthy result from https://172.16.80.202:2379cluster is healthy# 使用v2 API访问etcd成员列表etcdctl2 member list# 示例输出222fd3b0bb4a5931: name=k8s-m3 peerURLs=https://172.16.80.203:2380 clientURLs=https://172.16.80.203:2379 isLeader=false8349ef180b115a83: name=k8s-m1 peerURLs=https://172.16.80.201:2380 clientURLs=https://172.16.80.201:2379 isLeader=falsef525d2d797a7c465: name=k8s-m2 peerURLs=https://172.16.80.202:2380 clientURLs=https://172.16.80.202:2379 isLeader=true# 使用v3 API访问etcd的endpoint状态etcdctl3 endpoint health# 示例输出https://172.16.80.201:2379 is healthy: successfully committed proposal: took = 2.879402mshttps://172.16.80.203:2379 is healthy: successfully committed proposal: took = 6.708566mshttps://172.16.80.202:2379 is healthy: successfully committed proposal: took = 7.187607ms# 使用v3 API访问etcd成员列表etcdctl3 member list --write-out=table# 示例输出+------------------+---------+--------+----------------------------+----------------------------+| ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS |+------------------+---------+--------+----------------------------+----------------------------+| 222fd3b0bb4a5931 | started | k8s-m3 | https://172.16.80.203:2380 | https://172.16.80.203:2379 || 8349ef180b115a83 | started | k8s-m1 | https://172.16.80.201:2380 | https://172.16.80.201:2379 || f525d2d797a7c465 | started | k8s-m2 | https://172.16.80.202:2380 | https://172.16.80.202:2379 |+------------------+---------+--------+----------------------------+----------------------------+ Master组件服务master组件配置模板kube-apiserver.conf --allow-privileged=true启用容器特权模式 --apiserver-count=3指定集群运行模式，其它节点处于阻塞状态 --audit-policy-file=/etc/kubernetes/audit-policy.yaml 基于audit-policy.yaml文件定义的内容启动审计功能 --authorization-mode=Node,RBAC开启 Node 和 RBAC 授权模式，拒绝未授权的请求 --disable-admission-plugins=和--enable-admission-plugins禁用和启用准入控制插件。 准入控制插件会在请求通过认证和授权之后、对象被持久化之前拦截到达apiserver的请求。 准入控制插件依次执行，因此需要注意顺序。 如果插件序列中任何一个拒绝了请求，则整个请求将立刻被拒绝并返回错误给客户端。 关于admission-plugins官方文档里面有推荐配置，这里直接采用官方配置，注意针对不同kubernetes版本都会有不一样的配置，具体可以看这里 --enable-bootstrap-token-auth=true启用 kubelet bootstrap 的 token 认证 --experimental-encryption-provider-config=/etc/kubernetes/encryption.yaml启用加密特性将Secret数据加密存储到etcd --insecure-port=0关闭监听非安全端口8080 --runtime-config=api/all=true启用所有版本的 APIs --service-cluster-ip-range=10.96.0.0/12指定 Service Cluster IP 地址段 --service-node-port-range=30000-32767指定 NodePort 的端口范围 --token-auth-file=/etc/kubernetes/token.csv保存bootstrap的token信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647cat &gt; kube-apiserver.conf.example &lt;&lt;EOFKUBE_APISERVER_ARGS=" \\--advertise-address=&#123;PUBLIC_IP&#125; \\--allow-privileged=true \\--apiserver-count=3 \\--audit-log-maxage=30 \\--audit-log-maxbackup=3 \\--audit-log-maxsize=100 \\--audit-log-path=/var/log/kube-audit/audit.log \\--audit-policy-file=/etc/kubernetes/audit-policy.yaml \\--authorization-mode=Node,RBAC \\--bind-address=0.0.0.0 \\--client-ca-file=/etc/kubernetes/pki/kube-ca.pem \\--disable-admission-plugins=PersistentVolumeLabel \\--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset \\--enable-bootstrap-token-auth=true \\--etcd-cafile=/etc/kubernetes/pki/etcd-ca.pem \\--etcd-certfile=/etc/kubernetes/pki/etcd-client.pem \\--etcd-keyfile=/etc/kubernetes/pki/etcd-client-key.pem \\--etcd-servers=$ETCD_SERVERS \\--experimental-encryption-provider-config=/etc/kubernetes/encryption.yaml \\--event-ttl=1h \\--feature-gates=PodShareProcessNamespace=true,ExpandPersistentVolumes=true \\--insecure-port=0 \\--kubelet-client-certificate=/etc/kubernetes/pki/kube-apiserver.pem \\--kubelet-client-key=/etc/kubernetes/pki/kube-apiserver-key.pem \\--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \\--logtostderr=true \\--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem \\--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem \\--requestheader-allowed-names=aggregator \\--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem \\--requestheader-extra-headers-prefix=X-Remote-Extra- \\--requestheader-group-headers=X-Remote-Group \\--requestheader-username-headers=X-Remote-User \\--runtime-config=api/all=true \\--secure-port=6443 \\--service-account-key-file=/etc/kubernetes/pki/sa.pem \\--service-cluster-ip-range=10.96.0.0/12 \\--service-node-port-range=30000-32767 \\--storage-backend=etcd3 \\--tls-cert-file=/etc/kubernetes/pki/kube-apiserver.pem \\--tls-private-key-file=/etc/kubernetes/pki/kube-apiserver-key.pem \\--token-auth-file=/etc/kubernetes/token.csv \\--v=2 \\"EOF kube-controller-manager.conf --allocate-node-cidrs=true在cloud provider上分配和设置pod的CIDR --cluster-cidr集群内的pod的CIDR范围，需要 --allocate-node-cidrs设为true --experimental-cluster-signing-duration=8670h0m0s指定 TLS Bootstrap 证书的有效期 --feature-gates=RotateKubeletServerCertificate=true开启 kublet server 证书的自动更新特性 --horizontal-pod-autoscaler-use-rest-clients=true能够使用自定义资源（Custom Metrics）进行自动水平扩展 --leader-elect=true集群运行模式，启用选举功能，被选为 leader 的节点负责处理工作，其它节点为阻塞状态 --node-cidr-mask-size=24集群中node cidr的掩码 --service-cluster-ip-range=10.96.0.0/16指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致 1234567891011121314151617181920212223242526cat &gt; kube-controller-manager.conf.example &lt;&lt;EOFKUBE_CONTROLLER_MANAGER_ARGS=" \\--address=0.0.0.0 \\--allocate-node-cidrs=true \\--cluster-cidr=$POD_NET_CIDR \\--cluster-signing-cert-file=/etc/kubernetes/pki/kube-ca.pem \\--cluster-signing-key-file=/etc/kubernetes/pki/kube-ca-key.pem \\--controllers=*,bootstrapsigner,tokencleaner \\--experimental-cluster-signing-duration=8670h0m0s \\--feature-gates=RotateKubeletServerCertificate=true,ExpandPersistentVolumes=true \\--horizontal-pod-autoscaler-sync-period=10s \\--horizontal-pod-autoscaler-use-rest-clients=true \\--kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\--leader-elect=true \\--logtostderr=true \\--node-cidr-mask-size=24 \\--node-monitor-grace-period=40s \\--node-monitor-period=5s \\--pod-eviction-timeout=2m0s \\--root-ca-file=/etc/kubernetes/pki/kube-ca.pem \\--service-account-private-key-file=/etc/kubernetes/pki/sa-key.pem \\--service-cluster-ip-range=$SVC_CLUSTER_CIDR \\--use-service-account-credentials=true \\--v=2 \\"EOF kube-scheduler.conf --leader-elect=true集群运行模式，启用选举功能，被选为 leader 的节点负责处理工作，其它节点为阻塞状态 12345678910cat &gt; kube-scheduler.conf.example &lt;&lt;EOFKUBE_SCHEDULER_ARGS="\\--address=0.0.0.0 \\--algorithm-provider=DefaultProvider \\--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\--leader-elect=true \\--logtostderr=true \\--v=2 \\"EOF systemd服务文件kube-apiserver.service123456789101112131415161718cat &gt; kube-apiserver.service &lt;&lt;EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]User=kubeEnvironmentFile=-/etc/kubernetes/kube-apiserver.confExecStart=/usr/local/bin/kube-apiserver \$KUBE_APISERVER_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF kube-controller-manager.service123456789101112131415cat &gt; kube-controller-manager.service &lt;&lt;EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]User=kubeEnvironmentFile=-/etc/kubernetes/kube-controller-manager.confExecStart=/usr/local/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF kube-scheduler.service123456789101112131415cat &gt; kube-scheduler.service &lt;&lt;EOF[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]User=kubeEnvironmentFile=-/etc/kubernetes/kube-scheduler.confExecStart=/usr/local/bin/kube-scheduler \$KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 分发配置文件到各master节点 根据master节点的信息替换配置文件里面的字段 1234567891011for NODE in "$&#123;!MasterArray[@]&#125;";do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" rsync -avpt kube*service $NODE:/usr/lib/systemd/system/ sed -e "s/&#123;PUBLIC_IP&#125;/$&#123;MasterArray[$NODE]&#125;/g" kube-apiserver.conf.example &gt; kube-apiserver.conf.$&#123;NODE&#125; rsync -avpt kube-apiserver.conf.$&#123;NODE&#125; $NODE:/etc/kubernetes/kube-apiserver.conf rsync -avpt kube-controller-manager.conf.example $NODE:/etc/kubernetes/kube-controller-manager.conf rsync -avpt kube-scheduler.conf.example $NODE:/etc/kubernetes/kube-scheduler.conf rm -rf *conf.$&#123;NODE&#125; ssh $NODE systemctl daemon-reload ssh $NODE chown -R kube:kube /etc/kubernetesdone 启动kubernetes服务 可以先在k8s-m1上面启动服务，确认正常之后再在其他master节点启动 123systemctl enable --now kube-apiserver.servicesystemctl enable --now kube-controller-manager.servicesystemctl enable --now kube-scheduler.service 12345678910111213kubectl --kubeconfig=/etc/kubernetes/kube-admin.kubeconfig get cs# 输出示例NAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy &#123;"health":"true"&#125; etcd-0 Healthy &#123;"health":"true"&#125; etcd-1 Healthy &#123;"health":"true"&#125;kubectl --kubeconfig=/etc/kubernetes/kube-admin.kubeconfig get endpoints# 输出示例NAME ENDPOINTS AGEkubernetes 172.16.80.201:6443 27s 123456for NODE in "$&#123;!MasterArray[@]&#125;";do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" ssh $NODE "systemctl enable --now kube-apiserver" ssh $NODE "systemctl enable --now kube-controller-manager" ssh $NODE "systemctl enable --now kube-scheduler"done 三台master节点的kube-apiserver、kube-controller-manager、kube-scheduler服务启动成功后可以测试一下 1234kubectl --kubeconfig=/etc/kubernetes/kube-admin.kubeconfig get endpoints# 输出示例NAME ENDPOINTS AGEkubernetes 172.16.80.201:6443,172.16.80.202:6443,172.16.80.203:6443 12m 设置kubectl kubectl命令默认会加载~/.kube/config文件，如果文件不存在则连接http://127.0.0.1:8080，这显然不符合预期，这里使用之前生成的kube-admin.kubeconfig 在k8s-m1上操作 12345for NODE in "$&#123;!MasterArray[@]&#125;";do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" ssh $NODE mkdir -p /root/.kube rsync -avpt /root/pki/kube-admin.kubeconfig $NODE:/root/.kube/configdone 设置命令补全 设置kubectl 命令自动补全 123456789for NODE in "$&#123;!MasterArray[@]&#125;";do echo "--- $NODE $&#123;MasterArray[$NODE]&#125; ---" echo "--- kubectl命令自动补全 ---" ssh $NODE kubectl completion bash &gt;&gt; /etc/bash_completion.d/kubectl echo "--- kubeadm命令自动补全 ---" ssh $NODE kubeadm completion bash &gt;&gt; /etc/bash_completion.d/kubeadmdonesource /etc/bash_completion.d/kubectl 设置kubelet的bootstrap启动所需的RBAC 当集群开启了 TLS 认证后，每个节点的 kubelet 组件都要使用由 apiserver 使用的 CA 签发的有效证书才能与apiserver 通讯；此时如果节点多起来，为每个节点单独签署证书将是一件非常繁琐的事情；TLS bootstrapping 功能就是让 kubelet 先使用一个预定的低权限用户连接到 apiserver，然后向 apiserver 申请证书，kubelet 的证书由 apiserver 动态签署； 在其中一个master节点上执行就可以，以k8s-m1为例 创建工作目录12mkdir -p /root/yaml/tls-bootstrapcd /root/yaml/tls-bootstrap/ kubelet-bootstrap-rbac.yaml12345678910111213141516# 创建yaml文件cat &gt; kubelet-bootstrap-rbac.yaml &lt;&lt;EOF# 给予 kubelet-bootstrap 用户进行 node-bootstrapper 的权限apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubelet-bootstraproleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:node-bootstrappersubjects:- apiGroup: rbac.authorization.k8s.io kind: User name: kubelet-bootstrapEOF tls-bootstrap-clusterrole.yaml12345678910111213# 创建yaml文件cat &gt; tls-bootstrap-clusterrole.yaml &lt;&lt;EOF# A ClusterRole which instructs the CSR approver to approve a node requesting a# serving cert matching its client cert.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserverrules:- apiGroups: ["certificates.k8s.io"] resources: ["certificatesigningrequests/selfnodeserver"] verbs: ["create"]EOF node-client-auto-approve-csr.yaml12345678910111213141516# 创建yaml文件cat &gt; node-client-auto-approve-csr.yaml &lt;&lt;EOF# 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: node-client-auto-approve-csrroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclientsubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:bootstrappersEOF node-client-auto-renew-crt.yaml12345678910111213141516# 创建yaml文件cat &gt; node-client-auto-renew-crt.yaml &lt;&lt;EOF# 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: node-client-auto-renew-crtroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclientsubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodesEOF node-server-auto-renew-crt.yaml12345678910111213141516# 创建yaml文件cat &gt; node-server-auto-renew-crt.yaml &lt;&lt;EOF# 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: node-server-auto-renew-crtroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserversubjects:- apiGroup: rbac.authorization.k8s.io kind: Group name: system:nodesEOF 创建tls-bootstrap-rbac1kubectl apply -f . 设置kube-apiserver获取node信息的权限说明本文部署的kubelet关闭了匿名访问，因此需要额外为kube-apiserver添加权限用于访问kubelet的信息 若没添加此RBAC，则kubectl在执行logs、exec等指令的时候会提示401 Forbidden 12kubectl -n kube-system logs calico-node-pc8lq Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log calico-node-pc8lq) 参考文档：Kublet的认证授权 创建yaml文件1234567891011121314151617181920212223242526272829303132333435cat &gt; /root/yaml/apiserver-to-kubelet-rbac.yaml &lt;&lt;EOFapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: "true" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-apiserver-to-kubeletrules: - apiGroups: - "" resources: - nodes/proxy - nodes/stats - nodes/log - nodes/spec - nodes/metrics verbs: - "*"---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: system:kube-apiserver namespace: ""roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-apiserver-to-kubeletsubjects: - apiGroup: rbac.authorization.k8s.io kind: User name: kube-apiserverEOF 创建RBAC1kubectl apply -f /root/yaml/apiserver-to-kubelet-rbac.yaml kubernetes worker节点worker节点说明 安装Docker-ce，配置与master节点一致即可 安装cni-plugins、kubelet、kube-proxy 关闭防火墙和SELINUX kubelet和kube-proxy运行需要root权限 这里是以k8s-m1、k8s-m2、k8s-m3作为Work节点加入集群 kubelet 管理容器生命周期、节点状态监控 目前 kubelet 支持三种数据源来获取节点Pod信息： 本地文件 通过 url 从网络上某个地址来获取信息 API Server：从 kubernetes master 节点获取信息 使用kubeconfig与kube-apiserver通信 这里启用TLS-Bootstrap实现kubelet证书动态签署证书，并自动生成kubeconfig kube-proxy Kube-proxy是实现Service的关键插件，kube-proxy会在每台节点上执行，然后监听API Server的Service与Endpoint资源物件的改变，然后来依据变化调用相应的组件来实现网路的转发 kube-proxy可以使用userspace（基本已废弃）、iptables（默认方式）和ipvs来实现数据报文的转发 这里使用的是性能更好、适合大规模使用的ipvs 使用kubeconfig与kube-apiserver通信 切换工作目录 在k8s-m1上操作 1cd /root/worker worker组件配置模板kubelet.conf --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig指定bootstrap启动时使用的kubeconfig --network-plugin=cni定义网络插件，Pod生命周期使用此网络插件 --node-labels=node-role.kubernetes.io/node=&#39;&#39;kubelet注册当前Node时设置的Label，以key=value的格式表示，多个labe以逗号分隔 --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.1Pod的pause镜像 123456789101112131415cat &gt; kubelet.conf &lt;&lt;EOFKUBELET_ARGS=" \\--bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\--cert-dir=/etc/kubernetes/ssl \\--config=/etc/kubernetes/kubelet.config.file \\--cni-conf-dir=/etc/cni/net.d \\--cni-bin-dir=/opt/cni/bin \\--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\--logtostderr=true \\--network-plugin=cni \\--node-labels=node-role.kubernetes.io/node='' \\--pod-infra-container-image=gcrxio/pause:3.1 \\--v=2 \\"EOF kubelet.config.file1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798cat &gt; kubelet.config.file &lt;&lt;EOFapiVersion: kubelet.config.k8s.io/v1beta1kind: KubeletConfigurationaddress: 0.0.0.0authentication: # 匿名访问 anonymous: enabled: false webhook: cacheTTL: 2m0s enabled: true x509: # 这里写kubernetes-ca证书的路径 clientCAFile: /etc/kubernetes/pki/kube-ca.pemauthorization: mode: Webhook webhook: cacheAuthorizedTTL: 5m0s cacheUnauthorizedTTL: 30s# cgroups的驱动，可选systemd和cgroupfscgroupDriver: cgroupfscgroupsPerQOS: true# 指定Pod的DNS服务器IP地址clusterDNS:- 10.96.0.10# 集群的域名clusterDomain: cluster.localcontainerLogMaxFiles: 5containerLogMaxSize: 10MicontentType: application/vnd.kubernetes.protobufcpuCFSQuota: truecpuManagerPolicy: nonecpuManagerReconcilePeriod: 10senableControllerAttachDetach: trueenableDebuggingHandlers: trueenforceNodeAllocatable:- podseventBurst: 10eventRecordQPS: 5# 达到某些阈值之后，kubelet会驱逐Pod# A set of eviction thresholds (e.g. memory.available&lt;1Gi) that if met would trigger a pod eviction.# (default imagefs.available&lt;15%,memory.available&lt;100Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5%)evictionHard: imagefs.available: 15% memory.available: 1000Mi nodefs.available: 10% nodefs.inodesFree: 10%evictionPressureTransitionPeriod: 5m0s# 检测到系统已启用swap分区时kubelet会启动失败failSwapOn: false# 定义feature gatesfeatureGates: # kubelet 在证书即将到期时会自动发起一个 renew 自己证书的 CSR 请求 # 其实rotate证书已经默认开启，这里显示定义是为了方便查看 RotateKubeletClientCertificate: true RotateKubeletServerCertificate: true# 检查kubelet配置文件变更的间隔fileCheckFrequency: 20s# 允许endpoint在尝试访问自己的服务时会被负载均衡分发到自身# 可选值"promiscuous-bridge", "hairpin-veth" and "none"# 默认值为promiscuous-bridgehairpinMode: promiscuous-bridgehealthzBindAddress: 127.0.0.1healthzPort: 10248httpCheckFrequency: 20s# 这里定义容器镜像触发回收空间的上限值和下限值imageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80imageMinimumGCAge: 2m0siptablesDropBit: 15iptablesMasqueradeBit: 14kubeAPIBurst: 10kubeAPIQPS: 5makeIPTablesUtilChains: true# kubelet进程最大能打开的文件数量，默认是1000000maxOpenFiles: 1000000# 当前节点kubelet所能运行的最大Pod数量maxPods: 110# node状态上报间隔nodeStatusUpdateFrequency: 10soomScoreAdj: -999podPidsLimit: -1# kubelet服务端口port: 10250registryBurst: 10registryPullQPS: 5# 指定域名解析文件resolvConf: /etc/resolv.confrotateCertificates: trueruntimeRequestTimeout: 2m0s# 拉镜像时，同一时间只拉取一个镜像# We recommend *not* changing the default value on nodes that run docker daemon with version &lt; 1.9 or an Aufs storage backend. Issue #10959 has more details. (default true)serializeImagePulls: truestaticPodPath: /etc/kubernetes/manifestsstreamingConnectionIdleTimeout: 4h0m0ssyncFrequency: 1m0svolumeStatsAggPeriod: 1m0sEOF kube-proxy.conf123456cat &gt; kube-proxy.conf &lt;&lt;EOFKUBE_PROXY_ARGS=" \\--config=/etc/kubernetes/kube-proxy.config.file \\--v=2 \\"EOF kube-proxy.config.file12345678910111213141516171819202122232425262728293031323334353637383940414243cat &gt; kube-proxy.config.file &lt;&lt;EOFapiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationbindAddress: 0.0.0.0clientConnection: acceptContentTypes: "" burst: 10 contentType: application/vnd.kubernetes.protobuf kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig qps: 5# 集群中pod的CIDR范围，从这个范围以外发送到服务集群IP的流量将被伪装，从POD发送到外部LoadBalanceIP的流量将被定向到各自的集群IPclusterCIDR: "10.244.0.0/16"configSyncPeriod: 15m0sconntrack: max: null # 每个核心最大能跟踪的NAT连接数，默认32768 maxPerCore: 32768 min: 131072 tcpCloseWaitTimeout: 1h0m0s tcpEstablishedTimeout: 24h0m0senableProfiling: falsehealthzBindAddress: 0.0.0.0:10256hostnameOverride: ""iptables: # SNAT所有通过服务集群ip发送的通信 masqueradeAll: false masqueradeBit: 14 minSyncPeriod: 0s syncPeriod: 30sipvs: excludeCIDRs: null minSyncPeriod: 0s # ipvs调度类型，默认是rr scheduler: "rr" syncPeriod: 30smetricsBindAddress: 127.0.0.1:10249mode: "ipvs"nodePortAddresses: nulloomScoreAdj: -999portRange: ""resourceContainer: /kube-proxyudpIdleTimeout: 250msEOF systemd服务文件kubelet.service123456789101112131415161718cat &gt; kubelet.service &lt;&lt;EOF[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/kubelet.confExecStart=/usr/local/bin/kubelet \$KUBELET_ARGSRestart=on-failureKillMode=processLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF kube-proxy.service1234567891011121314151617cat &gt; kube-proxy.service &lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/kube-proxy.conf# 这里启动时使用ipvsadm将TCP的keepalive时间设置，默认是900ExecStartPre=/usr/sbin/ipvsadm --set 900 120 300 ExecStart=/usr/local/bin/kube-proxy \$KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 分发证书和kubeconfig文件 在k8s-m1上操作 在worker节点建立对应的目录 123456789101112131415for NODE in "$&#123;!WorkerArray[@]&#125;";do echo "--- $NODE ---" echo "--- 创建目录 ---" ssh $NODE mkdir -p /opt/cni/bin \ /etc/cni/net.d \ /etc/kubernetes/pki \ /etc/kubernetes/manifests \ /var/lib/kubelet rsync -avpt /root/pki/kube-proxy.kubeconfig \ /root/pki/bootstrap.kubeconfig \ $NODE:/etc/kubernetes/ rsync -avpt /root/pki/kube-ca.pem \ /root/pki/front-proxy-ca.pem \ $NODE:/etc/kubernetes/pki/done 分发二进制文件 在k8s-m1上操作 123456789for NODE in "$&#123;!WorkerArray[@]&#125;";do echo "--- $NODE ---" echo "--- 分发kubernetes二进制文件 ---" rsync -avpt /root/software/kubernetes/server/bin/kubelet \ /root/software/kubernetes/server/bin/kube-proxy \ $NODE:/usr/local/bin/ echo "--- 分发CNI-Plugins ---" rsync -avpt /root/software/cni-plugins/* $NODE:/opt/cni/bin/done 分发配置文件和服务文件123456for NODE in "$&#123;!WorkerArray[@]&#125;";do echo "--- $NODE ---" rsync -avpt kubelet.conf kubelet.config.file kube-proxy.conf kube-proxy.config.file $NODE:/etc/kubernetes/ rsync -avpt kubelet.service kube-proxy.service $NODE:/usr/lib/systemd/system/ ssh $NODE systemctl daemon-reloaddone 启动服务1234for NODE in "$&#123;!WorkerArray[@]&#125;";do echo "--- $NODE ---" ssh $NODE systemctl enable --now docker.service kubelet.service kube-proxy.servicedone 获取节点信息 此时由于未按照网络插件，所以节点状态为NotReady 123456kubectl get node -o wide# 示例输出NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-m1 NotReady node 12s v1.11.5 172.16.80.201 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.1.3.el7.x86_64 docker://18.3.1k8s-m2 NotReady node 12s v1.11.5 172.16.80.202 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.1.3.el7.x86_64 docker://18.3.1k8s-m3 NotReady node 12s v1.11.5 172.16.80.203 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.1.3.el7.x86_64 docker://18.3.1 kubernetes Core Addons网络组件部署（二选其一） 只要符合CNI规范的网络组件都可以给kubernetes使用 网络组件清单可以在这里看到Network Plugins 这里只列举kube-flannel和calico，flannel和calico的区别可以自己去找资料 网络组件只能选一个来部署 本文使用kube-flannel部署网络组件，calico已测试可用 在k8s-m1上操作 创建工作目录1mkdir -p /root/yaml/network-plugin/&#123;kube-flannel,calico&#125; kube-flannel说明 kube-flannel基于VXLAN的方式创建容器二层网络，使用端口8472/UDP通信 flannel 第一次启动时，从 etcd 获取 Pod 网段信息，为本节点分配一个未使用的 /24 段地址，然后创建 flannel.1（也可能是其它名称，如 flannel1 等） 接口。 官方提供yaml文件部署为DeamonSet 若需要使用NetworkPolicy功能，可以关注这个项目canal 架构图 切换工作目录1cd /root/yaml/network-plugin/kube-flannel 下载yaml文件1wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 官方yaml文件包含多个平台的daemonset，包括amd64、arm64、arm、ppc64le、s390x 这里以amd64作为例子，其他的可以自行根据需要修改或者直接删除不需要的daemonset 官方yaml文件已经配置好容器网络为10.244.0.0/16，这里需要跟kube-controller-manager.conf里面的--cluster-cidr匹配 如果在kube-controller-manager.conf里面把--cluster-cidr改成了其他地址段，例如192.168.0.0/16，用以下命令替换kube-flannel.yaml相应的字段 1sed -e 's,"Network": "10.244.0.0/16","Network": "192.168.0.0/16," -i kube-flannel.yml 如果服务器有多个网卡，需要指定网卡用于flannel通信，以网卡ens33为例 在args下面添加一行- --iface=ens33 123456789containers:- name: kube-flannel image: quay.io/coreos/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=ens33 修改backend flannel支持多种后端实现，可选值为VXLAN、host-gw、UDP 从性能上，host-gw是最好的，VXLAN和UDP次之 默认值是VXLAN，这里以修改为host-gw为例，位置大概在75行左右 1234567net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "host-gw" &#125; &#125; 部署kube-flannel1kubectl apply -f kube-flannel.yml 检查部署情况12345kubectl -n kube-system get pod -l k8s-app=flannelNAME READY STATUS RESTARTS AGEkube-flannel-ds-27jwl 2/2 Running 0 59skube-flannel-ds-4fgv6 2/2 Running 0 59skube-flannel-ds-mvrt7 2/2 Running 0 59s 如果等很久都没Running，可能是quay.io对你来说太慢了 可以替换一下镜像，重新apply 12sed -e 's,quay.io/coreos/,zhangguanzhang/quay.io.coreos.,g' -i kube-flannel.ymlkubectl apply -f kube-flannel.yaml Calico说明 Calico 是一款纯 Layer 3 的网络，节点之间基于BGP协议来通信。 这里以calico-v3.4.0来作为示例 部署文档 架构图 切换工作目录1cd /root/yaml/network-plugin/calico 下载yaml文件 这里使用kubernetes API来保存网络信息 12wget https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yamlwget https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calicoctl.yaml 官方yaml文件默认配置容器网络为192.168.0.0/16，这里需要跟kube-controller-manager.conf里面的--cluster-cidr匹配，需要替换相应字段 1sed -e "s,192.168.0.0/16,$&#123;POD_NET_CIDR&#125;,g" -i calico.yaml 官方yaml文件定义calicoctl为Pod，而不是deployment，所以需要调整一下 修改kind: Pod为kind: Deployment并补充其他字段 1234567891011121314151617181920212223242526272829303132333435apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: calicoctl namespace: kube-system labels: k8s-app: calicoctlspec: replicas: 1 selector: matchLabels: k8s-app: calicoctl template: metadata: name: calicoctl namespace: kube-system labels: k8s-app: calicoctl spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node.cloudprovider.kubernetes.io/uninitialized value: "true" hostNetwork: true serviceAccountName: calicoctl containers: - name: calicoctl image: quay.io/calico/ctl:v3.4.0 command: ["/bin/sh", "-c", "while true; do sleep 3600; done"] tty: true env: - name: DATASTORE_TYPE value: kubernetes 部署Calico1kubectl apply -f /root/yaml/network-plugin/calico/ 检查部署情况12345678910111213141516171819202122kubectl -n kube-system get pod -l k8s-app=calico-nodeNAME READY STATUS RESTARTS AGEcalico-node-fjcj4 2/2 Running 0 6mcalico-node-tzppt 2/2 Running 0 6mcalico-node-zdq64 2/2 Running 0 6mkubectl get pod -n kube-system -l k8s-app=calicoctlNAME READY STATUS RESTARTS AGEcalicoctl-58df8955f6-sp8q9 0/1 Running 0 38skubectl -n kube-system exec -it calicoctl-58df8955f6-sp8q9 -- /calicoctl get node -o wideNAME ASN IPV4 IPV6 k8s-m1 (unknown) 172.16.80.201/24 k8s-m2 (unknown) 172.16.80.202/24 k8s-m3 (unknown) 172.16.80.203/24kubectl -n kube-system exec -it calicoctl-58df8955f6-sp8q9 -- /calicoctl get profiles -o wideNAME LABELS kns.default map[] kns.kube-public map[] kns.kube-system map[] 如果镜像pull不下来，可以替换一下 替换完重新apply 12sed -e 's,quay.io/calico/,zhangguanzhang/quay.io.calico.,g' -i *yamlkubectl apply -f . 检查节点状态 网络组件部署完成之后，可以看到node状态已经为Ready 12345kubectl get node NAME STATUS ROLES AGE VERSIONk8s-m1 Ready node 1d v1.11.5k8s-m2 Ready node 1d v1.11.5k8s-m3 Ready node 1d v1.11.5 服务发现组件部署 kubernetes从v1.11之后，已经使用CoreDNS取代原来的KUBE DNS作为服务发现的组件 CoreDNS 是由 CNCF 维护的开源 DNS 方案，前身是 SkyDNS 在k8s-m1上操作 创建工作目录1mkdir -p /root/yaml/coredns 切换工作目录 1cd /root/yaml/coredns CoreDNS创建yaml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179cat &gt; coredns.yaml &lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: coredns namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsrules:- apiGroups: - "" resources: - endpoints - services - pods - namespaces verbs: - list - watch---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: annotations: rbac.authorization.kubernetes.io/autoupdate: "true" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:corednsroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:corednssubjects:- kind: ServiceAccount name: coredns namespace: kube-system---apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors log health kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload loadbalance &#125;---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: "CoreDNS"spec: replicas: 2 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: "" labels: k8s-app: kube-dns spec: serviceAccountName: coredns priorityClassName: system-cluster-critical tolerations: - key: CriticalAddonsOnly operator: Exists - effect: NoSchedule key: node-role.kubernetes.io/master containers: - name: coredns image: gcrxio/coredns:1.2.6 imagePullPolicy: IfNotPresent args: [ "-conf", "/etc/coredns/Corefile" ] livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 10 successThreshold: 1 failureThreshold: 5 ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP resources: limits: memory: 200Mi requests: cpu: 100m memory: 70Mi securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true - name: host-time mountPath: /etc/localtime dnsPolicy: Default volumes: - name: host-time hostPath: path: /etc/localtime - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile---apiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: "9153" prometheus.io/scrape: "true" labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" kubernetes.io/name: "CoreDNS"spec: selector: k8s-app: kube-dns clusterIP: $&#123;POD_DNS_SERVER_IP&#125; ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCPEOF 修改yaml文件 yaml文件里面定义了clusterIP这里需要与kubelet.config.file里面定义的cluster-dns一致 如果kubelet.conf里面的--cluster-dns改成别的，例如x.x.x.x，这里也要做相应变动，不然Pod找不到DNS，无法正常工作 这里定义静态的hosts解析，这样Pod可以通过hostname来访问到各节点主机 用下面的命令根据HostArray的信息生成静态的hosts解析 12345678sed -e '57r '&lt;(\ echo ' hosts &#123;'; \ for NODE in "$&#123;!HostArray[@]&#125;";do \ echo " $&#123;HostArray[$NODE]&#125; $NODE"; \ done;\ echo ' fallthrough'; \ echo ' &#125;';) \-i coredns.yaml 上面的命令的作用是，通过HostArray的信息生成hosts解析配置，顺序是打乱的，可以手工调整顺序 也可以手动修改coredns.yaml文件来添加对应字段 12345678910111213141516171819202122232425262728apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors log health kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure upstream fallthrough in-addr.arpa ip6.arpa &#125; hosts &#123; 172.16.80.202 k8s-m2 172.16.80.203 k8s-m3 172.16.80.201 k8s-m1 fallthrough in-addr.arpa ip6.arpa &#125; prometheus :9153 proxy . /etc/resolv.conf cache 30 reload loadbalance &#125; 部署CoreDNS1kubectl apply -f coredns.yaml 检查部署状态1234kubectl -n kube-system get pod -l k8s-app=kube-dnsNAME READY STATUS RESTARTS AGEcoredns-5566c96697-6gzzc 1/1 Running 0 45scoredns-5566c96697-q5slk 1/1 Running 0 45s 验证集群DNS服务 创建一个deployment测试DNS解析 123456789101112131415161718192021222324252627282930# 创建一个基于busybox的deploymentcat &gt; /root/yaml/busybox-deployment.yaml &lt;&lt;EOFapiVersion: apps/v1kind: Deploymentmetadata: labels: app: busybox name: busybox namespace: defaultspec: replicas: 1 selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: busybox imagePullPolicy: IfNotPresent image: busybox:1.26 command: - sleep - "3600"EOF# 基于文件创建deploymentkubectl apply -f /root/yaml/busybox-deployment.yaml 检查deployment部署情况 123kubectl get podNAME READY STATUS RESTARTS AGEbusybox-7b9bfb5658-872gj 1/1 Running 0 6s 验证集群DNS解析 上一个命令获取到pod名字为busybox-7b9bfb5658-872gj 通过kubectl命令连接到Pod运行nslookup命令测试使用域名来访问kube-apiserver和各节点主机 123456789101112131415161718192021222324252627282930313233echo "--- 通过CoreDNS访问kubernetes ---"kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup kubernetes# 示例输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.localecho "--- 通过CoreDNS访问k8s-m1 ---"# 示例输出kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup k8s-m1Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: k8s-m1Address 1: 172.16.80.201 k8s-m1echo "--- 通过CoreDNS访问k8s-m2 ---"kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup k8s-m2# 示例输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: k8s-n2Address 1: 172.16.80.202 k8s-m2echo "--- 通过CoreDNS访问并不存在的k8s-n3 ---"kubectl exec -it busybox-7b9bfb5658-4cz94 -- nslookup k8s-n3# 示例输出Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localnslookup: can't resolve 'k8s-n3' Metrics Server Metrics Server是实现了 Metrics API 的元件,其目标是取代 Heapster 作位 Pod 与 Node 提供资源的 Usagemetrics,该元件会从每个 Kubernetes 节点上的 Kubelet 所公开的 Summary API 中收集 Metrics Horizontal Pod Autoscaler（HPA）控制器用于实现基于CPU使用率进行自动Pod伸缩的功能。 HPA控制器基于Master的kube-controller-manager服务启动参数–horizontal-pod-autoscaler-sync-period定义是时长（默认30秒）,周期性监控目标Pod的CPU使用率,并在满足条件时对ReplicationController或Deployment中的Pod副本数进行调整,以符合用户定义的平均PodCPU使用率。 在新版本的kubernetes中 Pod CPU使用率不在来源于heapster,而是来自于metrics-server 官网原话是 The –horizontal-pod-autoscaler-use-rest-clients is true or unset. Setting this to false switches to Heapster-based autoscaling, which is deprecated. 在k8s-m1上操作 额外参数 设置kube-apiserver参数，这里在配置kube-apiserver阶段已经加进去了 front-proxy证书，在证书生成阶段已经完成且已分发 1234567--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.pem--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client-key.pem--requestheader-allowed-names=aggregator--requestheader-group-headers=X-Remote-Group--requestheader-extra-headers-prefix=X-Remote-Extra---requestheader-username-headers=X-Remote-User 创建工作目录1mkdir -p /root/yaml/metrics-server 切换工作目录1cd /root/yaml/metrics-server 下载yaml文件123456wget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/aggregated-metrics-reader.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/auth-delegator.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/auth-reader.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/metrics-apiservice.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/metrics-server-service.yamlwget https://raw.githubusercontent.com/kubernetes-incubator/metrics-server/master/deploy/1.8%2B/resource-reader.yaml 创建metrics-server-deployment.yaml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950cat &gt; metrics-server-deployment.yaml &lt;&lt;EOF---apiVersion: v1kind: ServiceAccountmetadata: name: metrics-server namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-serverspec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: ca-ssl hostPath: path: /etc/kubernetes/pki containers: - name: metrics-server image: gcrxio/metrics-server-amd64:v0.3.1 imagePullPolicy: IfNotPresent command: - /metrics-server - --metric-resolution=30s - --kubelet-port=10250 - --kubelet-preferred-address-types=InternalDNS,InternalIP,ExternalDNS,ExternalIP,Hostname - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.pem - --requestheader-username-headers=X-Remote-User - --requestheader-group-headers=X-Remote-Group - --requestheader-extra-headers-prefix=X-Remote-Extra- - --kubelet-insecure-tls - -v=2 volumeMounts: - name: ca-ssl mountPath: /etc/kubernetes/pkiEOF 部署metrics-server1kubectl apply -f . 查看pod状态123kubectl -n kube-system get pod -l k8s-app=metrics-serverNAME READY STATUS RESTARTS AGEpod/metrics-server-86bd9d7667-5hbn6 1/1 Running 0 1m 验证metrics 完成后,等待一段时间(约 30s - 1m)收集 Metrics 12345678910111213# 请求metrics api的结果kubectl get --raw /apis/metrics.k8s.io/v1beta1&#123;"kind":"APIResourceList","apiVersion":"v1","groupVersion":"metrics.k8s.io/v1beta1","resources":[&#123;"name":"nodes","singularName":"","namespaced":false,"kind":"NodeMetrics","verbs":["get","list"]&#125;,&#123;"name":"pods","singularName":"","namespaced":true,"kind":"PodMetrics","verbs":["get","list"]&#125;]&#125;kubectl get apiservice|grep metricsv1beta1.metrics.k8s.io 2018-12-09T08:17:26Z# 获取节点性能信息kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-m1 113m 2% 1080Mi 14% k8s-m2 133m 3% 1086Mi 14% k8s-m3 100m 2% 1029Mi 13% 至此集群已具备基本功能 下面的Extra Addons就是一些额外的功能 kubernetes Extra AddonsDashboard Dashboard 是kubernetes社区提供的GUI界面，用于图形化管理kubernetes集群，同时可以看到资源报表。 官方提供yaml文件直接部署，但是需要更改image以便国内部署 在k8s-m1上操作 创建工作目录1mkdir -p /root/yaml/kubernetes-dashboard 切换工作目录1cd /root/yaml/kubernetes-dashboard 获取yaml文件1wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml 修改镜像地址1sed -e 's,k8s.gcr.io/kubernetes-dashboard-amd64,gcrxio/kubernetes-dashboard-amd64,g' -i kubernetes-dashboard.yaml 创建kubernetes-Dashboard1kubectl apply -f kubernetes-dashboard.yaml 创建ServiceAccount RBAC 官方的yaml文件，ServiceAccount绑定的RBAC权限很低，很多资源无法查看 需要创建一个用于管理全局的ServiceAccount 123456789101112131415161718192021222324252627cat &gt; cluster-admin.yaml &lt;&lt;EOF---# 在kube-system中创建名为admin-user的ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---# 将admin-user和cluster-admin绑定在一起# cluster-admin是kubernetes内置的clusterrole，具有集群管理员权限# 其他内置的clusterrole可以通过kubectl get clusterrole查看apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-systemEOFkubectl apply -f cluster-admin.yaml 获取ServiceAccount的Token1kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '&#123;print $1&#125;') 查看部署情况1kubectl get all -n kube-system --selector k8s-app=kubernetes-dashboard 访问Dashboard kubernetes-dashborad的svc默认是clusterIP，需要修改为nodePort才能被外部访问 随机分配NodePort，分配范围由kube-apiserver的--service-node-port-range参数指定 1kubectl patch -n kube-system svc kubernetes-dashboard -p '&#123;"spec":&#123;"type":"NodePort"&#125;&#125;' 修改完之后，通过以下命令获取访问kubernetes-Dashboard的端口 123kubectl -n kube-system get svc --selector k8s-app=kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.106.183.192 &lt;none&gt; 443:30216/TCP 12s 可以看到已经将节点的30216端口暴露出来 IP地址不固定，只要运行了kube-proxy组件，都会在节点上添加30216端口规则用于转发请求到Pod https://172.16.80.200:30216 https://172.16.80.201:30216 https://172.16.80.202:30216 https://172.16.80.203:30216 登录Dashboard，上面已经获取了token，这里只需要把token的值填入输入框，点击SIGN IN即可登录 Dashboard UI预览图 Ingress Controller Ingress 是 Kubernetes 中的一个抽象资源，其功能是通过 Web Server 的 Virtual Host概念以域名(Domain Name)方式转发到內部 Service，这避免了使用 Service 中的 NodePort 与LoadBalancer 类型所带來的限制(如 Port 数量上限)，而实现 Ingress 功能则是通过 Ingress Controller来达成，它会负责监听 Kubernetes API 中的 Ingress 与 Service 资源，并在发生资源变化时，根据资源预期的结果来设置 Web Server。 Ingress Controller 有许多实现可以选择，这里只是列举一小部分 Ingress NGINX：Kubernetes 官方维护的方案，本次安装使用此方案 kubernetes-ingress：由nginx社区维护的方案，使用社区版nginx和nginx-plus treafik：一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置 在k8s-m1上操作 创建工作目录1mkdir -p /root/yaml/ingress/ingress-nginx 切换工作目录1cd /root/yaml/ingress/ingress-nginx 下载yaml文件12wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.20.0/deploy/mandatory.yamlwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.20.0/deploy/provider/baremetal/service-nodeport.yaml 修改镜像地址123sed -e 's,k8s.gcr.io/,zhangguanzhang/gcr.io.google_containers.,g' \ -e 's,quay.io/kubernetes-ingress-controller/,zhangguanzhang/quay.io.kubernetes-ingress-controller.,g' \ -i mandatory.yaml 创建ingress-nginx1kubectl apply -f . 检查部署情况1kubectl -n ingress-nginx get pod 访问ingress 默认的backend会返回404 123456789kubectl -n ingress-nginx get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.96.250.140 &lt;none&gt; 80:32603/TCP,443:30083/TCP 1mcurl http://172.16.80.200:32603default backend - 404curl -k https://172.16.80.200:30083default backend - 404 注意 这里部署之后，是deployment，且通过nodePort暴露服务 也可以修改yaml文件，将Ingress-nginx部署为DaemonSet 使用labels和nodeSelector来指定运行ingress-nginx的节点 使用hostNetwork=true来共享主机网络命名空间，或者使用hostPort指定主机端口映射 如果使用hostNetwork共享宿主机网络栈或者hostPort映射宿主机端口，记得要看看有没有端口冲突，否则无法启动 修改监听端口可以在ingress-nginx启动命令中添加--http-port=8180和--https-port=8543，还有下面的端口定义也相应变更即可 创建kubernetes-Dashboard的Ingress kubernetes-Dashboard默认是开启了HTTPS访问的 ingress-nginx需要以HTTPS的方式反向代理kubernetes-Dashboard 以HTTP方式访问kubernetes-Dashboard的时候会被重定向到HTTPS 需要创建HTTPS证书，用于访问ingress-nginx的HTTPS端口 创建HTTPS证书 这里的CN=域名/O=域名需要跟后面的ingress主机名匹配 1234567openssl req -x509 \ -nodes \ -days 3650 \ -newkey rsa:2048 \ -keyout tls.key \ -out tls.crt \ -subj "/CN=dashboard.k8s.local/O=dashboard.k8s.local" 创建secret对象 这里将HTTPS证书创建为kubernetes的secret对象dashboard-tls ingress创建的时候需要加载这个作为HTTPS证书 1kubectl -n kube-system create secret tls dashboard-tls --key ./tls.key --cert ./tls.crt 创建dashboard-ingress.yaml123456789101112131415161718192021apiVersion: extensions/v1beta1kind: Ingressmetadata: name: dashboard-ingress namespace: kube-system annotations: nginx.ingress.kubernetes.io/ssl-passthrough: "true" nginx.ingress.kubernetes.io/secure-backends: "true"spec: tls: - hosts: - dashboard.k8s.local secretName: dashboard-tls rules: - host: dashboard.k8s.local http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443 创建ingress1kubectl apply -f dashboard-ingress.yaml 检查ingress123kubectl -n kube-system get ingressNAME HOSTS ADDRESS PORTS AGEdashboard-ingress dashboard.k8s.local 80, 443 16m 访问kubernetes-Dashboard 修改主机hosts静态域名解析，以本文为例在hosts文件里添加172.16.80.200 dashboard.k8s.local 使用https://dashboard.k8s.local:30083访问kubernetesDashboard了 添加了TLS之后，访问HTTP会被跳转到HTTPS端口，这里比较坑爹，没法自定义跳转HTTPS的端口 此处使用的是自签名证书，浏览器会提示不安全，请忽略 建议搭配external-DNS和LoadBalancer一起食用，效果更佳 Helm Helm是一个kubernetes应用的包管理工具，用来管理charts——预先配置好的安装包资源，有点类似于Ubuntu的APT和CentOS中的yum。 Helm chart是用来封装kubernetes原生应用程序的yaml文件，可以在你部署应用的时候自定义应用程序的一些metadata，便与应用程序的分发。 Helm和charts的主要作用： 应用程序封装 版本管理 依赖检查 便于应用程序分发 环境要求 kubernetes v1.6及以上的版本，启用RBAC 集群可以访问到chart仓库 helm客户端主机能访问kubernetes集群 安装客户端 安装方式二选一，需要科学上网 直接脚本安装12echo '--- 使用脚本安装，默认是最新版 ---'curl https://raw.githubusercontent.com/helm/helm/master/scripts/get | bash 下载二进制文件安装12345echo '--- 下载二进制文件安装 ---'wget https://storage.googleapis.com/kubernetes-helm/helm-v2.12.0-linux-amd64.tar.gztar xzf helm-v2.12.0-linux-amd64.tar.gz linux-amd64/helmmv linux-amd64/helm /usr/local/bin/rm -rf linux-amd64 创建工作目录1mkdir /root/yaml/helm/ 切换工作目录1cd /root/yaml/helm 创建RBAC规则123456789101112131415161718192021222324cat &gt; /root/yaml/helm/helm-rbac.yaml &lt;&lt;EOF# 创建名为tiller的ServiceAccountapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---# 给tiller绑定cluster-admin权限apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tiller-cluster-ruleroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: tiller namespace: kube-systemEOFkubectl apply -f /root/yaml/helm/helm-rbac.yaml 安装服务端 这里指定了helm的stable repo国内镜像地址 具体说明请看这里 123helm init --tiller-image gcrxio/tiller:v2.12.0 \ --service-account tiller \ --stable-repo-url http://mirror.azure.cn/kubernetes/charts/ 检查安装情况123456789kubectl -n kube-system get pod -l app=helm,name=tiller# 输出示例NAME READY STATUS RESTARTS AGEtiller-deploy-84fc6cd5f9-nz4m7 1/1 Running 0 1mhelm version# 输出示例Client: &amp;version.Version&#123;SemVer:"v2.12.0", GitCommit:"d325d2a9c179b33af1a024cdb5a4472b6288016a", GitTreeState:"clean"&#125;Server: &amp;version.Version&#123;SemVer:"v2.12.0", GitCommit:"d325d2a9c179b33af1a024cdb5a4472b6288016a", GitTreeState:"clean"&#125; 添加命令行补全12helm completion bash &gt; /etc/bash_completion.d/helmsource /etc/bash_completion.d/helm Rook（测试用途）说明 Rook是一款云原生环境下的开源分布式存储编排系统，目前已进入CNCF孵化。Rook的官方网站是https://rook.io Rook将分布式存储软件转变为自我管理，自我缩放和自我修复的存储服务。它通过自动化部署，引导、配置、供应、扩展、升级、迁移、灾难恢复、监控和资源管理来实现。 Rook使用基础的云原生容器管理、调度和编排平台提供的功能来履行其职责。 Rook利用扩展点深入融入云原生环境，为调度、生命周期管理、资源管理、安全性、监控和用户体验提供无缝体验。 Ceph Custom Resource Definition（CRD）已经在Rook v0.8版本升级到Beta 其他特性请查看项目文档 这里只用作测试环境中提供StorageClass和持久化存储 请慎重考虑是否部署在生产环境中 Rook与kubernetes的集成 Rook架构图 安装 这里以Rook v0.8.3作为示例 这里默认使用/var/lib/rook/osd*目录来运行OSD 需要最少3个节点，否则无足够的节点启动集群 可以使用yaml文件部署和使用helm chart部署，这里使用yaml文件部署 创建工作目录1mkdir -p /root/yaml/rook/ 进入工作目录1cd /root/yaml/rook/ 下载yaml文件1234# operator实现自定义API用于管理rook-cephwget https://raw.githubusercontent.com/rook/rook/v0.8.3/cluster/examples/kubernetes/ceph/operator.yaml# cluster用于部署rook-ceph集群wget https://raw.githubusercontent.com/rook/rook/v0.8.3/cluster/examples/kubernetes/ceph/cluster.yaml 部署operator1kubectl apply -f operator.yaml 检查operator安装情况1234567891011121314151617181920kubectl -n rook-ceph-system get all# 输出示例NAME READY STATUS RESTARTS AGEpod/rook-ceph-agent-4qwvd 1/1 Running 0 11mpod/rook-ceph-agent-v5ghj 1/1 Running 0 11mpod/rook-ceph-agent-zv8s6 1/1 Running 0 11mpod/rook-ceph-operator-745f756bd8-9gdpk 1/1 Running 0 12mpod/rook-discover-44lx5 1/1 Running 0 11mpod/rook-discover-4d6mn 1/1 Running 0 11mpod/rook-discover-mvqfv 1/1 Running 0 11mNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdaemonset.apps/rook-ceph-agent 3 3 3 3 3 &lt;none&gt; 11mdaemonset.apps/rook-discover 3 3 3 3 3 &lt;none&gt; 11mNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeployment.apps/rook-ceph-operator 1 1 1 1 12mNAME DESIRED CURRENT READY AGEreplicaset.apps/rook-ceph-operator-745f756bd8 1 1 1 12m 部署cluster1kubectl apply -f cluster.yaml 检查cluster部署情况1234567891011121314151617181920212223242526272829303132333435363738394041kubectl -n rook-ceph get all# 输出示例NAME READY STATUS RESTARTS AGEpod/rook-ceph-mgr-a-7944d8d79b-pvrsf 1/1 Running 0 10mpod/rook-ceph-mon0-ll7fc 1/1 Running 0 11mpod/rook-ceph-mon1-cd2gb 1/1 Running 0 11mpod/rook-ceph-mon2-vlmfc 1/1 Running 0 10mpod/rook-ceph-osd-id-0-745486df7b-4dxdc 1/1 Running 0 10mpod/rook-ceph-osd-id-1-85fdf4cd64-ftmc4 1/1 Running 0 10mpod/rook-ceph-osd-id-2-6bc4fbb457-295pn 1/1 Running 0 10mpod/rook-ceph-osd-prepare-k8s-m1-klv5j 0/1 Completed 0 10mpod/rook-ceph-osd-prepare-k8s-m2-dt2pl 0/1 Completed 0 10mpod/rook-ceph-osd-prepare-k8s-m3-ndqpl 0/1 Completed 0 10mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/rook-ceph-mgr ClusterIP 10.100.158.219 &lt;none&gt; 9283/TCP 10mservice/rook-ceph-mgr-dashboard ClusterIP 10.107.141.138 &lt;none&gt; 7000/TCP 10mservice/rook-ceph-mgr-dashboard-external NodePort 10.99.89.12 &lt;none&gt; 7000:30660/TCP 10mservice/rook-ceph-mon0 ClusterIP 10.100.50.229 &lt;none&gt; 6790/TCP 11mservice/rook-ceph-mon1 ClusterIP 10.110.105.207 &lt;none&gt; 6790/TCP 11mservice/rook-ceph-mon2 ClusterIP 10.103.223.166 &lt;none&gt; 6790/TCP 10mNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeployment.apps/rook-ceph-mgr-a 1 1 1 1 10mdeployment.apps/rook-ceph-osd-id-0 1 1 1 1 10mdeployment.apps/rook-ceph-osd-id-1 1 1 1 1 10mdeployment.apps/rook-ceph-osd-id-2 1 1 1 1 10mNAME DESIRED CURRENT READY AGEreplicaset.apps/rook-ceph-mgr-a-7944d8d79b 1 1 1 10mreplicaset.apps/rook-ceph-mon0 1 1 1 11mreplicaset.apps/rook-ceph-mon1 1 1 1 11mreplicaset.apps/rook-ceph-mon2 1 1 1 10mreplicaset.apps/rook-ceph-osd-id-0-745486df7b 1 1 1 10mreplicaset.apps/rook-ceph-osd-id-1-85fdf4cd64 1 1 1 10mreplicaset.apps/rook-ceph-osd-id-2-6bc4fbb457 1 1 1 10mNAME DESIRED SUCCESSFUL AGEjob.batch/rook-ceph-osd-prepare-k8s-m1 1 1 10mjob.batch/rook-ceph-osd-prepare-k8s-m2 1 1 10mjob.batch/rook-ceph-osd-prepare-k8s-m3 1 1 10m 检查ceph集群状态 上面命令已经获取ceph-mon0节点的pod名rook-ceph-mon0-ll7fc，以此pod为例运行以下命令 12345678910111213141516kubectl -n rook-ceph exec -it rook-ceph-mon0-ll7fc -- ceph -s# 输出示例 cluster: id: 1fcee02c-fd98-4b13-bfed-de7b6605a237 health: HEALTH_OK services: mon: 3 daemons, quorum rook-ceph-mon0,rook-ceph-mon2,rook-ceph-mon1 mgr: a(active) osd: 3 osds: 3 up, 3 in data: pools: 1 pools, 100 pgs objects: 0 objects, 0 bytes usage: 22767 MB used, 96979 MB / 116 GB avail pgs: 100 active+clean 暴露ceph-mgr的dashboard12wget https://raw.githubusercontent.com/rook/rook/v0.8.3/cluster/examples/kubernetes/ceph/dashboard-external.yamlkubectl apply -f dashboard-external.yaml 访问已暴露的dashboard123456789kubectl -n rook-ceph get svc# 输出示例NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGErook-ceph-mgr ClusterIP 10.100.158.219 &lt;none&gt; 9283/TCP 12mrook-ceph-mgr-dashboard ClusterIP 10.107.141.138 &lt;none&gt; 7000/TCP 12mrook-ceph-mgr-dashboard-external NodePort 10.99.89.12 &lt;none&gt; 7000:30660/TCP 11mrook-ceph-mon0 ClusterIP 10.100.50.229 &lt;none&gt; 6790/TCP 13mrook-ceph-mon1 ClusterIP 10.110.105.207 &lt;none&gt; 6790/TCP 13mrook-ceph-mon2 ClusterIP 10.103.223.166 &lt;none&gt; 6790/TCP 12m 可以见到这里暴露30660端口，通过此端口可以访问Dashboard 添加StorageClass 添加多副本存储池 注释部分是创建纠删码存储池 添加StorageClass指定使用多副本存储池，格式化为xfs 12345678910111213141516171819202122232425262728293031cat &gt; rbd-storageclass.yaml &lt;&lt;EOFapiVersion: ceph.rook.io/v1beta1kind: Poolmetadata: name: replicapool namespace: rook-cephspec: replicated: size: 3 # For an erasure-coded pool, comment out the replication size above and uncomment the following settings. # Make sure you have enough OSDs to support the replica size or erasure code chunks. #erasureCoded: # dataChunks: 2 # codingChunks: 1---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: rook-ceph-blockprovisioner: ceph.rook.io/blockparameters: pool: replicapool # Specify the namespace of the rook cluster from which to create volumes. # If not specified, it will use `rook` as the default namespace of the cluster. # This is also the namespace where the cluster will be clusterNamespace: rook-ceph # Specify the filesystem type of the volume. If not specified, it will use `ext4`. fstype: xfsEOFkubectl apply -f rbd-storageclass.yaml 还可以添加cephFS和object类型的存储池，然后创建对应的StorageClass 具体可以看filesystem.yaml和object.yaml 检查StorageClass 创建sc时，会在rook-ceph上创建对应的Pool 这里以rbd-storageclass.yaml为例 123456789101112131415161718192021222324252627kubectl get sc# 输出示例NAME PROVISIONER AGErook-ceph-block ceph.rook.io/block 15mkubectl describe sc rook-ceph-block # 输出示例Name: rook-ceph-blockIsDefaultClass: NoAnnotations: kubectl.kubernetes.io/last-applied-configuration=&#123;"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":&#123;"annotations":&#123;&#125;,"name":"rook-ceph-block","namespace":""&#125;,"parameters":&#123;"clusterNamespace":"rook-ceph","fstype":"xfs","pool":"replicapool"&#125;,"provisioner":"ceph.rook.io/block"&#125;Provisioner: ceph.rook.io/blockParameters: clusterNamespace=rook-ceph,fstype=xfs,pool=replicapoolAllowVolumeExpansion: &lt;unset&gt;MountOptions: &lt;none&gt;ReclaimPolicy: DeleteVolumeBindingMode: ImmediateEvents: &lt;none&gt;kubectl -n rook-ceph exec -it rook-ceph-mon0-ll7fc -- ceph df# 输出示例GLOBAL: SIZE AVAIL RAW USED %RAW USED 116G 96979M 22767M 19.01 POOLS: NAME ID USED %USED MAX AVAIL OBJECTS replicapool 1 0 0 29245M 0 卸载Rook-ceph 这里提供卸载的操作步骤，请按需操作！ 删除StorageClass1kubectl delete -f rbd-storageclass.yaml 删除Rook-Ceph-Cluster1kubectl delete -f cluster.yaml 删除Rook-Operator1kubectl delete -f operator.yaml 清理目录 注意！这里是所有运行rook-ceph集群的节点都需要做清理 1rm -rf /var/lib/rook Prometheus Operator说明 Prometheus Operator 是 CoreOS 开发的基于 Prometheus 的 Kubernetes 监控方案，也可能是目前功能最全面的开源方案。 Prometheus Operator 通过 Grafana 展示监控数据，预定义了一系列的 Dashboard 要求kubernetes版本大于等于1.8.0 CoreOS/Prometheus-Operator项目地址 Prometheus Prometheus 是一套开源的系统监控报警框架，启发于 Google 的 borgmon 监控系统，作为社区开源项目进行开发，并成为CNCF第二个毕业的项目（第一个是kubernetes） 特点 强大的多维度数据模型 灵活而强大的查询语句（PromQL） 易于管理，高效 使用 pull 模式采集时间序列数据，这样不仅有利于本机测试而且可以避免有问题的服务器推送坏的 metrics。 可以采用 push gateway 的方式把时间序列数据推送至 Prometheus server 端 可以通过服务发现或者静态配置去获取监控的 targets 有多种可视化图形界面 易于伸缩 Prometheus组成架构 Prometheus Server: 用于收集和存储时间序列数据 Client Library: 客户端库，为需要监控的服务生成相应的 metrics 并暴露给Prometheus server Push Gateway: 主要用于短期的 jobs。 jobs 可以直接向 Prometheus server 端推送它们的metrics。这种方式主要用于服务层面的 metrics。 Exporters: 用于暴露已有的第三方服务的 metrics 给 Prometheus。 Alertmanager: 从 Prometheus server 端接收到 alerts后，会进行去除重复数据，分组，并路由到对收的接受方式，发出报警。 架构图 Operator架构 Operator 即 Prometheus Operator，在 Kubernetes 中以 Deployment 运行。其职责是部署和管理Prometheus Server，根据 ServiceMonitor 动态更新 Prometheus Server 的监控对象。 Prometheus Server Prometheus Server 会作为 Kubernetes 应用部署到集群中。为了更好地在 Kubernetes 中管理 Prometheus，CoreOS 的开发人员专门定义了一个命名为 Prometheus 类型的 Kubernetes 定制化资源。我们可以把 Prometheus看作是一种特殊的 Deployment，它的用途就是专门部署 Prometheus Server。 Service 这里的Service 就是 Cluster 中的 Service 资源，也是 Prometheus 要监控的对象，在 Prometheus 中叫做Target。每个监控对象都有一个对应的 Service。比如要监控 Kubernetes Scheduler，就得有一个与 Scheduler对应的 Service。当然，Kubernetes 集群默认是没有这个 Service 的，Prometheus Operator会负责创建。 ServiceMonitor Operator能够动态更新 Prometheus 的 Target 列表，ServiceMonitor 就是 Target 的抽象。比如想监控Kubernetes Scheduler，用户可以创建一个与 Scheduler Service 相映射的 ServiceMonitor对象。Operator 则会发现这个新的 ServiceMonitor，并将 Scheduler 的 Target 添加到 Prometheus的监控列表中。 ServiceMonitor 也是 Prometheus Operator 专门开发的一种 Kubernetes 定制化资源类型。 Alertmanager 除了 Prometheus 和 ServiceMonitor，Alertmanager 是 Operator 开发的第三种 Kubernetes 定制化资源。我们可以把 Alertmanager 看作是一种特殊的 Deployment，它的用途就是专门部署 Alertmanager 组件。 部署Prometheus-Operator切换工作目录12mkdir -p /root/yaml/prometheus-operatorcd /root/yaml/prometheus-operator 添加coreos源12# 添加coreos源helm repo add coreos https://s3-eu-west-1.amazonaws.com/coreos-charts/stable/ 创建命名空间1kubectl create namespace monitoring 部署prometheus-operator 这里通过--set指定了image的地址 1234567helm install coreos/prometheus-operator \ --name coreos-prometheus-operator \ --namespace monitoring \ --set global.hyperkube.repository=zhangguanzhang/quay.io.coreos.hyperkube \ --set image.repository=zhangguanzhang/quay.io.coreos.prometheus-operator \ --set prometheusConfigReloader.repository=zhangguanzhang/quay.io.coreos.prometheus-config-reloader \ --set rbacEnable=true 部署kube-prometheus 通过运行helm命令安装时，指定一些变量来达到自定义配置的目的 定义grafana初始admin密码为password，默认值是admin 定义alertmanager和prometheus使用名为rook-ceph-block的StorageClass，访问模式为ReadWriteOnce，大小5Gi，默认是50Gi 定义grafana、alertmanager、prometheus的Service类型为NodePort，默认是ClusterIP 这里的--set可以定义很多变量，具体可以在这里，查看里面每个文件夹的values.yaml 这里配置的变量请自己根据情况修改 12345678910111213141516171819202122232425262728293031helm install coreos/kube-prometheus \ --name kube-prometheus \ --namespace monitoring \ --set alertmanager.image.repository="zhangguanzhang/quay.io.prometheus.alertmanager" \ --set alertmanager.service.type="NodePort" \ --set alertmanager.storageSpec.volumeClaimTemplate.spec.storageClassName="rook-ceph-block" \ --set alertmanager.storageSpec.volumeClaimTemplate.spec.accessModes[0]="ReadWriteOnce" \ --set alertmanager.storageSpec.volumeClaimTemplate.spec.resources.requests.storage="5Gi" \ --set grafana.adminPassword="password" \ --set grafana.service.type="NodePort" \ --set prometheus.image.repository="zhangguanzhang/quay.io.prometheus.prometheus" \ --set prometheus.service.type="NodePort" \ --set prometheus.storageSpec.volumeClaimTemplate.spec.storageClassName="rook-ceph-block" \ --set prometheus.storageSpec.volumeClaimTemplate.spec.accessModes[0]="ReadWriteOnce" \ --set prometheus.storageSpec.volumeClaimTemplate.spec.resources.requests.storage="5Gi" \ --set prometheus.deployCoreDNS=true \ --set prometheus.deployKubeDNS=false \ --set prometheus.deployKubeEtcd=true \ --set exporter-kube-controller-manager.endpoints[0]="172.16.80.201" \ --set exporter-kube-controller-manager.endpoints[1]="172.16.80.202" \ --set exporter-kube-controller-manager.endpoints[2]="172.16.80.203" \ --set exporter-kube-etcd.etcdPort=2379 \ --set exporter-kube-etcd.scheme="https" \ --set exporter-kube-etcd.endpoints[0]="172.16.80.201" \ --set exporter-kube-etcd.endpoints[1]="172.16.80.202" \ --set exporter-kube-etcd.endpoints[2]="172.16.80.203" \ --set exporter-kube-scheduler.endpoints[0]="172.16.80.201" \ --set exporter-kube-scheduler.endpoints[1]="172.16.80.202" \ --set exporter-kube-scheduler.endpoints[2]="172.16.80.203" \ --set exporter-kube-state.kube_state_metrics.image.repository="gcrxio/kube-state-metrics" \ --set exporter-kube-state.addon_resizer.image.repository="gcrxio/addon-resizer" 检查部署情况1234567891011121314151617181920212223242526272829303132333435363738kubectl -n monitoring get all# 输出示例NAME READY STATUS RESTARTS AGEpod/alertmanager-kube-prometheus-0 2/2 Running 0 43mpod/kube-prometheus-exporter-kube-state-66b8849c9b-cq5pp 2/2 Running 0 42mpod/kube-prometheus-exporter-node-p6z67 1/1 Running 0 43mpod/kube-prometheus-exporter-node-qnmjt 1/1 Running 0 43mpod/kube-prometheus-exporter-node-vr4sp 1/1 Running 0 43mpod/kube-prometheus-grafana-f869c754-x5x7n 2/2 Running 0 43mpod/prometheus-kube-prometheus-0 3/3 Running 1 43mpod/prometheus-operator-5db9df7ffc-dxtqh 1/1 Running 0 49mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/alertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,6783/TCP 43mservice/kube-prometheus NodePort 10.97.183.252 &lt;none&gt; 9090:30900/TCP 43mservice/kube-prometheus-alertmanager NodePort 10.105.140.173 &lt;none&gt; 9093:30903/TCP 43mservice/kube-prometheus-exporter-kube-state ClusterIP 10.108.236.146 &lt;none&gt; 80/TCP 43mservice/kube-prometheus-exporter-node ClusterIP 10.96.14.75 &lt;none&gt; 9100/TCP 43mservice/kube-prometheus-grafana NodePort 10.109.4.170 &lt;none&gt; 80:30164/TCP 43mservice/prometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 43mNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdaemonset.apps/kube-prometheus-exporter-node 3 3 3 3 3 &lt;none&gt; 43mNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeployment.apps/kube-prometheus-exporter-kube-state 1 1 1 1 43mdeployment.apps/kube-prometheus-grafana 1 1 1 1 43mdeployment.apps/prometheus-operator 1 1 1 1 49mNAME DESIRED CURRENT READY AGEreplicaset.apps/kube-prometheus-exporter-kube-state-658f46b8dd 0 0 0 43mreplicaset.apps/kube-prometheus-exporter-kube-state-66b8849c9b 1 1 1 42mreplicaset.apps/kube-prometheus-grafana-f869c754 1 1 1 43mreplicaset.apps/prometheus-operator-5db9df7ffc 1 1 1 49mNAME DESIRED CURRENT AGEstatefulset.apps/alertmanager-kube-prometheus 1 1 43mstatefulset.apps/prometheus-kube-prometheus 1 1 43m 访问Prometheus-Operator 部署时已经定义alertmanager、prometheus、grafana的Service为NodePort 根据检查部署的情况，得知 kube-prometheus的NodePort为30900 kube-prometheus-alertmanager的NodePort为30903 kube-prometheus-grafana的NodePort为30164 直接通过这些端口访问即可 grafana已内嵌了基础的Dashboard模板，以admin用户登录即可见 EFK说明 官方提供简单的fluentd-elasticsearch样例，可以作为测试用途 已经包含在kubernetes项目当中链接 这里使用kubernetes-server-linux-amd64.tar.gz里面的kubernetes-src.tar.gz提供的Addons 修改elasticsearch使用rook-ceph提供的StorageClass作为持久化存储，默认是使用emptyDir 注意 EFK集群部署之后，kibana和elasticsearch初始化过程会极大的消耗服务器资源 请保证你的环境能撑的住！！！！ 配置不够，服务器真的会失去响应 实测3节点4C 16G SSD硬盘，CPU持续十几分钟的满载 解压源代码12345678910tar xzf kubernetes-server-linux-amd64.tar.gz kubernetes/kubernetes-src.tar.gzcd kubernetestar xzf kubernetes/kubernetes-src.tar.gztar xzf kubernetes-src.tar.gz \cluster/addons/fluentd-elasticsearch/es-service.yaml \cluster/addons/fluentd-elasticsearch/es-statefulset.yaml \cluster/addons/fluentd-elasticsearch/fluentd-es-configmap.yaml \cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yaml \cluster/addons/fluentd-elasticsearch/kibana-deployment.yaml \cluster/addons/fluentd-elasticsearch/kibana-service.yaml 切换工作目录1cd cluster/addons/fluentd-elasticsearch/ 修改yaml文件 删除es-statefuleset.yaml里面的字段，位置大概在100行左右 123volumes: - name: elasticsearch-logging emptyDir: &#123;&#125; 添加volumeClaimTemplates字段，声明使用rook-ceph提供的StorageClass，大小5Gi 位置在StatefulSet.spec，大概67行左右 123456789volumeClaimTemplates:- metadata: name: elasticsearch-logging spec: accessModes: [ "ReadWriteOnce" ] storageClassName: "rook-ceph-block" resources: requests: storage: 5Gi 修改后，es-statefulset.yaml内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119# RBAC authn and authzapiVersion: v1kind: ServiceAccountmetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: elasticsearch-logging labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilerules:- apiGroups: - "" resources: - "services" - "namespaces" - "endpoints" verbs: - "get"---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: kube-system name: elasticsearch-logging labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilesubjects:- kind: ServiceAccount name: elasticsearch-logging namespace: kube-system apiGroup: ""roleRef: kind: ClusterRole name: elasticsearch-logging apiGroup: ""---# Elasticsearch deployment itselfapiVersion: apps/v1kind: StatefulSetmetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging version: v5.6.4 kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: serviceName: elasticsearch-logging replicas: 2 selector: matchLabels: k8s-app: elasticsearch-logging version: v5.6.4 volumeClaimTemplates: - metadata: name: elasticsearch-logging spec: accessModes: [ "ReadWriteOnce" ] storageClassName: rook-ceph-block resources: requests: storage: 5Gi template: metadata: labels: k8s-app: elasticsearch-logging version: v5.6.4 kubernetes.io/cluster-service: "true" spec: serviceAccountName: elasticsearch-logging containers: - image: gcrxio/elasticsearch:v5.6.4 name: elasticsearch-logging resources: # need more cpu upon initialization, therefore burstable class limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: elasticsearch-logging mountPath: /data env: - name: "NAMESPACE" valueFrom: fieldRef: fieldPath: metadata.namespace volumes: - name: elasticsearch-logging emptyDir: &#123;&#125; # Elasticsearch requires vm.max_map_count to be at least 262144. # If your OS already sets up this number to a higher value, feel free # to remove this init container. initContainers: - image: alpine:3.6 command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"] name: elasticsearch-logging-init securityContext: privileged: true 注释kibana-deployment.yaml定义的环境变量 大概在35行左右 12# - name: SERVER_BASEPATH# value: /api/v1/namespaces/kube-system/services/kibana-logging/proxy 修改镜像地址 默认yaml定义的镜像地址是k8s.gcr.io，需要科学上网 变更成gcrxio 1sed -e 's,k8s.gcr.io,gcrxio,g' -i *yaml 给节点打Label fluentd-es-ds.yaml有nodeSelector字段定义了运行在带有beta.kubernetes.io/fluentd-ds-ready: &quot;true&quot;标签的节点上 这里为了方便，直接给所有节点都打上标签 1kubectl label node --all beta.kubernetes.io/fluentd-ds-ready=true 部署EFK1kubectl apply -f . 查看部署情况12345678910111213141516171819202122kubectl -n kube-system get pod -l k8s-app=elasticsearch-loggingNAME READY STATUS RESTARTS AGEelasticsearch-logging-0 1/1 Running 1 10melasticsearch-logging-1 1/1 Running 0 10mkubectl -n kube-system get pod -l k8s-app=kibana-loggingNAME READY STATUS RESTARTS AGEkibana-logging-56fb9d765-l95kj 1/1 Running 1 37mkubectl -n kube-system get pod -l k8s-app=fluentd-esNAME READY STATUS RESTARTS AGEfluentd-es-v2.0.4-2mwz7 1/1 Running 0 3mfluentd-es-v2.0.4-7mk4d 1/1 Running 0 3mfluentd-es-v2.0.4-zqtpc 1/1 Running 0 3mkubectl -n kube-system get svc -l k8s-app=elasticsearch-loggingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEelasticsearch-logging ClusterIP 10.111.107.21 &lt;none&gt; 9200/TCP 39mkubectl -n kube-system get svc -l k8s-app=kibana-loggingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkibana-logging ClusterIP 10.96.170.77 &lt;none&gt; 5601/TCP 39m 访问EFK 修改elasticsearch和kibana的svc为nodePort 12kubectl patch -n kube-system svc elasticsearch-logging -p '&#123;"spec":&#123;"type":"NodePort"&#125;&#125;'kubectl patch -n kube-system svc kibana-logging -p '&#123;"spec":&#123;"type":"NodePort"&#125;&#125;' 查看分配的nodePort 1234567kubectl -n kube-system get svc -l k8s-app=elasticsearch-loggingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEelasticsearch-logging NodePort 10.111.107.21 &lt;none&gt; 9200:30542/TCP 42mkubectl -n kube-system get svc -l k8s-app=kibana-loggingNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkibana-logging NodePort 10.96.170.77 &lt;none&gt; 5601:30998/TCP 42m 可以看到端口分别为30542和30998 在github上获取yaml文件 如果不想用kubernetes-src.tar.gz里面的Addons 可以直接下载github上面的文件，也是一样的 123456wget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/es-service.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/es-statefulset.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/fluentd-es-configmap.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/fluentd-es-ds.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/kibana-deployment.yamlwget https://raw.githubusercontent.com/kubernetes/kubernetes/$&#123;KUBERNETES_VERSION&#125;/cluster/addons/fluentd-elasticsearch/kibana-service.yaml 本文至此结束]]></content>
      <tags>
        <tag>kubernetes docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS-7.6(1810)虚拟机模板制作]]></title>
    <url>%2F2018%2F12%2F05%2FCentOS-7-6-1810-%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%A8%A1%E6%9D%BF%E5%88%B6%E4%BD%9C.html</url>
    <content type="text"><![CDATA[CentOS-7.6(1810)虚拟机模板制作 基于RHEL7.6的CentOS-7.6(1810)在12月初正式发布了 发行注记 顺便更新一下虚拟机模板，这里记录一下操作过程。 下载镜像 可以在CentOS官网下载，也可以通过国内镜像源下载 下载镜像名CentOS-7-x86_64-Minimal-1810.iso CentOS官网下载链接 阿里云下载链接 创建虚拟机 这里使用VMware Workstation 14 Pro 版本号14.1.3 build-9474260 虚拟机规格 客户机操作系统版本Red Hat Enterprise Linux 7 64 位 处理器数量1 内存2GB 硬盘40GB 网络适配器NAT模式 安装操作系统 语言选择English 软件包选择Minimal Install 硬盘分区 /dev/sda1boot分区、1GB、EXT4 /dev/sda2/分区、39GB、XFS 这里不使用swap分区，有需要可以自己增加swap分区 网络设置 NAT地址段为172.16.80.0/24 这里设置为 IP地址172.16.80.200 子网掩码255.255.255.0 网关172.16.80.2 DNS114.114.114.114 时区选择Asia/Shanghai 打开网络对时 KDUMP看情况选择打开或者关闭，这里我选择关闭 设置ROOT密码 操作系统启动后初始化设置关闭SELINUX12sed -i 's,SELINUX=enforcing,SELINUX=disabled,' /etc/selinux/configsetenforce 0 关闭防火墙123systemctl stop firewalldsystemctl disable firewalldsystemctl mask firewalld 清空iptables规则1234iptables -Ziptables -P INPUT ACCEPTiptables -P FORWARD ACCEPTiptables -P OUTPUT ACCEPT 配置ssh证书登录 通过ssh命令生成密钥对 将~/.ssh/id_rsa.pub提取出来 1ssh-keygen -t rsa -b 2048 -N "" -f ~/.ssh/id_rsa 添加sysctl参数1234567891011121314151617181920212223cat &gt; /etc/sysctl.d/centos.conf &lt;&lt;EOF # 最大文件句柄数fs.file-max=1024000# 在CentOS7.4引入了一个新的参数来控制内核的行为。 # /proc/sys/fs/may_detach_mounts 默认设置为0# 当系统有容器运行的时候，需要将该值设置为1。fs.may_detach_mounts = 1# 最大文件打开数fs.nr_open=1024000# 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤net.bridge.bridge-nf-call-arptables=1net.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1# 关闭严格校验数据包的反向路径net.ipv4.conf.default.rp_filter=0net.ipv4.conf.all.rp_filter=0# 打开ipv4数据包转发net.ipv4.ip_forward=1# 允许应用程序能够绑定到不属于本地网卡的地址net.ipv4.ip_nonlocal_bind=1 # 内存耗尽才使用swap分区vm.swappiness = 0 EOF 修改limits参数123456cat &gt; /etc/security/limits.d/99-centos.conf &lt;&lt;EOF* soft nproc 1024000* hard nproc 1024000* soft nofile 1024000* hard nofile 1024000EOF 修改终端提示符1234export PS1='[\t]\[\033[1;31m\]&lt;\u@\h:\w&gt;\[\033[0m\]\$ 'cat &gt;&gt; /root/.bashrc &lt;&lt; EOFexport PS1='[\t]\[\033[1;31m\]&lt;\u@\h:\w&gt;\[\033[0m\]\\$ 'EOF 修改网卡配置信息 CentOS安装设置网卡后，会添加很多不需要的字段，例如UUID、HWADDR什么的 删减后字段信息如下 123456789101112cat /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=EthernetBOOTPROTO=noneNAME=ens33DEVICE=ens33ONBOOT=yesIPADDR=172.16.80.200NETMASK=255.255.255.0GATEWAY=172.16.80.2DNS1=114.114.114.114NM_CONTROLLED=noUSERCTL=no 更新软件包 通常来说，安装完操作系统，都需要更新一下软件包 1yum update -y 安装常用软件包 CentOS最小化安装不能满足我的使用，需要额外安装一些软件包 123yum groups install base -yyum install epel-release -yyum install redhat-lsb vim ipvsadm tree dstat iotop htop socat ipset conntrack bash-completion-extras -y 添加vim设置（可选） 将vim设置写入~/.vimrc文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859cat &gt; ~/.vimrc &lt;&lt;EOF" 显示行号set number" 高亮光标所在行set cursorline" 打开语法显示syntax on" 关闭备份set nobackup" 没有保存或文件只读时弹出确认set confirm" tab缩进set tabstop=4set shiftwidth=4set expandtabset smarttab" 默认缩进4个空格大小 set shiftwidth=4 " 文件自动检测外部更改set autoread" 高亮查找匹配set hlsearch" 显示匹配set showmatch" 背景色设置为黑色set background=dark" 浅色高亮显示当前行autocmd InsertLeave * se nocul" 显示输入的命令set showcmd" 字符编码set encoding=utf-8" 开启终端256色显示set t_Co=256" 增量式搜索 set incsearch" 设置默认进行大小写不敏感查找set ignorecase" 如果有一个大写字母，则切换到大小写敏感查找set smartcase" 不产生swap文件set noswapfile" 关闭提示音set noerrorbells" 历史记录set history=10000" 显示行尾空格set listchars=tab:»■,trail:■" 显示非可见字符set list" c文件自动缩进set cindent" 文件自动缩进set autoindent" 检测文件类型filetype on" 智能缩进set smartindentEOF 编写ipvs模块启动关闭脚本（可选）12345678910111213# 启动脚本cat &gt; /usr/local/bin/enable_ipvs.sh &lt;&lt;EOF#!/bin/bashipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4"for kernel_module in \$&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe \$&#123;kernel_module&#125; fidoneEOFchmod 755 /usr/local/bin/enable_ipvs.sh 12345678910111213# 关闭脚本cat &gt; /usr/local/bin/disable_ipvs.sh &lt;&lt;EOF#!/bin/bashipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4"for kernel_module in \$&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe -r \$&#123;kernel_module&#125; fidoneEOFchmod 755 /usr/local/bin/disable_ipvs.sh 123456789101112# 开机启动脚本cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4"for kernel_module in \$&#123;ipvs_modules&#125;; do /sbin/modinfo -F filename \$&#123;kernel_module&#125; &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then /sbin/modprobe \$&#123;kernel_module&#125; fidoneEOFchmod 755 /etc/sysconfig/modules/ipvs.modules 安装docker（可选） 版本可以自行选择自带的Docker1.13或者docker-ce 这里以docker-ce 18.03版本为例 1234# 删除原有的Docker包# 安装依赖包yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine -yyum install -y yum-utils device-mapper-persistent-data lvm2 -y 1234# 添加Docker-CE YUM源yum-config-manager --add-repo http://mirrors.ustc.edu.cn/docker-ce/linux/centos/docker-ce.repo# 修改Docker—CE源地址sed -e 's,https://download.docker.com,https://mirrors.ustc.edu.cn/docker-ce,g' -i /etc/yum.repos.d/docker-ce.repo 12# 安装Docker-CE 18.03yum install docker-ce-18.03.1.ce -y 1234567891011121314# 配置dockermkdir -p /etc/dockercat&gt;/etc/docker/daemon.json&lt;&lt;EOF&#123; "registry-mirrors": ["https://registry.docker-cn.com"], "insecure-registries": [], "log-driver": "json-file", "log-opts": &#123; "max-size": "100m", "max-file": "3" &#125;, "max-concurrent-downloads": 10&#125;EOF 修改HISTORY参数1234567cat &gt;&gt; /etc/profile &lt;&lt;EOFexport HISTSIZE=10000export HISTFILESIZE=10000export HISTCONTROL=ignoredupsexport HISTTIMEFORMAT="`whoami` %F %T "export HISTIGNORE="ls:pwd:"EOF 清理现场清理yum缓存1yum clean all 关闭操作系统1history -c &amp;&amp; sys-unconfig]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello world~]]></title>
    <url>%2F2018%2F12%2F01%2Fhello-world.html</url>
    <content type="text"><![CDATA[参考了Hexo文档，简单搭建完hexo之后，又花了一些时间来调整主题什么的。 慢慢会将以前积累的文档，放到这里来。 用输出倒逼输入]]></content>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
</search>
